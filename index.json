[{"categories":null,"contents":"LDBC\u0026rsquo;s Social Network Benchmark [4] (LDBC SNB) is an industrial and academic initiative, formed by principal actors in the field of graph-like data management. Its goal is to define a framework where different graph-based technologies can be fairly tested and compared, that can drive the identification of systems' bottlenecks and required functionalities, and can help researchers open new frontiers in high-performance graph data management.\nLDBC SNB provides Datagen (Data Generator), which produces synthetic datasets, mimicking a social network\u0026rsquo;s activity during a period of time. Datagen is defined by the charasteristics of realism, scalability, determinism and usability. To address scalability in particular, Datagen has been implemented on the MapReduce computation model to enable scaling out across a distributed cluster. However, since its inception in the early 2010s there has been a tremendous amount of development in the big data landscape, both in the sophistication of distributed processing platforms, as well as public cloud IaaS offerings. In the light of this, we should reevaluate this implementation, and in particular, investigate if Apache Spark would be a more cost-effective solution for generating datasets on the scale of tens of terabytes, on public clouds such as Amazon Web Services (AWS).\nOverview The benchmark\u0026rsquo;s specification describes a social network data model which divides its components into two broad categories: static and dynamic. The dynamic element consists of an evolving network where people make friends, post in forums, comment or like each others posts, etc. In contrast, the static component contains related attributes such as countries, universities and organizations and are fixed values. For the detailed specifications of the benchmark and the Datagen component, see References.\nDatasets are generated in a multi-stage process captured as a sequence of MapReduce steps (shown in the diagram below).\nFigure 1. LDBC SNB Datagen Process on Hadoop\nIn the initialization phase dictionaries are populated and distributions are initialized. In the first generation phase persons are synthesized, then relationships are wired between them along 3 dimensions (university, interest and random). After merging the graph of person relationships, the resulting dataset is output. Following this, activities such as forum posts, comments, likes and photos are generated and output. Finally, the static components are output.\nNote: The diagram shows the call sequence as implemented. All steps are sequential \u0026ndash; including the relationship generation \u0026ndash;, even in cases when the data dependencies would allow for parallelization.\nEntities are generated by procedural Java code and are represented as POJOs in memory and as sequence files on disk. Most entities follow a shallow representation, i.e foreign keys (in relational terms) are mapped to integer ids, which makes serialization straightforward.1 A notable exception is the Knows edge which contains only the target vertex, and is used as a navigation property on the source Person. The target Person is replaced with only the foreign key augmented with some additional information in order to keep the structure free of cycles. Needless to say, this edge as property representation makes the data harder to handle in SQL than it would be with a flat join table.\nEntity generation amounts to roughly one fifth of the main codebase. It generates properties drawn from several random distributions using mutable pRNGs. Determinism is achieved by initializing the pRNGs to seeds that are fully defined by the configuration with constants, and otherwise having no external state in the logic.2\nSerialization is done by hand-written serializers for the supported output formats (e.g. CSV) and comprises just a bit less than one third of the main codebase. Most of the output is created by directly interacting with low-level HDFS file streams. Ideally, this code should be migrated to higher-level writers that handle faults and give consistent results when the task has to be restarted.\nMotivations for the migration The application is written using Hadoop MapReduce, which is now largely superseded by more modern distributed batch processing platforms, notably Apache Spark. For this reason, it was proposed to migrate Datagen to Spark. The migration provides the following benefits:\n  Better memory utilization: MapReduce is disk-oriented, i.e. it writes the output to disk after each reduce stage which is then read by the next MapReduce job. As public clouds provide virtual machines with sufficient RAM to encapsulate any generated dataset, time and money are wasted by the overhead this unnecessary disk I/O incurs. Instead, the intermediate results should be cached in memory where possible. The lack of support for this is a well-known limitation of MapReduce.\n  Smaller codebase: The Hadoop MapReduce library is fairly ceremonial and boilerplatey. Spark provides a higher-level abstraction that is simpler to work with, while still providing enough control on the lower-level details required for this workload.\n  Small entry cost: Spark and MapReduce are very close conceptually, they both utilise HDFS under the hood, and run on the JVM. This means that a large chunk of the existing code can be reused, and migration to Spark can, therefore, be completed with relatively small effort. Additionally, MapReduce and Spark jobs can be run on AWS EMR using basically the same HW/SW configuration, which facilitates straightforward performance comparisons.\n  Incremental improvements: Spark exposes multiple APIs for different workloads and operating on different levels of abstraction. Datagen may initially utilise the lower-level, Java-oriented RDDs (which offer the clearest 1 to 1 mapping when coming from MapReduce) and gradually move towards DataFrames to support Parquet output in the serializers and maybe unlock some SQL optimization capabilities in the generators later down the road.\n  OSS, commodity: Spark is one of the most widely used open-source big data platforms. Every major public cloud provides a managed offering for Spark. Together these mean that the migration increases the approachability and portability of the code.\n  First steps The first milestone is a successful run of LDBC Datagen on Spark while making the minimum necessary amount of code alterations. This entails the migration of the Hadoop wrappers around the generators and serializers. The following bullet-points summarize the key notions that cropped up during the process.\n  Use your memory: A strong focus was placed on keeping the call sequence intact, so that the migrated code evaluates the same steps in the same order, but with data passed as RDDs. It was hypothesised that the required data could be either cached in memory entirely at all times, or if not, regenerating them would still be faster than involving the disk I/O loop (e.g by using MEMORY_AND_DISK). In short, the default caching strategy was used everywhere.\n  Regression tests: Lacking tests apart from an id uniqueness check, meant there were no means to detect bugs introduced by the migration. Designing and implementing a comprehensive test suite was out of scope, so instead, regression testing was utilised, with the MapReduce output as the baseline. The original output mostly consists of Hadoop sequence files which can be read into Spark, allowing comparisons to be drawn with the output from the RDD produced by the migrated code.\n  Thread-safety concerns: Soon after migrating the first generator and running the regression tests, there were clear discrepancies in the output. These only surfaced when the parallelization level was set greater than 1. This indicated the presence of potential race conditions. Thread-safety wasn\u0026rsquo;t a concern in the original implementation due to the fact that MapReduce doesn\u0026rsquo;t use thread-based parallelization for mappers and reducers.3 In Spark however, tasks are executed by parallel threads in the same JVM application, so the code is required to be thread-safe. After some debugging, a bug was discovered originating from the shared use of java.text.SimpleDateFormat (notoriously known to be not thread-safe) in the serializers. This was resolved simply by changing to java.time.format.DateTimeFormatter. There were multiple instances of some static field on an object being mutated concurrently. In some cases this was a temporary buffer and was easily resolved by making it an instance variable. In another case a shared context variable was used, which was resolved by passing dedicated instances as function arguments. Sadly, the Java language has the same syntax for accessing locals, fields and statics, 4 which makes it somewhat harder to find potential unguarded shared variables.\n  Case study: Person ranking Migrating was rather straightforward, however, the so-called person ranking step required some thought. The goal of this step is to organize persons so that similar ones appear close to each other in a deterministic order. This provides a scalable way to cluster persons according to a similarity metric, as introduced in the S3G2 paper [3].\nThe original MapReduce version Figure 2. Diagram of the MapReduce code for ranking persons\nThe implementation, shown in pseudocode above, works as follows:\n The equivalence keys are mapped to each person and fed into TotalOrderPartitioner which maintains an order sensitive partitioning while trying to emit more or less equal sized groups to keep the data skew low. The reducer keys the partitions with its own task id and a counter variable which has been initialized to zero and incremented on each person, establishing a local ranking inside the group. The final state of the counter (which is the total number of persons in that group) is saved to a separate \u0026ldquo;side-channel\u0026rdquo; file upon the completion of a reduce task. In a consecutive reduce-only stage, the global order is established by reading all of these previously emitted count files in the order of their partition number in each reducer, then creating an ordered map from each partition number to the corresponding cumulative count of persons found in all preceding ones. This is done in the setup phase. In the reduce function, the respective count is incremented and assigned to each person.  Once this ranking is done, the whole range is sliced up into equally sized blocks, which are processed independently. For example, when wiring relationships between persons, only those appearing in the same block are considered.\nThe migrated version Spark provides a sortBy function which takes care of the first step above in a single line. The gist of the problem remains collecting the partition sizes and making them available in a later step. While the MapReduce version uses a side output, in Spark the partition sizes are collected in a separate job and passed into the next phase using a broadcast variable. The resulting code size is a fraction of the original one.\nBenchmarks Benchmarks were carried out on AWS EMR, originally utilising i3.xlarge instances because of their fast NVMe SSD storage and ample amount of RAM.\nThe application parameter hadoop.numThreads controls the number of reduce threads in each Hadoop job for the MapReduce version and the number of partitions in the serialization jobs in the Spark one. For MapReduce, this was set to n_nodes, i.e. the number of machines; experimentation yield slowdowns for higher values. The Spark version on the other hand, performed better with this parameter set to n_nodes * v_cpu. The scale factor (SF) parameter determines the output size. It is defined so that one SF unit generates around 1 GB of data. That is, SF10 generates around 10 GB, SF30 around 30 GB, etc. It should be noted however, that incidentally the output was only 60% of this in these experiments, stemming from two reasons. One, update stream serialization was not migrated to Spark, due to problems in the original implementation. Of course, for the purpose of faithful comparison the corresponding code was removed from the MapReduce version as well before executing the benchmarks. This explains a 10% reduction from the expected size. The rest can be attributed to incorrectly tuned parameters.5 The MapReduce results were as follows:\n   SF workers Platform Instance Type runtime (min) runtime * worker/SF (min)     10 1 MapReduce i3.xlarge 16 1.60   30 1 MapReduce i3.xlarge 34 1.13   100 3 MapReduce i3.xlarge 40 1.20   300 9 MapReduce i3.xlarge 44 1.32    It can be observed that the runtime per scale factor only increases slowly, which is good. The metric charts show an underutilized, bursty CPU. The bursts are supposedly interrupted by the disk I/O parts when the node is writing the results of a completed job. It can also be seen that the memory only starts to get consumed after 10 minutes of the run have passed.\nFigure 3. CPU Load for the Map Reduce cluster is bursty and less than 50% on average (SF100, 2nd graph shows master)\nFigure 4. The job only starts to consume memory when already 10 minutes into the run (SF100, 2nd graph shows master)\nLet\u0026rsquo;s see how Spark fares.\n   SF workers Platform Instance Type runtime (min) runtime * worker/SF (min)     10 1 Spark i3.xlarge 10 1.00   30 1 Spark i3.xlarge 21 0.70   100 3 Spark i3.xlarge 27 0.81   300 9 Spark i3.xlarge 36 1.08   1000 30 Spark i3.xlarge 47 1.41   3000 90 Spark i3.xlarge 47 1.41    A similar trend here, however the run times are around 70% of the MapReduce version. It can be seen that the larger scale factors (SF1000 and SF3000) yielded a long runtime than expected. On the metric charts of SF100 the CPU shows full utilization, except at the end, when the results are serialized in one go and the CPU is basically idle (the snapshot of the diagram doesn\u0026rsquo;t include this part unfortunately). Spark can be seen to have used up all memory pretty fast even in case of SF100. In case of SF1000 and SF3000, the nodes are running so low on memory that most probably some of the RDDs have to be calculated multiple times (no disk level serialization was used here), which seem to be the most plausible explanation for the slowdowns experienced. In fact, the OOM errors encountered when running SF3000 supports this hypothesis even further. It was thus proposed to scale up the RAM in the instances. The CPU utilization hints that adding some extra vCPUs as well can further yield speedup.\nFigure 5. Full CPU utilization for Spark (SF100, last graph shows master)\nFigure 6. Spark eats up memory fast (SF100, 2nd graph shows master)\ni3.2xlarge would have been the most straightforward option for scaling up the instances, however the humongous 1.9 TB disk of this image is completely unnecessary for the job. Instead the cheaper r5d.2xlarge instance was utilised, largely identical to i3.2xlarge, except it only has a 300 GB SSD.\n   SF workers Platform Instance Type runtime (min) runtime * worker/SF (min)     100 3 Spark r5d.2xlarge 16 0.48   300 9 Spark r5d.2xlarge 21 0.63   1000 30 Spark r5d.2xlarge 26 0.78   3000 90 Spark r5d.2xlarge 25 0.75   10000 303 Spark r5d.2xlarge 25 0.75    The last column clearly demonstrates our ability to keep the cost per scale factor unit constant.\nNext steps The next improvement is refactoring the serializers so they use Spark\u0026rsquo;s high-level writer facilities. The most compelling benefit is that it will make the jobs fault-tolerant, as Spark maintains the integrity of the output files in case the task that writes it fails. This makes Datagen more resilient and opens up the possibility to run on less reliable hardware configuration (e.g. EC2 spot nodes on AWS) for additional cost savings. They will supposedly also yield some speedup on the same cluster configuration.\nAs already mentioned, the migration of the update stream serialization was ignored due to problems with the original code. Ideally, they should be implemented with the new serializers.\nThe Spark migration also serves as an important building block for the next generation of LDBC benchmarks. As part of extending the SNB benchmark suite, the SNB task force has recently extended Datagen with support for generating delete operations [1]. The next step for the task force is to fine-tune the temporal distributions of these deletion operations to ensure that the emerging sequence of events is realistic, i.e. the emerging distribution resembles what a database system would experience when serving a real social network.\nAcknowledgements This work is based upon the work of Arnau Prat, Gábor Szárnyas, Ben Steer, Jack Waudby and other LDBC contributors. Thanks for your help and feedback!\nReferences [1] Supporting Dynamic Graphs and Temporal Entity Deletions in the LDBC Social Network Benchmark\u0026rsquo;s Data Generator\n[2] 9th TUC Meeting \u0026ndash; LDBC SNB Datagen Update \u0026ndash; Arnau Prat (UPC) - slides\n[3] S3G2: a Scalable Structure-correlated Social Graph Generator\n[4] The LDBC Social Network Benchmark\n[5] LDBC - LDBC GitHub organization\n  Also makes it easier to map to a tabular format thus it is a SQL friendly representation. \u0026#x21a9;\u0026#xfe0e;\n It\u0026rsquo;s hard to imagine this done declaratively in SQL. \u0026#x21a9;\u0026#xfe0e;\n Instead, multiple YARN containers have to be used if you want to parallelize on the same machine. \u0026#x21a9;\u0026#xfe0e;\n Although editors usually render these using different font styles. \u0026#x21a9;\u0026#xfe0e;\n With the addition of deletes, entities often get inserted and deleted during the simulation (which is normal in a social network). During serialization, we check for such entities and omit them. However we forgot to calculate this when determining the output size, which we will amend when tuning the distributions. \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://ldbc.github.io/post/speeding-up-ldbc-snb-datagen/","tags":["LDBC Datagen","SNB"],"title":"Speeding Up LDBC SNB Datagen"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-journalscorrabs-2010-12243/","tags":[],"title":"An analysis of the SIGMOD 2014 Programming Contest: Complex queries on the LDBC social network graph"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confsigmod-waudby-sps-20/","tags":[],"title":"Supporting Dynamic Graphs and Temporal Entity Deletions in the LDBC Social Network Benchmark's Data Generator"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-journalscorrabs-2011-15028/","tags":[],"title":"The LDBC Graphalytics Benchmark"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-journalscorrabs-2001-02299/","tags":[],"title":"The LDBC Social Network Benchmark"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/tpctc-acid/","tags":[],"title":"Towards Testing ACID Compliance in the LDBC Social Network Benchmark"},{"categories":null,"contents":"Read more information and how to register HERE [CLOSED]\nCALL FOR CONTRIBUTIONS:\n[CLOSED] The LDBC meeting will feature updates on activities of LDBC, but is also seeking contributions by community members and graph practitioners and researchers. These proposals can be emailed to Peter Boncz (boncz@cwi.nl)\nFINAL PROGRAM (UPDATED):\n 08:30-10:30 LDBC Board Meeting (non-public) 10:30-11:00 Coffee 11:00-12:45 Session 1: Graph Benchmarks  11:00-11:05 Welcome \u0026amp; introduction 11:05-11:45 Gabor Szarnyas (BME), Benjamin Steer (QMUL), Jack Waudby (Newcastle University): Business Intelligence workload: Progress report and roadmap 11:45-12:00 Frank McSherry (Materialize): Experiences implementing LDBC queries in a dataflow system 12:00-12:25 Vasileios Trigonakis (Oracle): Evaluating a new distributed graph query engine with LDBC: Experiences and limitations 12:25-12:45 Ahmed Musaafir (VU Amsterdam): LDBC Graphalytics   12:45-14:00 Lunch 14:00-16:05 Session 2: Graph Query Languages  14:00-14:25 Juan Sequeda (Capsenta): Property Graph Schema Working Group: A progress report 4:25-14:50 Stefan Plantikow (Neo4j): GQL: Scope and features 14:50-15:15 Vasileios Trigonakis (Oracle): Property graph extensions for the SQL standard 15:15-15:40 Alin Deutsch (TigerGraph): Modern graph analytics support in GSQL, TigerGraph's query language 15:40-16:05 Jan Posiadała (Nodes and Edges, Poland): Executable semantics of graph query language.   16:05-16:30 Coffee 16:30-17:50 Session 3: Graph System Performance  16:30-16:50 Per Fuchs (CWI): Fast, scalable WCOJ graph-pattern matching on in-memory graphs in Spark 16:50-17:10 Semih Salihoglu (University of Waterloo): Optimizing subgraph queries with a mix of tradition and modernity 17:10-17:30 Roi Lipman (RedisGraph): Evaluating Cypher queries and procedures as algebraic operations within RedisGraph 17:30-17:50 Alexandru Uta (VU Amsterdam): Low-latency Spark queries on updatable data    ","permalink":"https://ldbc.github.io/event/twelfth-tuc-meeting-at-sigmod-2019-amsterdam/","tags":["TUC Meeting"],"title":"Twelfth TUC Meeting at SIGMOD 2019 Amsterdam"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confgrades-szarnyas-pampkeb-18/","tags":[],"title":"An early look at the LDBC Social Network Benchmark's Business Intelligence workload"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confsigmod-angles-abbfglpps-18/","tags":[],"title":"G-CORE: A Core for Future Graph Query Languages"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confgrades-leo-b-17/","tags":[],"title":"Extending SQL for Computing Shortest Paths"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confgrades-ngai-hhi-17/","tags":[],"title":"Granula: Toward Fine-grained Performance Analysis of Large-scale Graph Processing Platforms"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confgrades-prat-perez-gskdb-17/","tags":[],"title":"Towards a property graph generator for benchmarking"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confsemweb-kotsev-mpefk-16/","tags":[],"title":"Benchmarking RDF Query Engines: The LDBC Semantic Publishing Benchmark"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-journalspvldb-iosup-hnhpmccsat-16/","tags":[],"title":"LDBC Graphalytics: A Benchmark for Large-Scale Graph Analysis on Parallel and Distributed Platforms"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confwww-pham-peb-15/","tags":[],"title":"Deriving an Emergent Relational Schema from RDF Data"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-journalsws-loizou-ag-15/","tags":[],"title":"On the formulation of performant SPARQL queries"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confsigmod-erling-alcgppb-15/","tags":[],"title":"The LDBC Social Network Benchmark: Interactive Workload"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confsigmod-guisado-gamez-p-14/","tags":[],"title":"Understanding Graph Structure of Wikipedia for Query Expansion"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-journalsercim-angles-pb-14/","tags":[],"title":"Benchmarking Linked Open Data Management Systems"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-bookscrclinked-14-boncz-ep-14/","tags":[],"title":"Experiences with Virtuoso Cluster RDF Column Store"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confedbt-gubichev-014/","tags":[],"title":"Exploiting the query structure for efficient join ordering in SPARQL queries"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confsigmod-gubichev-t-14/","tags":[],"title":"Graph Pattern Matching - Do We Have to Reinvent the Wheel?"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confwww-prat-perez-dl-14/","tags":[],"title":"High quality, scalable and parallel community detection for large real graphs"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confsigmod-prat-d-14/","tags":[],"title":"How community-like is the structure of synthetically generated graphs?"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-conficde-gubichev-ab-14/","tags":[],"title":"How to generate query parameters in RDF benchmarks?"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confrweb-larriba-pey-md-14/","tags":[],"title":"Introduction to Graph Databases"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-journalsercim-pham-b-14/","tags":[],"title":"MonetDB/RDF: Discovering and Exploiting the Emergent Schema of RDF Data"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-conftpctc-gubichev-b-14/","tags":[],"title":"Parameter Curation for Benchmark Queries"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-journalssigmod-angles-blf-0-enmkt-14/","tags":[],"title":"The Linked Data Benchmark Council: A graph and RDF industry benchmarking effort"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confsigmod-martinez-bazan-d-14/","tags":[],"title":"Using semijoin programs to solve traversal queries in graph databases"},{"categories":null,"contents":" Introduction Agenda Logistics LDBC/TUC Background  Social Network Benchmark Semantic Publishing Benchmark     The LDBC consortium is pleased to announce the third Technical User Community (TUC) meeting!\nThis will be a one day event in London on the 19 November 2013 running in collaboration with the GraphConnect event (18/19 November). Registered TUC participants that would like a free pass to all of GraphConnect should register for GraphConnect using this following coupon code: LDBCTUC.\nThe TUC event will include:\n Introduction to the objectives and progress of the LDBC project Description of the progress of the benchmarks being evolved through Task Forces Users explaining their use-cases and describing the limitations they have found in current technology Industry discussions on the contents of the benchmarks  We will also be launching the LDBC non-profit organization, so anyone outside the EU project will be able to join as a member.\nWe will kick off new benchmark development task forces in the coming year, and talks at this coming TUC will play an important role in deciding the use case scenarios that will drive those benchmarks.\nAll users of RDF and graph databases are welcome to attend. If you are interested, please contact: ldbc AT ac DOT upc DOT edu\nAgenda November 19th - Public TUC Meeting\n8:00 Breakfast and registration will open for Graph Connect/TUC at 8:00 am (Dexter House)\nshort LDBC presentation (Peter Boncz) during GraphConnect keynote by Emil Eifrem (09:00-09:30 Dexter House)\nNOTE: the TUC meeting is at the Tower Hotel, nearby Dexter House. 10:00 TUC Meeting Opening (Peter Boncz)\n10:10 TUC Presentations (RDF Application Descriptions)\n Johan Hjerling (BBC): BBC Linked Data and the Semantic Publishing Benchmark Andreas Both (Unister): Ontology-driven applications in an e-commerce context Nuno Carvalho (Fujitsu Laboratories Europe): Fujitsu RDF use cases and benchmarking requirements Robina Clayphan (Europeana): Europeana and Open Data  11:30 Semantic Publishing Benchmark (SPB)\n Venelin Kotsev (Ontotext - LDBC): Semantic Publishing Benchmark Task Force Update and Report  12:00-13:00 Lunch at the Graph Connect venue\nTalks During Lunch:\n [Pedro Furtado, Jorge Bernardino (Univ. Coimbra): KEYSTONE Cost Action]  13:00 TUC Presentations (Graph Application Descriptions)\n Minqi Zhou / Weining Qian (East China Normal University): Elastic and realistic social media data generation Andrew Sherlock (Shapespace): Shapespace Use Case Sebastian Verheughe (Telenor): Real-time Resource Authorization  14:00 Social Network Benchmark (SNB)\n Norbert Martinez (UPC - LDBC): Social Network Benchmark Task Force Update and Report  14:30 Break\n14:45 TUC Presentations (Graph Analytics)\n Keith Houck (IBM): Benchmarking experiences with [System G Native Store (tentative title)] Abraham Bernstein (University of Zurich): Streams and Advanced Processing: Benchmarking RDF querying beyond the Standard SPARQL Triple Store Luis Ceze (University of Washington): Grappa and GraphBench Status Update  15:45 Break\n16:00 TUC Presentations* (Possible Future RDF Benchmarking Topics)*\n Christian-Emil Ore (Unit for Digital Documentation, University of Oslo, Norway): CIDOC-CRM [Atanas Kiryakov (Ontotext): Large-scale Reasoning with a Complex Cultural Heritage Ontology (CIDOC CRM) Kostis Kyzirakos (National and Kapodistrian University of Athens / CWI): Geographica: A Benchmark for Geospatial RDF Stores Xavier Lopez (Oracle): W3C Property Graph progress Thomas Scharrenbach (University Zurich) PCKS: Benchmarking Semantic Flow Processing Systems  17:20 Meeting Conclusion (Josep Larriba Pey)\n17:30 End of TUC meeting\n19:00 Social dinner\nNovember 20th - Internal LDBC Meeting\n10:00 Start\n12:30 End of meeting\n coffee and lunch provided  Logistics Date\n19th November 2013\nLocation\nThe TUC meeting will be held in The Tower hotel (Google Maps link) approximately 4 minutes walk from the GraphConnect conference in London.\nGetting there\n From City Airport is the easiest: short ride on the DLR to Tower Gateway. Easy. From London Heathrow: first need to take the Heathrow Express to Paddington. Then take the Circle line to Tower Hill. See attached.  Accomodation\nTower Hill is nice http://www.booking.com/hotel/gb/sleep-inn-city-of-london.en-us.html - book early to get a good rate\nSocial Dinner\nThe social dinner will take place at 7 pm on Nov 19.** TODO more details**\nTravel costs\n[There is some small budget available that can be used to assist some attendees that are otherwise unable to fund their trip. Please contact us using the following email address if you would like more information: ldbcgrants AT ac DOT upc DOT edu]\nLDBC/TUC Background Looking back, we have been working on two benchmarks for the past year: a Social Network Benchmark (SNB) and a Semantic Publishing Benchmark (SPB). While below we provide a short summary, all the details of the work on these benchmark development efforts can be found in the first yearly progress reports:\n LDBC_SNB_Report_Nov2013.pdf LDBC_SPB_Report_Nov2013.pdf  A summary of these efforts can be read below or, for a more detailed account, please refer to: The Linked Data Benchmark Council: a Graph and RDF industry benchmarking effort\nAnnual reports about the progress, results, and future work of these two efforts will soon be available for download here, and will be discussed in depth at the TUC.\nSocial Network Benchmark The Social Network Benchmark (SNB) is designed for evaluating a broad range of technologies for tackling graph data management workloads. The systems targeted are quite broad: from graph, RDF, and relational database systems to Pregel-like graph compute frameworks. The social network scenario was chosen with the following goals in mind:\n it should be understandable, and the relevance of managing such data should be understandable it should cover the complete range of interesting challenges, according to the benchmark scope the queries should be realistic, i.e., similar data and workloads are encountered in practice  SNB includes a data generator for creation of synthetic social network data with the following characteristics:\n data schema is representative of real social networks data generated includes properties occurring in real data, e.g. irregular structure, structure/value correlations, power-law distributions the software generator is easy-to-use, configurable and scalable  SNB is intended to cover a broad range of aspects of social network data management, and therefore includes three distinct workloads:\n Interactive  Tests system throughput with relatively simple queries and concurrent updates, it is designed to test ACID features and scalability in an online operational setting. The targeted systems are expected to be those that offer transactional functionality.   Business Intelligence  Consists of complex structured queries for analyzing online behavior of users for marketing purposes, it is designed to stress query execution and optimization. The targeted systems are expected to be those that offer an abstract query language.   Graph Analytics  Tests the functionality and scalability of systems for graph analytics, which typically cannot be expressed in a query language. Analytics is performed on most/all of the data in the graph as a single operation and produces large intermediate results, and it is not not expected to be transactional or need isolation.  The targeted systems are graph compute frameworks though database systems may compete, for example by using iterative implementations that repeatedly execute queries and keep intermediate results in temporary data structures.    Semantic Publishing Benchmark  The Semantic Publishing Benchmark (SPB) simulates the management and consumption of RDF metadata that describes media assets, or creative works.\nThe scenario is a media organization that maintains RDF descriptions of its catalogue of creative works \u0026ndash; input was provided by actual media organizations which make heavy use of RDF, including the BBC. The benchmark is designed to reflect a scenario where a large number of aggregation agents provide the heavy query workload, while at the same time a steady stream of creative work description management operations are in progress. This benchmark only targets RDF databases, which support at least basic forms of semantic inference. A tagging ontology is used to connect individual creative work descriptions to instances from reference datasets, e.g. sports, geographical, or political information. The data used will fall under the following categories: reference data, which is a combination of several Linked Open Data datasets, e.g. GeoNames and DBpedia; domain ontologies, that are specialist ontologies used to describe certain areas of expertise of the publishing, e.g., sport and education; publication asset ontologies, that describe the structure and form of the assets that are published, e.g., news stories, photos, video, audio, etc.; and tagging ontologies and the metadata, that links assets with reference/domain ontologies.\nThe data generator is initialized by using several ontologies and datasets. The instance data collected from these datasets are then used at several points during the execution of the benchmark. Data generation is performed by generating SPARQL fragments for create operations on creative works and executing them against the RDF database system.\nTwo separate workloads are modeled in SPB:\n Editorial Simulates creating, updating and deleting creative work metadata descriptions. Media companies use both manual and semi-automated processes for efficiently and correctly managing asset descriptions, as well as annotating them with relevant instances from reference ontologies. Aggregation Simulates the dynamic aggregation of content for consumption by the distribution pipelines (e.g. a web-site). The publishing activity is described as \u0026ldquo;dynamic\u0026rdquo;, because the content is not manually selected and arranged on, say, a web page. Instead, templates for pages are defined and the content is selected when a consumer accesses the page.   Attachments: article.pdf (application/pdf)\nldbc_tuc_london.pdf (application/pdf)\nLDBC_SPB_Report_Nov2013.pdf (application/pdf)\nLDBC_SNB_Report_Nov2013.pdf (application/pdf)\nLDBC_SPB_Report_Nov2013.pdf (application/pdf)\nldbc_tuc_19-11-2013_Europeana-Robina_Clayphan.ppt (application/vnd.ms-powerpoint)\nLDBC-TUC-Fujitsu-Final.pdf (application/pdf)\nLDBC London 19 Nov 2013 - Telenor Resource Authorization.pdf (application/pdf)\nKyzirakos-Geographica.pdf (application/pdf)\nBBC-JohanHjerling-LDBC-TUC-3.pdf (application/pdf)\nLDBC_TUC_wnqian.pdf (application/pdf)\nLondonLDBCKEYSTONE.pdf (application/pdf)\nLDBC-CRM-reasoning-Alexiev2013-AK.pdf (application/pdf)\nLDBC_TUC3_SNB.pdf (application/download)\nLDBC_Status of the Semantic Publishing Benchmark.pdf (application/download)\nandreas-both__linked-data-benchmark-concil__3rd-technical-user-meeting_2013.pdf (application/download)\\\n","permalink":"https://ldbc.github.io/event/third-tuc-meeting/","tags":["TUC Meeting"],"title":"Third TUC Meeting"},{"categories":null,"contents":"Created by Josep Larriba Pey, last modified by Peter Bonczon Apr 25, 2013\nThe LDBC consortium are pleased to announce the second Technical User Community (TUC) meeting.\nThis will be a two day event in Munich on the 22/23rd April 2013.\nThe event will include:\n Introduction to the objectives and progress of the LDBC project. Description of the progress of the benchmarks being evolved through Task Forces. Users explaining their use-cases and describing the limitations they have found in current technology. Industry discussions on the contents of the benchmarks.  All users of RDF and graph databases are welcome to attend. If you are interested, please contact: ldbc AT ac DOT upc DOT edu\nNews: due to the airline strikes at Lufthansa and El Al, Herman Ravkin (BIG project) and Jesús Lanchas (ACCESO) and Thomas Scharrenbach (University of Zurich) cannot come, and Norbert Martinez will arrive only tomorrow. The schedule has been adapted, and may change again, keep an eye on it.\n Agenda Slides Logistics  Date Location Venue  Getting to the TUM Campus from the Munich city center: Subway (U-Bahn) Getting to the TUM Campus from the Munich Airport Getting to the TUM Campus from Garching: U-Bahn   Getting there Accomodation Social Dinner Travel costs    Agenda April 22nd\n10:00 Registration.\n10:30 Josep Lluis Larriba Pey (UPC) - Welcome and Introduction.\n10:30 Peter Boncz (VUA): LDBC: goals and status\n Social Network Use Cases (with discussion moderated by Josep Lluis Larriba Pey)\n11:00 Josep Lluis Larriba Pey (UPC): Social Network Benchmark Task Force\n11:30 Gustavo González (Mediapro): Graph-based User Modeling through Real-time Social Streams12:00 Klaus Großmann (Dshini): Neo4j at Dshini\n12:30 Lunch\n Semantic Publishing Use Cases (with discussion moderated by Barry Bishop)\n13:30 Barry Bishop (Ontotext): Semantic Publishing Benchmark Task Force14:00 Dave Rogers (BBC): Linked Data Platform at the BBC14:30 Edward Thomas (Wolters Kluwer): Semantic Publishing at Wolters Kluwer\n15:00 coffee breakProjects Related to LDBC\n15:30 Fabian Suchanek (MPI): \u0026ldquo;YAGO: A large knowledge base from Wikipedia and WordNet\u0026rdquo;\n16:00 Antonis Loziou (VUA): The OpenPHACTS approach to data integration16:30 Mirko Kämpf (Brox): \u0026ldquo;GeoKnow - Spatial Data Web project and Supply Chain Use Case\u0026rdquo;17:00 End of first day\n19:00 Social dinner\nApril 23rd\n Industry \u0026amp; Hardware Aspects\n10:00 Xavier Lopez (Oracle): Graph Database Performance an Oracle Perspective.pdf 10:30 Pedro Trancoso (University of Cyprus): \u0026ldquo;Benchmarking and computer architecture: the research side\u0026rdquo;\n11:00 coffee breakFuture Steps and TUC feedback session\n11:30 Peter Boncz (VUA) moderates: \u0026ldquo;next steps in the Social Networking Task Force\u0026rdquo;\n12:00 Barry Bishop (Ontotext) moderates: \u0026ldquo;next steps in the Semantic Publishing Task Force\u0026rdquo;\n12:30 End of meetingSlides The slides of the presentations during the meeting will we posted here.\nLogistics Date 22nd and 23th April 2013\nLocation The TUC meeting will be held at LE009 room at LRZ (Leibniz-Rechenzentrum) located inside the TU Munich campus in Garching, Germany. The address is:\nLRZ (Leibniz-Rechenzentrum)Boltzmannstraße 185748 Garching, Germany\nVenue To reach the campus, there are several options, including Taxi and Subway Ubahn\nGetting to the TUM Campus from the Munich city center: Subway (U-Bahn) Take the U-bahn line U6 in the direction of Garching-Forschungszentrum, exit at the end station. Take the south exit to MI-Building and LRZ on the Garching Campus. The time of the journey from the city center is approx.25-30 minutes. In order to get here from the City Center, you need the Munich XXL ticket that costs around 7.50 euros and covers all types of transportation for one day. The ticket has to be validated before ride.\nGetting to the TUM Campus from the Munich Airport  (except weekends) S-Bahn S8 line in the direction of (Hauptbahnhof) Munich Central Station until the third stop, Ismaning (approx. 13 minutes). From here Bus Nr. 230 until stop MI-Building on the Garching Campus. Alternatively: S1 line until Neufahrn, then with the Bus 690, which stops at Boltzmannstraße. S-Bahn lines S8 or S1 towards City Center until Marienplatz stop. Then change to U-bahn U6 line towards Garching-Forschungszentrum, exit at the last station. Take the south exit to MI-Building and LRZ. Taxi: fare is ca,30-40 euros.  For cases 1 and 2, before the trip get the One-day Munich Airport ticket and validate it. It will cover all public transportation for that day.\nGetting to the TUM Campus from Garching: U-Bahn The city of Garching is located on the U6 line, one stop before the Garching-Forschungszentrum**.** In order to get from Garching to Garching-Forschungszentrum with the U-bahn, a special one-way ticket called Kurzstrecke (1.30 euros) can be purchased.\nFinding LRZ@TUM\nOpenStreetMap link\nGoogleMaps link\n Getting there Flying: Munich airport is located 28.5 km northeast of Munich. There are two ways to get from the airport to the city center: suburban train (S-bahn) and Taxi.\nS-Bahn:\nS-bahn lines S1 and S8 will get you from the Munich airport to the city center, stopping at both Munich Central Station (Hauptbahnhof) and Marienplatz. One-day Airport-City ticket costs 11.20 euros and is valid for the entire Munich area public transportation during the day of purchase (the tickets needs to be validated before the journey). S-bahn leaves every 5-20 minutes and reaches the city center in approx.40 minutes\nTaxi: taxi from the airport to the city center costs approximately 50 euros\n Accomodation The following hotels are recommended. The first two are located near the TUM campus in this google map\nHotel Koenig Ludwig II\nBürgerplatz 3,\nGarching, Germany\nTel: +49 89 320 50 46\nhttp://www.hkl.de/home-en/ Hotel Am Park\nBürgermeister-Amon-Straße 2,\nGarching, Germany\nTel: +49 89 320 40 84\nhttp://hotel-am-park.com/ Hotel-Pension Carolin\nKaulbachstraße 42\nMunich, Germany\nTel: +49 89 34 57 57\nhttp://www.pension-carolin.com/ Cosmopolitan Hotel\nHohenzollernstrasse 5\nMunich, Germany\nTel: +49 89 38 38 10\nhttp://www.cosmopolitanhotel.de/en/ Hotel Pullmann\nTheodor-Dombart-Straße 4\nMunich, Germany\nTel: +49 89 360 99 0\nhttp://www.pullman-hotel-munich.com/default-en.htmlSocial Dinner The social dinner will take place at 7 pm on April 22 in Hofbräuhaus (second floor)\nAddress: Hofbräuhaus, Platzl 9, Munich\nGoogleMaps link\nTravel costs There is some small budget available that can be used to assist some attendees that are otherwise unable to fund their trip. Please contact us using the following email address if you would like more information: ldbcgrants AT ac DOT upc DOT edu\nAttachments: e19940cef9.png (image/png)\ngetBuildingMap.gif (image/gif)\nmapbigger.gif (image/gif)\n5d84f31b1f.png (image/png)\nBBC_LDBC_presentation_Dave_Rogers.pptx (application/vnd.openxmlformats-officedocument.presentationml.presentation)\nDshini_LDBC_Klaus_Grossman.pdf (application/pdf)\nOntotext_LDBC_Barry_Bishop_Publishing.pptx (application/vnd.openxmlformats-officedocument.presentationml.presentation)\nMediaPro_LDBC_Gustavo_Gonzalez-Sanchez.pdf (application/pdf)\nUPC_LDBC_Norbert_Martinez.pdf (application/pdf)\nCWI_LDBC_Peter_Boncz.pptx (application/vnd.openxmlformats-officedocument.presentationml.presentation)\nWolters_Kluwer_LDBC_Edward_Thomas.pdf (application/pdf)\nOpenPhacts_LDBC_Antonis_Loizou.pptx (application/vnd.openxmlformats-officedocument.presentationml.presentation)\nGraph Database Performance an Oracle Perspective.pdf (application/pdf)\n","permalink":"https://ldbc.github.io/event/second-tuc-meeting/","tags":["TUC Meeting"],"title":"Second TUC Meeting"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confamw-angles-br-13/","tags":[],"title":"A Practical Query Language for Graph DBs"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confsigmod-angles-pdl-13/","tags":[],"title":"Benchmarking database systems for social network applications"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confsigmod-ma-wqyxz-13/","tags":[],"title":"On benchmarking online social media analytical queries"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-journalscorrabs-1301-5121/","tags":[],"title":"Partitioning Graph Databases - A Quantitative Evaluation"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-conficde-pham-13/","tags":[],"title":"Self-organizing structured RDF in MonetDB"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confsigmod-gubichev-bs-13/","tags":[],"title":"Sparqling Kleene: fast property paths in RDF-3X"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-journalsdbsk-boncz-fgl-013/","tags":[],"title":"The Linked Data Benchmark Council Project"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confsigmod-cattuto-qpa-13/","tags":[],"title":"Time-varying social networks in a graph database: a Neo4j use case"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-conftpctc-boncz-ne-13/","tags":[],"title":"TPC-H Analyzed: Hidden Messages and Lessons Learned from an Influential Benchmark"},{"categories":null,"contents":"The LDBC consortium are pleased to announce the first Technical User Community (TUC) meeting. This will be a two day event in Barcelona on the 19/20th November 2012.\nSo far more than six commercial consumers of graph/RDF database technology have expressed an interest in attending the event and more are welcome. The proposed format of the event wil include:\n Introduction by the coordinator and technical director explaining the objectives of the LDBC project Invitation to users to explain their use-cases and describe the limitations they have found in current technology Brain-storming session for identifying trends and mapping out strategies to tackle existing choke-points  The exact agenda will be published here as things get finalised before the event.\nAll users of RDF and graph databases are welcome to attend. If you are interested, please contact: ldbc AT ac DOT upc DOT edu\n Agenda Slides Logistics  Date Location Venue Getting there Accomodation Travel costs    Agenda We will start at 9:00 on Monday for a full day, followed by a half a day on Tuesday to allow attendees to travel home on the evening of the 20th.\nDay 1\n09:00 Welcome (Location: Aula Master)\n09:30 Project overview (Enphasis on task forces?) + Questionnaire results?\n10:30 Coffee break\n11:00 User talks (To gather information for use cases?)\n13:00 Lunch\n14:00 User talks (cont.)\n15:00 Use case discussions (based on questionnaire results + consortium proposal + user talks).\n16:00 Task force proposals (consortium)\n17:00 Finish first day\n20:00 Social dinner Day 2\n10:00 Task force discussion (consortium + TUC)\n11:00 Coffe break\n11:30 Task force discussion (consortium + TUC)\n12:30 Summaries (Task forces, use cases, \u0026hellip;) and actions\n13:00 Lunch and farewell\n15:00 LDBC Internal meeting\nSlides Opening session:\n CWI - Peter Boncz - Objectives UPC - Larri - Questionnaire  User stories:\n BBC - Jem Rayfield CA Technologies - Victor Muntés Connected Discovery (Open Phacts) - Bryn Williams-Jones Elsevier - Alan Yagoda ERA7 Bioinformatics- Eduardo Pareja Press Association - Jarred McGinnis RJLee - David Neuer Yale - Lec Maj   Benchmark proposals:\n Publishing benchmark proposal - Ontotext - Barry Bishop Social Network Benchmark Proposal - UPC - Larri   Logistics Date 19th and 20th November 2012\nLocation The TUC meeting will be held at “Aula Master” at A3 building located inside the “Campus Nord de la UPC” in Barcelona. The address is:\nAula Master\nEdifici A3, Campus Nord UPC\nC. Jordi Girona, 1-3\n08034 Barcelona, Spain\nVenue To reach the campus, there are several options, including Taxi, Metro and Bus.\nFinding UPC\nFinding the meeting room\n Getting there Flying: Barcelona airport is situated 12 km from the city. There are several ways of getting from the airport to the centre of Barcelona, the cheapest of which is to take the train located outside just a few minutes walking distance past the parking lots at terminal 2 (there is a free bus between terminal 1 and terminal 2, see this map of the airport. It is possible to buy 10 packs of train tickets which makes it cheaper. Taking the bus to the centre of town is more convenient as they leave directly from terminal 1 and 2, however it is more expensive than the train.\nRail: The Renfe commuter train leaves the airport every 30 minutes from 6.13 a.m. to 11.40 p.m. Tickets cost around 3€ and the journey to the centre of Barcelona (Sants or Plaça Catalunya stations) takes 20 minutes.\nBus: The Aerobus leaves the airport every 12 minutes, from 6.00 a.m. to 24.00, Monday to Friday, and from 6.30 a.m. to 24.00 on Saturdays, Sundays and public holidays. Tickets cost 6€ and the journey ends in Plaça Catalunya in the centre of Barcelona.\nTaxi: From the airport, you can take one of Barcelona\u0026rsquo;s typical black and yellow taxis. Taxis may not take more than four passengers. Unoccupied taxis display a green light and have a clearly visible sign showing LIBRE or LLIURE. The trip to Sants train station costs approximately €16 and trips to other destinations in the city cost approximately €18.\n**Train and bus: **Barcelona has two international train stations: Sants and França. Bus companies have different points of arrival in different parts of the city. You can find detailed information in the following link: http://www.barcelona-airport.com/eng/transport_eng.htm The locations of the airport and the city centre\nAccomodation The following hotels are recommended. The two first are located near the UPC campus and they take 10-15 min by foot to reach the TUC meeting location. The two last are located at the city center. They require about 30 min (taking metro L3 at plaça Catalunya) to reach the TUC meeting location. You can see the hotel locations in this google map.\nHotel Husa Pedralbes\nFontcoberta, 4\n8034 Barcelona\n932 037 112\nhttp://www.hotelhusapedralbes.com/\n Hotel Bonanova Park\nCapita Arenas, 51\n08034 Barcelona\n932 04 09 00\nhttp://www.hotelbonanovapark.com/\n Hotel Inglaterra Barcelona\nCarrer de Pelai, 14\n08001 Barcelona\n935 05 11 00\nhttp://www.hotel-inglaterra.com/\n Hotel Jazz\nCarrer de Pelai, 3\n08001 Barcelona\n935 52 96 96\nhttp://www.hoteljazz.com/\nIn addition to the hotels above, there is the possibility to stay at the \u0026ldquo;Torre Girona\u0026rdquo; residence. It is the closest and cheapest option available. You can find detailed information here. Basically, it costs 53 euros for a single room and 60 to 88 euros for a double room depending if it is occupied by one or two people. Currently, there are 20 individual and 20 double rooms free for these days. If anyone is interested in this option, you should send an email to torregirona@resa.es asking for a reservation.\nTorre Girona Residence Hall\nPasseig dels Tillers, 19\n08034 Barcelona\nTelephone: 0034 93 390 43 00\nFax: 0034 93 205 69 10\nE-mail: torregirona@resa.es\n Travel costs There is some small budget available that can be used to assist some attendees that are otherwise unable to fund their trip. Please contact us using the following email address if you would like more information: ldbcgrants AT ac DOT upc DOT edu\n Attachments: upc_map.jpg (image/jpeg)\nbarcelona_map.jpg (image/jpeg)\nbus_map.jpg (image/jpeg)\nmeeting_room_map.jpg (image/jpeg)\nldbc_tuc_19-11-2012_Ontotext-Barry_Bishop.pdf (application/force-download)\nldbc_tuc_19-11-2012_Yale-Lec_Maj.pdf (application/force-download)\nldbc_tuc_19-11-2012_CWI-Peter_Boncz.pptx (application/vnd.openxmlformats-officedocument.presentationml.presentation)\nldbc_tuc_19-11-2012_BBC-Jem_Rayfield.pdf (application/force-download)\nERA7_BIOINFORMATICS_Bio4j_LDBC_TUC_meeting_Nov_2012.pptx (application/vnd.openxmlformats-officedocument.presentationml.presentation)\nldbc_tuc_19-11-2012_Questionnaire.pdf (application/pdf)\nldbc_tuc_19-11-2012_social_network_taskforce_proposal.pdf (application/pdf)\nldbc_tuc_19-11-2012_Elsevier-Alan_Yagoda.pptx (application/vnd.openxmlformats-officedocument.presentationml.presentation)\nldbc_tuc_19-11-2012_RJLee-David_Neuer.pptx (application/vnd.openxmlformats-officedocument.presentationml.presentation)\nldbc_tuc_19-11-2012_Press_Association-Jarred_McGinnis.pptx (application/vnd.openxmlformats-officedocument.presentationml.presentation)\n","permalink":"https://ldbc.github.io/event/first-tuc-meeting/","tags":["TUC Meeting"],"title":"First TUC Meeting"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confcikm-gubichev-n-12/","tags":[],"title":"Fast approximation of Steiner trees in large graphs"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confedbt-tsialiamanis-sfcb-12/","tags":[],"title":"Heuristics-based query optimisation for SPARQL"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confsemweb-phuoc-dpbef-12/","tags":[],"title":"Linked Stream Data Processing Engines: Facts and Figures"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-conftpctc-pham-be-12/","tags":[],"title":"S3G2: A Scalable Structure-Correlated Social Graph Generator"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-journalsdebu-erling-12/","tags":[],"title":"Virtuoso, a Hybrid RDBMS/Graph Column Store"},{"categories":null,"contents":"This file is here so that Hugo generates the member page.\n","permalink":"https://ldbc.github.io/current-members/placeholder/","tags":null,"title":""},{"categories":null,"contents":"","permalink":"https://ldbc.github.io/pages/page-name/","tags":null,"title":""},{"categories":null,"contents":"Benefits The benefits of LDBC membership are:\n Access to the internal LDBC information via its wiki and mailing lists, which includes access to discussion documents, graph user scenario\u0026rsquo;s, datasets, draft benchmark specifications and software, results and discussions. Right to membership of LDBC task forces. Currently, there are task forces on the Semantic Publishing Benchmark, the Social Network Benchmark Interactive, BI and the Graphalytics workloads, as well as the Graph Query Language task force. Access to research resources at academic partners. This includes the ability to look into research agenda\u0026rsquo;s provide feedback and establish working relationships with students, as well as arrange targeted internships with MSc and PhD students provided by these partners.  Access fillable membership form here 2021 one-year membership dues  Individual Member: EUR 150 (reduced rate: EUR 10) Non-Profit Organization Member: EUR 1000 Company Member: EUR 2000 Sponsor Company Member: EUR 8000  ","permalink":"https://ldbc.github.io/becoming-a-member/","tags":null,"title":"Becoming a Member"},{"categories":null,"contents":"The Graphalytics benchmark is an industrial-grade benchmark for graph analysis platforms such as Giraph. It consists of six core algorithms, standard datasets, synthetic dataset generators, and reference outputs, enabling the objective comparison of graph analysis platforms. The design of the benchmark process takes into account that graph-processing is impeded by three dimensions of diversity: platform, algorithms and dataset. The benchmark harness consists of a core component, which is extendable by a driver for each different platform implementation. The choice of the six algorithms:\n breadth-first search, PageRank, weakly connected components, community detection using label propagation, local clustering coefficient, and single-source shortest paths  was carefully motivated, using the LDBC TUC and extensive literature surveys to ensure good coverage of scenarios. The standard datasets include both real and synthetic datasets, which are classified into intuitive “T-shirt” sizes (e.g., S, M, L, XL).\nEach experiment set in Graphalytics consists of multiple platform runs (a platform executes an algorithm on a dataset), and diverse set of experiments are carried out to evaluate different performance characteristics of a system-under-test.\nAll completed benchmarks must go through a strict validation process to ensure the integrity of the performance results.\nThe Graphalytics benchmarking process is made future-proof, through a renewal process that takes place regularly to ensure that the benchmark process meets the state-of-the-art development in the field of graph analytics.\nTo enhance the depth of the benchmark process, Graphalytics also facilitates a plugin-architecture, which allows external software tools to be added to the benchmark harness. For instance, it is possible to also use SNB Datagen (the data generator of the LDBC Social Network Benchmark), an advanced synthetic dataset generator to create synthetic graphs for custom test scenarios, or to use Granula, a fine-grained performance evaluation tool to obtain enriched performance results.\nThe development of Graphalytics is supported by many active vendors in the field of large-scale graph analytics. Currently, Graphalytics already facilitates benchmark for a large number of graph analytics platforms, such as Giraph, GraphX, GraphMat, OpenG, and PGXD, allowing comparison of the state-of-the-art system performance of both community-driven and industrial-driven platforms. To get started, the details of the Graphalyics documentation and its software components are described below.\n","permalink":"https://ldbc.github.io/benchmarks/graphalytics/","tags":null,"title":"LDBC Graphalytics Benchmark (LDBC-Graphalytics)"},{"categories":null,"contents":"The Semantic Publishing Benchmark (SPB) is an LDBC benchmark for testing the performance of RDF engines inspired by the Media/Publishing industry. In particular, LDBC worked with British Broadcasting Corporation BBC to define this benchmark, for which BBC donated workloads, ontologies and data. The publishing industry is an area where significant adoption of RDF is taking place.\nThere have been many academic benchmarks for RDF but none of these are truly industrial-grade. The SPB combines a set of complex queries under inference with continuous updates and special failover tests for systems implementing replication.\nSPB performance is measured by producing a workload of CRUD (Create, Read, Update, Delete) operations which are executed simultaneously. The benchmark offers a data generator that uses real reference data to produce datasets of various sizes and tests the scalability aspect of RDF systems. The benchmark workload consists of (a) editorial operations that add new data, alter or delete existing (b) aggregation operations that retrieve content according to various criteria. The benchmark also tests conformance for various rules inside the OWL2-RL rule-set.\nThe SPB specification contains the description of the benchmark and the data generator and all information about its software components can be found on the SPB developer page.\nSemantic Publishing Benchmark (SPB) Audited Results for Scale Factors SF1 - 64M, SF3 - 256M and SF5 - 1G triples are shown below.     Scale Factor Interactive (Q/s) Updates (ops/sec) Analytical Cost Software Hardware Test Sponsor Date Full Disclosure Report     1 100.85 10.19 n.a. €37,504 GraphDB EE6.2 Xeon1650v3 6-core 3.5Ghz 96GB RAM ONTOTEXT AD 2015/04/26 Full Disclosure Report   1 142.7588 10.6725 n.a €35,323 GraphDB SE 6.3 alpha CPU Intel Xeon E5-1650 v3 3.5Ghz,15MB L3 cache, s2011 ONTOTEXT AD 2015/06/10 Full Disclosure Report   3 29.90 9.50 n.a. €37,504 GraphDB EE6.2 Xeon1650v3 6-core 3.5Ghz 96GB RAM ONTOTEXT AD 2015/04/26 Full Disclosure Report   3 54.6364 9.4967 n.a €35,323 GraphDB SE 6.3 alpha CPU Intel Xeon E5-1650 v3 3.5Ghz,15MB L3 cache, s2011 ONTOTEXT AD 2015/06/10 Full Disclosure Report   1 149.0385 156.8325 n.a. $20,213 (€17,801 rate of 21/06/2015) Virtuoso Opensource Version 7.50.3213 Intel Xeon E5-2630, 6x 2.30GHz, Sockel 2011, boxed, 192 GB RAM OpenLink Software 2015/06/09 Full Disclosure Report   3 80.6158 92.7072 n.a. $20,213 (€17,801 rate of 21/06/2015) Virtuoso Opensource Version 7.50.3213 Intel Xeon E5-2630, 6x 2.30GHz, Sockel 2011, boxed, 192 GB RAM OpenLink Software 2015/06/09 Full Disclosure Report   3 115.3838 109.8517 n.a. $24,528 (€21,601 rate 21/06/2015) Virtuoso Opensource Version 7.50.3213 Amazon EC2, r3.8xlarge OpenLink Software 2015/06/09 Full Disclosure Report   5 32.2789 72.7192 n.a. $20,213 (€17,801 rate 21/06/2015) Virtuoso Opensource Version 7.50.3213 Intel Xeon E5-2630, 6x 2.30GHz, Sockel 2011, boxed, 192 GB RAM OpenLink Software 2015/06/09 Full Disclosure Report   5 45.8101 55.4467 n.a $24,528 (€21,601 rate 21/06/2015) Virtuoso Opensource Version 7.50.3213 Amazon EC2, r3.8xlarge OpenLink Software 2015/06/10 Full Disclosure Report    ","permalink":"https://ldbc.github.io/benchmarks/spb/","tags":null,"title":"LDBC Semantic Publishing Benchmark (LDBC-SPB)"},{"categories":null,"contents":"The Social Network Benchmark consists in fact of three distinct benchmarks on a common dataset, since there are three different workloads. Each workload produces a single metric for performance at the given scale and a price/performance metric at the scale. The full disclosure further breaks down the composition of the metric into its constituent parts, e.g. single query execution times.\n The Social Network Benchmark's Interactive workload, a benchmark focusing on transactional graph processing with complex read queries that access the neighbourhood of a given node in the graph and update operations that continuously insert new data in the graph. The Social Network Benchmark's Business Intelligence workload, a benchmark that focuses on aggregation- and join-heavy complex queries touching a large portion of the graph with microbatches of insert/delete update operations. This workload is not yet finalized.  Social Network Benchmark Interactive (version 0.3.2) audited results TuGraph TuGraph was audited in July 2020. TuGraph is owned by Ant Group now.\n   SF Throughput (ops/sec) Cost Software Hardware Test Sponsor Date Full Disclosure Report     30 5,436.47 $280,650 TuGraph 1.10 AWS r5d.12xlarge instance, 48*Intel Xeon Platinum 8175M @ 2.5GHz, 374GB RAM FMA 2020/07/26 Full Disclosure Report   100 5,010.77 $280,650 TuGraph 1.10 AWS r5d.12xlarge instance, 48*Intel Xeon Platinum 8175M @ 2.5GHz, 374GB RAM FMA 2020/07/26 Full Disclosure Report   300 4,855.52 $280,650 TuGraph 1.10 AWS r5d.12xlarge instance, 48*Intel Xeon Platinum 8175M @ 2.5GHz, 374GB RAM FMA 2020/07/26 Full Disclosure Report    Supplementary material for the TuGraph audits:\n Executive summary Signatures Attachments  Social Network Benchmark Interactive (version 0.2.2) audited results    SF Throughput (ops/sec) Cost Software Hardware Test Sponsor Date Full Disclosure Report     10 101.20 €30,427 Sparksee 5.1.1 2*Xeon 2630v3 8-core 2.4GHz, 256GB RAM Sparsity Technologies SA 2015/04/27 Full Disclosure Report   30 1287.17 €20,212 Virtuoso 07.50.3213 v7fasttrack 2*Xeon2630 6-core 2.4GHz, 192GB RAM OpenLink Software 2015/04/27 Full Disclosure Report   30 86.50 €30,427 Sparksee 5.1.1 2*Xeon 2630v3 8-core 2.4GHz, 256GB RAM Sparsity Technologies SA 2015/04/27 Full Disclosure Report   100 1200.00 €20,212 Virtuoso 07.50.3213 v7fasttrack 2*Xeon2630 6-core 2.4GHz, 192GB RAM OpenLink Software 2015/04/27 Full Disclosure Report   100 81.70 €37,927 Sparksee 5.1.1 2*Xeon 2630v3 8-core 2.4GHz, 256GB RAM Sparsity Technologies SA 2015/04/27 Full Disclosure Report   300 635 €20,212 Virtuoso 07.50.3213 v7fasttrack 2*Xeon2630 6-core 2.4GHz, 192GB RAM OpenLink Software 2015/04/27 Full Disclosure Report    ","permalink":"https://ldbc.github.io/benchmarks/snb/","tags":null,"title":"LDBC Social Network Benchmark (LDBC-SNB)"}]