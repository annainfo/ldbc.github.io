[{"categories":null,"contents":"LDBC\u0026rsquo;s Social Network Benchmark [4] (LDBC SNB) is an industrial and academic initiative, formed by principal actors in the field of graph-like data management. Its goal is to define a framework where different graph-based technologies can be fairly tested and compared, that can drive the identification of systems' bottlenecks and required functionalities, and can help researchers open new frontiers in high-performance graph data management.\nLDBC SNB provides Datagen (Data Generator), which produces synthetic datasets, mimicking a social network\u0026rsquo;s activity during a period of time. Datagen is defined by the charasteristics of realism, scalability, determinism and usability. To address scalability in particular, Datagen has been implemented on the MapReduce computation model to enable scaling out across a distributed cluster. However, since its inception in the early 2010s there has been a tremendous amount of development in the big data landscape, both in the sophistication of distributed processing platforms, as well as public cloud IaaS offerings. In the light of this, we should reevaluate this implementation, and in particular, investigate if Apache Spark would be a more cost-effective solution for generating datasets on the scale of tens of terabytes, on public clouds such as Amazon Web Services (AWS).\nOverview The benchmark\u0026rsquo;s specification describes a social network data model which divides its components into two broad categories: static and dynamic. The dynamic element consists of an evolving network where people make friends, post in forums, comment or like each others posts, etc. In contrast, the static component contains related attributes such as countries, universities and organizations and are fixed values. For the detailed specifications of the benchmark and the Datagen component, see References.\nDatasets are generated in a multi-stage process captured as a sequence of MapReduce steps (shown in the diagram below).\nFigure 1. LDBC SNB Datagen Process on Hadoop\nIn the initialization phase dictionaries are populated and distributions are initialized. In the first generation phase persons are synthesized, then relationships are wired between them along 3 dimensions (university, interest and random). After merging the graph of person relationships, the resulting dataset is output. Following this, activities such as forum posts, comments, likes and photos are generated and output. Finally, the static components are output.\nNote: The diagram shows the call sequence as implemented. All steps are sequential \u0026ndash; including the relationship generation \u0026ndash;, even in cases when the data dependencies would allow for parallelization.\nEntities are generated by procedural Java code and are represented as POJOs in memory and as sequence files on disk. Most entities follow a shallow representation, i.e foreign keys (in relational terms) are mapped to integer ids, which makes serialization straightforward.1 A notable exception is the Knows edge which contains only the target vertex, and is used as a navigation property on the source Person. The target Person is replaced with only the foreign key augmented with some additional information in order to keep the structure free of cycles. Needless to say, this edge as property representation makes the data harder to handle in SQL than it would be with a flat join table.\nEntity generation amounts to roughly one fifth of the main codebase. It generates properties drawn from several random distributions using mutable pRNGs. Determinism is achieved by initializing the pRNGs to seeds that are fully defined by the configuration with constants, and otherwise having no external state in the logic.2\nSerialization is done by hand-written serializers for the supported output formats (e.g. CSV) and comprises just a bit less than one third of the main codebase. Most of the output is created by directly interacting with low-level HDFS file streams. Ideally, this code should be migrated to higher-level writers that handle faults and give consistent results when the task has to be restarted.\nMotivations for the migration The application is written using Hadoop MapReduce, which is now largely superseded by more modern distributed batch processing platforms, notably Apache Spark. For this reason, it was proposed to migrate Datagen to Spark. The migration provides the following benefits:\n  Better memory utilization: MapReduce is disk-oriented, i.e. it writes the output to disk after each reduce stage which is then read by the next MapReduce job. As public clouds provide virtual machines with sufficient RAM to encapsulate any generated dataset, time and money are wasted by the overhead this unnecessary disk I/O incurs. Instead, the intermediate results should be cached in memory where possible. The lack of support for this is a well-known limitation of MapReduce.\n  Smaller codebase: The Hadoop MapReduce library is fairly ceremonial and boilerplatey. Spark provides a higher-level abstraction that is simpler to work with, while still providing enough control on the lower-level details required for this workload.\n  Small entry cost: Spark and MapReduce are very close conceptually, they both utilise HDFS under the hood, and run on the JVM. This means that a large chunk of the existing code can be reused, and migration to Spark can, therefore, be completed with relatively small effort. Additionally, MapReduce and Spark jobs can be run on AWS EMR using basically the same HW/SW configuration, which facilitates straightforward performance comparisons.\n  Incremental improvements: Spark exposes multiple APIs for different workloads and operating on different levels of abstraction. Datagen may initially utilise the lower-level, Java-oriented RDDs (which offer the clearest 1 to 1 mapping when coming from MapReduce) and gradually move towards DataFrames to support Parquet output in the serializers and maybe unlock some SQL optimization capabilities in the generators later down the road.\n  OSS, commodity: Spark is one of the most widely used open-source big data platforms. Every major public cloud provides a managed offering for Spark. Together these mean that the migration increases the approachability and portability of the code.\n  First steps The first milestone is a successful run of LDBC Datagen on Spark while making the minimum necessary amount of code alterations. This entails the migration of the Hadoop wrappers around the generators and serializers. The following bullet-points summarize the key notions that cropped up during the process.\n  Use your memory: A strong focus was placed on keeping the call sequence intact, so that the migrated code evaluates the same steps in the same order, but with data passed as RDDs. It was hypothesised that the required data could be either cached in memory entirely at all times, or if not, regenerating them would still be faster than involving the disk I/O loop (e.g by using MEMORY_AND_DISK). In short, the default caching strategy was used everywhere.\n  Regression tests: Lacking tests apart from an id uniqueness check, meant there were no means to detect bugs introduced by the migration. Designing and implementing a comprehensive test suite was out of scope, so instead, regression testing was utilised, with the MapReduce output as the baseline. The original output mostly consists of Hadoop sequence files which can be read into Spark, allowing comparisons to be drawn with the output from the RDD produced by the migrated code.\n  Thread-safety concerns: Soon after migrating the first generator and running the regression tests, there were clear discrepancies in the output. These only surfaced when the parallelization level was set greater than 1. This indicated the presence of potential race conditions. Thread-safety wasn\u0026rsquo;t a concern in the original implementation due to the fact that MapReduce doesn\u0026rsquo;t use thread-based parallelization for mappers and reducers.3 In Spark however, tasks are executed by parallel threads in the same JVM application, so the code is required to be thread-safe. After some debugging, a bug was discovered originating from the shared use of java.text.SimpleDateFormat (notoriously known to be not thread-safe) in the serializers. This was resolved simply by changing to java.time.format.DateTimeFormatter. There were multiple instances of some static field on an object being mutated concurrently. In some cases this was a temporary buffer and was easily resolved by making it an instance variable. In another case a shared context variable was used, which was resolved by passing dedicated instances as function arguments. Sadly, the Java language has the same syntax for accessing locals, fields and statics, 4 which makes it somewhat harder to find potential unguarded shared variables.\n  Case study: Person ranking Migrating was rather straightforward, however, the so-called person ranking step required some thought. The goal of this step is to organize persons so that similar ones appear close to each other in a deterministic order. This provides a scalable way to cluster persons according to a similarity metric, as introduced in the S3G2 paper [3].\nThe original MapReduce version Figure 2. Diagram of the MapReduce code for ranking persons\nThe implementation, shown in pseudocode above, works as follows:\n The equivalence keys are mapped to each person and fed into TotalOrderPartitioner which maintains an order sensitive partitioning while trying to emit more or less equal sized groups to keep the data skew low. The reducer keys the partitions with its own task id and a counter variable which has been initialized to zero and incremented on each person, establishing a local ranking inside the group. The final state of the counter (which is the total number of persons in that group) is saved to a separate \u0026ldquo;side-channel\u0026rdquo; file upon the completion of a reduce task. In a consecutive reduce-only stage, the global order is established by reading all of these previously emitted count files in the order of their partition number in each reducer, then creating an ordered map from each partition number to the corresponding cumulative count of persons found in all preceding ones. This is done in the setup phase. In the reduce function, the respective count is incremented and assigned to each person.  Once this ranking is done, the whole range is sliced up into equally sized blocks, which are processed independently. For example, when wiring relationships between persons, only those appearing in the same block are considered.\nThe migrated version Spark provides a sortBy function which takes care of the first step above in a single line. The gist of the problem remains collecting the partition sizes and making them available in a later step. While the MapReduce version uses a side output, in Spark the partition sizes are collected in a separate job and passed into the next phase using a broadcast variable. The resulting code size is a fraction of the original one.\nBenchmarks Benchmarks were carried out on AWS EMR, originally utilising i3.xlarge instances because of their fast NVMe SSD storage and ample amount of RAM.\nThe application parameter hadoop.numThreads controls the number of reduce threads in each Hadoop job for the MapReduce version and the number of partitions in the serialization jobs in the Spark one. For MapReduce, this was set to n_nodes, i.e. the number of machines; experimentation yield slowdowns for higher values. The Spark version on the other hand, performed better with this parameter set to n_nodes * v_cpu. The scale factor (SF) parameter determines the output size. It is defined so that one SF unit generates around 1 GB of data. That is, SF10 generates around 10 GB, SF30 around 30 GB, etc. It should be noted however, that incidentally the output was only 60% of this in these experiments, stemming from two reasons. One, update stream serialization was not migrated to Spark, due to problems in the original implementation. Of course, for the purpose of faithful comparison the corresponding code was removed from the MapReduce version as well before executing the benchmarks. This explains a 10% reduction from the expected size. The rest can be attributed to incorrectly tuned parameters.5 The MapReduce results were as follows:\n   SF workers Platform Instance Type runtime (min) runtime * worker/SF (min)     10 1 MapReduce i3.xlarge 16 1.60   30 1 MapReduce i3.xlarge 34 1.13   100 3 MapReduce i3.xlarge 40 1.20   300 9 MapReduce i3.xlarge 44 1.32    It can be observed that the runtime per scale factor only increases slowly, which is good. The metric charts show an underutilized, bursty CPU. The bursts are supposedly interrupted by the disk I/O parts when the node is writing the results of a completed job. It can also be seen that the memory only starts to get consumed after 10 minutes of the run have passed.\nFigure 3. CPU Load for the Map Reduce cluster is bursty and less than 50% on average (SF100, 2nd graph shows master)\nFigure 4. The job only starts to consume memory when already 10 minutes into the run (SF100, 2nd graph shows master)\nLet\u0026rsquo;s see how Spark fares.\n   SF workers Platform Instance Type runtime (min) runtime * worker/SF (min)     10 1 Spark i3.xlarge 10 1.00   30 1 Spark i3.xlarge 21 0.70   100 3 Spark i3.xlarge 27 0.81   300 9 Spark i3.xlarge 36 1.08   1000 30 Spark i3.xlarge 47 1.41   3000 90 Spark i3.xlarge 47 1.41    A similar trend here, however the run times are around 70% of the MapReduce version. It can be seen that the larger scale factors (SF1000 and SF3000) yielded a long runtime than expected. On the metric charts of SF100 the CPU shows full utilization, except at the end, when the results are serialized in one go and the CPU is basically idle (the snapshot of the diagram doesn\u0026rsquo;t include this part unfortunately). Spark can be seen to have used up all memory pretty fast even in case of SF100. In case of SF1000 and SF3000, the nodes are running so low on memory that most probably some of the RDDs have to be calculated multiple times (no disk level serialization was used here), which seem to be the most plausible explanation for the slowdowns experienced. In fact, the OOM errors encountered when running SF3000 supports this hypothesis even further. It was thus proposed to scale up the RAM in the instances. The CPU utilization hints that adding some extra vCPUs as well can further yield speedup.\nFigure 5. Full CPU utilization for Spark (SF100, last graph shows master)\nFigure 6. Spark eats up memory fast (SF100, 2nd graph shows master)\ni3.2xlarge would have been the most straightforward option for scaling up the instances, however the humongous 1.9 TB disk of this image is completely unnecessary for the job. Instead the cheaper r5d.2xlarge instance was utilised, largely identical to i3.2xlarge, except it only has a 300 GB SSD.\n   SF workers Platform Instance Type runtime (min) runtime * worker/SF (min)     100 3 Spark r5d.2xlarge 16 0.48   300 9 Spark r5d.2xlarge 21 0.63   1000 30 Spark r5d.2xlarge 26 0.78   3000 90 Spark r5d.2xlarge 25 0.75   10000 303 Spark r5d.2xlarge 25 0.75    The last column clearly demonstrates our ability to keep the cost per scale factor unit constant.\nNext steps The next improvement is refactoring the serializers so they use Spark\u0026rsquo;s high-level writer facilities. The most compelling benefit is that it will make the jobs fault-tolerant, as Spark maintains the integrity of the output files in case the task that writes it fails. This makes Datagen more resilient and opens up the possibility to run on less reliable hardware configuration (e.g. EC2 spot nodes on AWS) for additional cost savings. They will supposedly also yield some speedup on the same cluster configuration.\nAs already mentioned, the migration of the update stream serialization was ignored due to problems with the original code. Ideally, they should be implemented with the new serializers.\nThe Spark migration also serves as an important building block for the next generation of LDBC benchmarks. As part of extending the SNB benchmark suite, the SNB task force has recently extended Datagen with support for generating delete operations [1]. The next step for the task force is to fine-tune the temporal distributions of these deletion operations to ensure that the emerging sequence of events is realistic, i.e. the emerging distribution resembles what a database system would experience when serving a real social network.\nAcknowledgements This work is based upon the work of Arnau Prat, Gábor Szárnyas, Ben Steer, Jack Waudby and other LDBC contributors. Thanks for your help and feedback!\nReferences [1] Supporting Dynamic Graphs and Temporal Entity Deletions in the LDBC Social Network Benchmark\u0026rsquo;s Data Generator\n[2] 9th TUC Meeting \u0026ndash; LDBC SNB Datagen Update \u0026ndash; Arnau Prat (UPC) - slides\n[3] S3G2: a Scalable Structure-correlated Social Graph Generator\n[4] The LDBC Social Network Benchmark\n[5] LDBC - LDBC GitHub organization\n  Also makes it easier to map to a tabular format thus it is a SQL friendly representation. \u0026#x21a9;\u0026#xfe0e;\n It\u0026rsquo;s hard to imagine this done declaratively in SQL. \u0026#x21a9;\u0026#xfe0e;\n Instead, multiple YARN containers have to be used if you want to parallelize on the same machine. \u0026#x21a9;\u0026#xfe0e;\n Although editors usually render these using different font styles. \u0026#x21a9;\u0026#xfe0e;\n With the addition of deletes, entities often get inserted and deleted during the simulation (which is normal in a social network). During serialization, we check for such entities and omit them. However we forgot to calculate this when determining the output size, which we will amend when tuning the distributions. \u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://ldbc.github.io/post/speeding-up-ldbc-snb-datagen/","tags":["LDBC Datagen","SNB"],"title":"Speeding Up LDBC SNB Datagen"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-journalscorrabs-2010-12243/","tags":[],"title":"An analysis of the SIGMOD 2014 Programming Contest: Complex queries on the LDBC social network graph"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confsigmod-waudby-sps-20/","tags":[],"title":"Supporting Dynamic Graphs and Temporal Entity Deletions in the LDBC Social Network Benchmark's Data Generator"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-journalscorrabs-2011-15028/","tags":[],"title":"The LDBC Graphalytics Benchmark"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-journalscorrabs-2001-02299/","tags":[],"title":"The LDBC Social Network Benchmark"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/tpctc-acid/","tags":[],"title":"Towards Testing ACID Compliance in the LDBC Social Network Benchmark"},{"categories":null,"contents":"Read more information and how to register HERE [CLOSED]\nCALL FOR CONTRIBUTIONS:\n[CLOSED] The LDBC meeting will feature updates on activities of LDBC, but is also seeking contributions by community members and graph practitioners and researchers. These proposals can be emailed to Peter Boncz (boncz@cwi.nl)\nFINAL PROGRAM (UPDATED):\n 08:30-10:30 LDBC Board Meeting (non-public) 10:30-11:00 Coffee 11:00-12:45 Session 1: Graph Benchmarks  11:00-11:05 Welcome \u0026amp; introduction 11:05-11:45 Gabor Szarnyas (BME), Benjamin Steer (QMUL), Jack Waudby (Newcastle University): Business Intelligence workload: Progress report and roadmap 11:45-12:00 Frank McSherry (Materialize): Experiences implementing LDBC queries in a dataflow system 12:00-12:25 Vasileios Trigonakis (Oracle): Evaluating a new distributed graph query engine with LDBC: Experiences and limitations 12:25-12:45 Ahmed Musaafir (VU Amsterdam): LDBC Graphalytics   12:45-14:00 Lunch 14:00-16:05 Session 2: Graph Query Languages  14:00-14:25 Juan Sequeda (Capsenta): Property Graph Schema Working Group: A progress report 4:25-14:50 Stefan Plantikow (Neo4j): GQL: Scope and features 14:50-15:15 Vasileios Trigonakis (Oracle): Property graph extensions for the SQL standard 15:15-15:40 Alin Deutsch (TigerGraph): Modern graph analytics support in GSQL, TigerGraph's query language 15:40-16:05 Jan Posiadała (Nodes and Edges, Poland): Executable semantics of graph query language.   16:05-16:30 Coffee 16:30-17:50 Session 3: Graph System Performance  16:30-16:50 Per Fuchs (CWI): Fast, scalable WCOJ graph-pattern matching on in-memory graphs in Spark 16:50-17:10 Semih Salihoglu (University of Waterloo): Optimizing subgraph queries with a mix of tradition and modernity 17:10-17:30 Roi Lipman (RedisGraph): Evaluating Cypher queries and procedures as algebraic operations within RedisGraph 17:30-17:50 Alexandru Uta (VU Amsterdam): Low-latency Spark queries on updatable data    ","permalink":"https://ldbc.github.io/event/twelfth-tuc-meeting-at-sigmod-2019-amsterdam/","tags":["TUC Meeting"],"title":"Twelfth TUC Meeting at SIGMOD 2019 Amsterdam"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confgrades-szarnyas-pampkeb-18/","tags":[],"title":"An early look at the LDBC Social Network Benchmark's Business Intelligence workload"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confsigmod-angles-abbfglpps-18/","tags":[],"title":"G-CORE: A Core for Future Graph Query Languages"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confgrades-leo-b-17/","tags":[],"title":"Extending SQL for Computing Shortest Paths"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confgrades-ngai-hhi-17/","tags":[],"title":"Granula: Toward Fine-grained Performance Analysis of Large-scale Graph Processing Platforms"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confgrades-prat-perez-gskdb-17/","tags":[],"title":"Towards a property graph generator for benchmarking"},{"categories":null,"contents":"LDBC is proud to announce the new LDBC Graphalytics Benchmark draft specification.\nLDBC Graphalytics is the first industry-grade graph data management benchmark for graph analysis platforms such as Giraph. It consists of six core algorithms, standard datasets, synthetic dataset generators, and reference outputs, enabling the objective comparison of graph analysis platforms. It has strong industry support from Oracle, Intel, Huawei and IBM, and was tested and optimized on the best industrial and open-source systems.\nTim Hegeman of TU Delft is today presenting the technical paper describing LDBC Graphalytics at the important VLDB (Very Large DataBases) conference in New Delhi, where his talk also marks the release by LDBC of Graphalytics as a benchmark draft. Practitioners are invited to read the PVLDB paper, download the software and try running it.\nLDBC is eager to use any feedback for its future adoption of LDBC Graphalytics.\nLearn more: http://ldbcouncil.org/ldbc-graphalytics\nGitHub: https://github.com/tudelft-atlarge/graphalytics\n","permalink":"https://ldbc.github.io/post/ldbc-is-proud-to-announce-the-new-ldbc-graphalytics-benchmark-draft-specification/","tags":["benchmarking","Graphalytics","TU Delft"],"title":"LDBC Is Proud to Announce the New LDBC Graphalytics Benchmark Draft Specification"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confsemweb-kotsev-mpefk-16/","tags":[],"title":"Benchmarking RDF Query Engines: The LDBC Semantic Publishing Benchmark"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-journalspvldb-iosup-hnhpmccsat-16/","tags":[],"title":"LDBC Graphalytics: A Benchmark for Large-Scale Graph Analysis on Parallel and Distributed Platforms"},{"categories":null,"contents":"Apache Flink [1] is an open source platform for distributed stream and batch data processing. Flink\u0026rsquo;s core is a streaming dataflow engine that provides data distribution, communication, and fault tolerance for distributed computations over data streams. Flink also builds batch processing on top of the streaming engine, overlaying native iteration support, managed memory, and program optimization.\nFlink offers multiple APIs to process data from various data sources (e.g. HDFS, HBase, Kafka and JDBC). The DataStream and DataSet APIs allow the user to apply general-purpose data operations, like map, reduce, groupBy and join, on streams and static data respectively. In addition, Flink provides libraries for machine learning (Flink ML), graph processing (Gelly) and SQL-like operations (Table). All APIs can be used together in a single Flink program which enables the definition of powerful analytical workflows and the implementation of distributed algorithms.\nThe following snippet shows how a wordcount program can be expressed in Flink using the DataSet API:\nDataSet\u0026lt;String\u0026gt; text = env.fromElements( \u0026#34;He who controls the past controls the future.\u0026#34;, \u0026#34;He who controls the present controls the past.\u0026#34;); DataSet\u0026lt;Tuple2\u0026lt;String, Integer\u0026gt;\u0026gt; wordCounts = text .flatMap(new LineSplitter()) // splits the line and outputs (word,1)  tuples.groupBy(0) // group by word  .sum(1); // sum the 1\u0026#39;s  wordCounts.print(); At the Leipzig University, we use Apache Flink as execution layer for our graph analytics platform Gradoop [2]. The LDBC datagen helps us to evaluate the scalability of our algorithms and operators in a distributed execution environment. To use the generated graph data in Flink, we wrote a tool that transforms the LDBC output files into Flink data sets for further processing [3]. Using the class LDBCToFlink, LDBC output files can be read directly from HDFS or from the local file system:\nfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); final LDBCToFlink ldbcToFlink = new LDBCToFlink( \u0026#34;hdfs:///ldbc_snb_datagen/social_network\u0026#34;, // or \u0026#34;/path/to/social_network\u0026#34;  env); DataSet\u0026lt;LDBCVertex\u0026gt; vertices = ldbcToFlink.getVertices(); DataSet\u0026lt;LDBCEdge\u0026gt; edges = ldbcToFlink.getEdges(); The tuple classes LDBCVertex and LDBCEdge hold the information generated by the LDBC datagen and are created directly from its output files. During the transformation process, globally unique vertex identifiers are created based on the LDBC identifier and the vertex class. When reading edge files, source and target vertex identifiers are computed in the same way to ensure consistent linking between vertices.\nEach LDBCVertex instance contains:\n an identifier, which is unique among all vertices a vertex label (e.g. Person, Comment) a key-value map of properties including also multivalued properties (e.g. Person.email)  Each LDBCEdge instance contains:\n an identifier, which is unique among all edges an edge label (e.g. knows, likes) a source vertex identifier a target vertex identifier a key-value map of properties  The resulting datasets can be used by the DataSet API and all libraries that are built on top of it (i.e. Flink ML, Gelly and Table). In the following example, we load the LDBC graph from HDFS, filter vertices with the label Person and edges with the label knows and use Gelly to compute the connected components of that subgraph. The full source code is available on GitHub [4].\nfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); final LDBCToFlink ldbcToFlink = new LDBCToFlink( \u0026#34;/home/s1ck/Devel/Java/ldbc_snb_datagen/social_network\u0026#34;, env); // filter vertices with label “Person” DataSet\u0026lt;LDBCVertex\u0026gt; ldbcVertices = ldbcToFlink.getVertices() .filter(new VertexLabelFilter(LDBCConstants.VERTEX_CLASS_PERSON)); // filter edges with label “knows” DataSet\u0026lt;LDBCEdge\u0026gt; ldbcEdges = ldbcToFlink.getEdges() .filter(new EdgeLabelFilter(LDBCConstants.EDGE_CLASS_KNOWS)); // create Gelly vertices suitable for connected components DataSet\u0026lt;Vertex\u0026lt;Long, Long\u0026gt;\u0026gt; vertices = ldbcVertices.map(new VertexInitializer()); // create Gelly edges suitable for connected components DataSet\u0026lt;Edge\u0026lt;Long, NullValue\u0026gt;\u0026gt; edges = ldbcEdges.map(new EdgeInitializer()); // create Gelly graph Graph\u0026lt;Long, Long, NullValue\u0026gt; g = Graph.fromDataSet(vertices, edges, env); // run connected components on the subgraph for 10 iterations DataSet\u0026lt;Vertex\u0026lt;Long, Long\u0026gt;\u0026gt; components = g.run(new ConnectedComponents\u0026lt;Long, NullValue\u0026gt;(10)); // print the component id of the first 10 vertices components.first(10).print(); The ldbc-flink-import tool is available on Github [3] and licensed under the GNU GPLv3. If you have any questions regarding the tool please feel free to contact me on GitHub. If you find bugs or have any ideas for improvements, please create an issue or a pull request.\nIf you want to learn more about Apache Flink, a good starting point is the main documentation [5] and if you have any question feel free to ask the official mailing lists. There is also a nice set of videos [6] available from the latest Flink Forward conference.\nReferences [1] http://flink.apache.org/\n[2] https://github.com/dbs-leipzig/gradoop\n[3] https://github.com/s1ck/ldbc-flink-import\n[4] https://gist.github.com/s1ck/b33e6a4874c15c35cd16\n[5] https://ci.apache.org/projects/flink/flink-docs-release-0.10/\n[6] https://www.youtube.com/channel/UCY8_lgiZLZErZPF47a2hXMA\n","permalink":"https://ldbc.github.io/post/ldbc-and-apache-flink/","tags":["Apache Flink","Flink","datagen","DataSet API","LDBC Datagen"],"title":"LDBC and Apache Flink"},{"categories":null,"contents":"The number of datasets published in the Web of Data as part of the Linked Data Cloud is constantly increasing. The Linked Data paradigm is based on the unconstrained publication of information by different publishers, and the interlinking of web resources through “same-as” links which specify that two URIs correspond to the same real world object. In the vast number of data sources participating in the Linked Data Cloud, this information is not explicitly stated but is discovered using instance matching techniques and tools. Instance matching is also known as record linkage [[1]], duplicate detection [2], entity resolution [3] and object identification [4].\nFor instance, a search in Geonames (http://www.geonames.org/) for \u0026ldquo;Athens\u0026rdquo; would return a resource (i.e., URI) accompanied with a map of the area and information about the place; additional information for the city of Athens can be found in other datasets such as for instance DBpedia (http://dbpedia.org/) or Open Government Datasets (http://data.gov.gr/). To exploit all obtain all necessary information about the city of Athens we need to establish that the retrieved resources refer to the same real world object.\nWeb resources are published by \u0026ldquo;autonomous agents\u0026rdquo; who choose their preferred information representation or the one that best fits the application of interest. Furthermore, different representations of the same real world entity are due to data acquisition errors or different acquisition techniques used to process scientific data. Moreover, real world entities evolve and change over time, and sources need to keep track of these developments, a task that is very hard and often not possible. Finally, when integrating data from multiple sources, the process itself may add new erroneous data. Clearly, these reasons are not limited to problems that did arise in the era of Web Data, it is thus not surprising that instance matching systems have been around for several years [2][5].\nIt is though essential at this point to develop, along with instance and entity matching systems, instance matching benchmarks to determine the weak and strong points of those systems, as well as their overall quality in order to support users in deciding the system to use for their needs. Hence, well defined, and good quality benchmarks are important for comparing the performance of the available or under development instance matching systems. Benchmarks are used not only to inform users of the strengths and weaknesses of systems, but also to motivate developers, researchers and technology vendors to deal with the weak points of their systems and to ameliorate their performance and functionality. They are also useful for identifying the settings in which each of the systems has optimal performance. Benchmarking aims at providing an objective basis for such assessments.\nAn instance matching benchmark for Linked Data consists of a source and target dataset implementing a set of test-cases, where each test case addresses a different kind of requirement regarding instance matching, a ground truth or gold standard and finally the evaluation metrics used to assess the benchmark.\nDatasets are the raw material of a benchmark. A benchmark comprises of a source and target dataset and the objective of an instance matching system is to discover the matches of the two. Datasets are characterized by (a) their nature (real or synthetic), (b) the schemas/ontologies they use, (c) their domains, (d) the languages they are written in, and (e) the variations/heterogeneities of the datasets. Real datasets are widely used in benchmarks since they offer realistic conditions for heterogeneity problems and they have realistic distributions. Synthetic datasets are generated using automated data generators and are useful because they offer fully controlled test conditions, have accurate gold standards and allow setting the focus on specific types of heterogeneity problems in a systematic manner\nDatasets (and benchmarks) may contain different kinds of variations that correspond to different test cases. According to Ferrara et.al. [6][7], three kinds of variations exist for Linked Data, namely data variations, structural variations and logical variations. The first refers mainly to differences due to typographical errors, differences in the employed data formats, language etc. The second refers to the differences in the structure of the employed Linked Data schemas. Finally, the third type derives from the use of semantically rich RDF and OWL constructs that enable one to define hierarchies and equivalence of classes and properties, (in)equality of instances, complex class definitions through union and intersection among others.\nThe common case in real benchmarks is that the datasets to be matched contain different kinds (combinations) of variations. On the other hand, synthetic datasets may be purposefully designed to contain specific types (or combinations) of variations (e.g., only structural), or may be more general in an effort to illustrate all the common cases of discrepancies that appear in reality between individual descriptions.\nThe gold standard is considered as the “correct answer sheet” of the benchmark, and is used to judge the completeness and soundness of the result sets of the benchmarked systems. For instance matching benchmarks employing synthetic datasets, the gold standard is always automatically generated, as the errors (variations) that are added into the datasets are known and systematically created. When it comes to real datasets, the gold standard can be either manually curated or (semi-) automatically generated. In the first case, domain experts manually mark the matches between the datasets, whereas in the second, supervised and crowdsourcing techniques aid the process of finding the matches, a process that is often time consuming and error prone.\nLast, an instance matching benchmark uses evaluation metrics to determine and assess the systems’ output quality and performance. For instance matching tools, performance is not a critical aspect. On the other hand, an instance matching tool should return all and only the correct answers. So, what matters most is returning the relevant matches, rather than returning them quickly. For this reason, the evaluation metrics that are dominantly employed for instance matching benchmarks are the standard precision, recall and f-measure metrics.\nReferences [1] Li, C., Jin, L., and Mehrotra, S. (2006) Supporting efficient record linkage for large data sets using mapping techniques. WWW 2006.\n[2] Dragisic, Z., Eckert, K., Euzenat, J., Faria, D., Ferrara, A., Granada, R., Ivanova, V.,Jimenez-Ruiz, E., Oskar Kempf, A., Lambrix, P., Montanelli, S., Paulheim, H., Ritze, D., Shvaiko, P., Solimando, A., Trojahn, C., Zamaza, O., and Cuenca Grau, B. (2014) Results of the Ontology Alignment Evaluation Initiative 2014. Proc. 9th ISWC workshop on ontology matching (OM 2014).\n[3] Bhattacharya, I. and Getoor, L. (2006) Entity resolution in graphs. Mining Graph Data. Wiley and Sons 2006.\n[4] Noessner, J., Niepert, M., Meilicke, C., and Stuckenschmidt, H. (2010) Leveraging Terminological Structure for Object Reconciliation. In ESWC 2010.\n[5] Flouris, G., Manakanatas, D., Kondylakis, H., Plexousakis, D., Antoniou, G. Ontology Change: Classification and Survey (2008) Knowledge Engineering Review (KER 2008), pages 117-152.\n[6] Ferrara, A., Lorusso, D., Montanelli, S., and Varese, G. (2008) Towards a Benchmark for Instance Matching. Proc. 3th ISWC workshop on ontology matching (OM 2008).\n[7] Ferrara, A., Montanelli, S., Noessner, J., and Stuckenschmidt, H. (2011) Benchmarking Matching Applications on the Semantic Web. In ESWC, 2011.\n","permalink":"https://ldbc.github.io/post/elements-of-instance-matching-benchmarks-a-short-overview/","tags":["instance","matching","benchmarks","SPB"],"title":"Elements of Instance Matching Benchmarks: a Short Overview"},{"categories":null,"contents":"In this post we will look at running the LDBC SNB on Virtuoso.\nFirst, let\u0026rsquo;s recap what the benchmark is about:\n fairly frequent short updates, with no update contention worth mentioning short random lookups medium complex queries centered around a person\u0026rsquo;s social environment  The updates exist so as to invalidate strategies that rely too heavily on precomputation. The short lookups exist for the sake of realism; after all, an online social application does lookups for the most part. The medium complex queries are to challenge the DBMS.\nThe DBMS challenges have to do firstly with query optimization, and secondly with execution with a lot of non-local random access patterns. Query optimization is not a requirement, per se, since imperative implementations are allowed, but we will see that these are no more free of the laws of nature than the declarative ones.\nThe workload is arbitrarily parallel, so intra-query parallelization is not particularly useful, if also not harmful. There are latency constraints on operations which strongly encourage implementations to stay within a predictable time envelope regardless of specific query parameters. The parameters are a combination of person and date range, and sometimes tags or countries. The hardest queries have the potential to access all content created by people within 2 steps of a central person, so possibly thousands of people, times 2000 posts per person, times up to 4 tags per post. We are talking in the millions of key lookups, aiming for sub-second single-threaded execution.\nThe test system is the same as used in the TPC-H series: dual Xeon E5-2630, 2x6 cores x 2 threads, 2.3GHz, 192 GB RAM. The software is the feature/analytics branch of v7fasttrack, available from www.github.com.\nThe dataset is the SNB 300G set, with:\n   1,136,127 persons     125,249,604 knows edges   847,886,644 posts, including replies   1,145,893,841 tags of posts or replies   1,140,226,235 likes of posts or replies    As an initial step, we run the benchmark as fast as it will go. We use 32 threads on the driver side for 24 hardware threads.\nBelow are the numerical quantities for a 400K operation run after 150K operations worth of warmup.\nDuration: 10:41.251\nThroughput: 623.71 (op/s)\nThe statistics that matter are detailed below, with operations ranked in order of descending client-side wait-time. All times are in milliseconds.\n   % of total total_wait name count mean min max     20 % \u0026ldquo;4,231,130\u0026rdquo; LdbcQuery5 656 \u0026ldquo;6,449.89\u0026rdquo; 245 \u0026ldquo;10,311\u0026rdquo;   11 % \u0026ldquo;2,272,954\u0026rdquo; LdbcQuery8 \u0026ldquo;18,354\u0026rdquo; 123.84 14 \u0026ldquo;2,240\u0026rdquo;   10 % \u0026ldquo;2,200,718\u0026rdquo; LdbcQuery3 388 \u0026ldquo;5,671.95\u0026rdquo; 468 \u0026ldquo;17,368\u0026rdquo;   7.3 % \u0026ldquo;1,561,382\u0026rdquo; LdbcQuery14 \u0026ldquo;1,124\u0026rdquo; \u0026ldquo;1,389.13\u0026rdquo; 4 \u0026ldquo;5,724\u0026rdquo;   6.7 % \u0026ldquo;1,441,575\u0026rdquo; LdbcQuery12 \u0026ldquo;1,252\u0026rdquo; \u0026ldquo;1,151.42\u0026rdquo; 15 \u0026ldquo;3,273\u0026rdquo;   6.5 % \u0026ldquo;1,396,932\u0026rdquo; LdbcQuery10 \u0026ldquo;1,252\u0026rdquo; \u0026ldquo;1,115.76\u0026rdquo; 13 \u0026ldquo;4,743\u0026rdquo;   5 % \u0026ldquo;1,064,457\u0026rdquo; LdbcShortQuery3PersonFriends \u0026ldquo;46,285\u0026rdquo; 22.9979 0 \u0026ldquo;2,287\u0026rdquo;   4.9 % \u0026ldquo;1,047,536\u0026rdquo; LdbcShortQuery2PersonPosts \u0026ldquo;46,285\u0026rdquo; 22.6323 0 \u0026ldquo;2,156\u0026rdquo;   4.1 % \u0026ldquo;885,102\u0026rdquo; LdbcQuery6 \u0026ldquo;1,721\u0026rdquo; 514.295 8 \u0026ldquo;5,227\u0026rdquo;   3.3 % \u0026ldquo;707,901\u0026rdquo; LdbcQuery1 \u0026ldquo;2,117\u0026rdquo; 334.389 28 \u0026ldquo;3,467\u0026rdquo;   2.4 % \u0026ldquo;521,738\u0026rdquo; LdbcQuery4 \u0026ldquo;1,530\u0026rdquo; 341.005 49 \u0026ldquo;2,774\u0026rdquo;   2.1 % \u0026ldquo;440,197\u0026rdquo; LdbcShortQuery4MessageContent \u0026ldquo;46,302\u0026rdquo; 9.50708 0 \u0026ldquo;2,015\u0026rdquo;   1.9 % \u0026ldquo;407,450\u0026rdquo; LdbcUpdate5AddForumMembership \u0026ldquo;14,338\u0026rdquo; 28.4175 0 \u0026ldquo;2,008\u0026rdquo;   1.9 % \u0026ldquo;405,243\u0026rdquo; LdbcShortQuery7MessageReplies \u0026ldquo;46,302\u0026rdquo; 8.75217 0 \u0026ldquo;2,112\u0026rdquo;   1.9 % \u0026ldquo;404,002\u0026rdquo; LdbcShortQuery6MessageForum \u0026ldquo;46,302\u0026rdquo; 8.72537 0 \u0026ldquo;1,968\u0026rdquo;   1.8 % \u0026ldquo;387,044\u0026rdquo; LdbcUpdate3AddCommentLike \u0026ldquo;12,659\u0026rdquo; 30.5746 0 \u0026ldquo;2,060\u0026rdquo;   1.7 % \u0026ldquo;361,290\u0026rdquo; LdbcShortQuery1PersonProfile \u0026ldquo;46,285\u0026rdquo; 7.80577 0 \u0026ldquo;2,015\u0026rdquo;   1.6 % \u0026ldquo;334,409\u0026rdquo; LdbcShortQuery5MessageCreator \u0026ldquo;46,302\u0026rdquo; 7.22234 0 \u0026ldquo;2,055\u0026rdquo;   1 % \u0026ldquo;220,740\u0026rdquo; LdbcQuery2 \u0026ldquo;1,488\u0026rdquo; 148.347 2 \u0026ldquo;2,504\u0026rdquo;   0.96 % \u0026ldquo;205,910\u0026rdquo; LdbcQuery7 \u0026ldquo;1,721\u0026rdquo; 119.646 11 \u0026ldquo;2,295\u0026rdquo;   0.93 % \u0026ldquo;198,971\u0026rdquo; LdbcUpdate2AddPostLike \u0026ldquo;5,974\u0026rdquo; 33.3062 0 \u0026ldquo;1,987\u0026rdquo;   0.88 % \u0026ldquo;189,871\u0026rdquo; LdbcQuery11 \u0026ldquo;2,294\u0026rdquo; 82.7685 4 \u0026ldquo;2,219\u0026rdquo;   0.85 % \u0026ldquo;182,964\u0026rdquo; LdbcQuery13 \u0026ldquo;2,898\u0026rdquo; 63.1346 1 \u0026ldquo;2,201\u0026rdquo;   0.74 % \u0026ldquo;158,188\u0026rdquo; LdbcQuery9 78 \u0026ldquo;2,028.05\u0026rdquo; \u0026ldquo;1,108\u0026rdquo; \u0026ldquo;4,183\u0026rdquo;   0.67 % \u0026ldquo;143,457\u0026rdquo; LdbcUpdate7AddComment \u0026ldquo;3,986\u0026rdquo; 35.9902 1 \u0026ldquo;1,912\u0026rdquo;   0.26 % \u0026ldquo;54,947\u0026rdquo; LdbcUpdate8AddFriendship 571 96.2294 1 988   0.2 % \u0026ldquo;43,451\u0026rdquo; LdbcUpdate6AddPost \u0026ldquo;1,386\u0026rdquo; 31.3499 1 \u0026ldquo;2,060\u0026rdquo;   0.01% \u0026ldquo;1,848\u0026rdquo; LdbcUpdate4AddForum 103 17.9417 1 65   0.00% 44 LdbcUpdate1AddPerson 2 22 10 34    At this point we have in-depth knowledge of the choke points the benchmark stresses, and we can give a first assessment of whether the design meets its objectives for setting an agenda for the coming years of graph database development.\nThe implementation is well optimized in general but still has maybe 30% room for improvement. We note that this is based on a compressed column store. One could think that alternative data representations, like in-memory graphs of structs and pointers between them, are better for the task. This is not necessarily so; at the least, a compressed column store is much more space efficient. Space efficiency is the root of cost efficiency, since as soon as the working set is not in memory, a random access workload is badly hit.\nThe set of choke points (technical challenges) actually revealed by the benchmark is so far as follows:\n Cardinality estimation under heavy data skew — Many queries take a tag or a country as a parameter. The cardinalities associated with tags vary from 29M posts for the most common to 1 for the least common. Q6 has a common tag (in top few hundred) half the time and a random, most often very infrequent, one the rest of the time. A declarative implementation must recognize the cardinality implications from the literal and plan accordingly. An imperative one would have to count. Missing this makes Q6 take about 40% of the time instead of 4.1% when adapting. Covering indices — Being able to make multi-column indices that duplicate some columns from the table often saves an entire table lookup. For example, an index onpost by author can also contain the post\u0026rsquo;s creation date. Multi-hop graph traversal — Most queries access a two-hop environment starting at a person. Two queries look for shortest paths of unbounded length. For the two-hop case, it makes almost no difference whether this is done as a union or a special graph traversal operator. For shortest paths, this simply must be built into the engine; doing this client-side incurs prohibitive overheads. A bidirectional shortest path operation is a requirement for the benchmark. Top K — Most queries returning posts order results by descending date. Once there are at least k results, anything older than the __k__th can be dropped, adding a dateselection as early as possible in the query. This interacts with vectored execution, so that starting with a short vector size more rapidly produces an initial top k. Late projection — Many queries access several columns and touch millions of rows but only return a few. The columns that are not used in sorting or selection can be retrieved only for the rows that are actually returned. This is especially useful with a column store, as this removes many large columns (e.g., text of a post) from the working set. Materialization — Q14 accesses an expensive-to-compute edge weight, the number of post-reply pairs between two people. Keeping this precomputed drops Q14 from the top place. Other materialization would be possible, for example Q2 (top 20 posts by friends), but since Q2 is just 1% of the load, there is no need. One could of course argue that this should be 20x more frequent, in which case there could be a point to this. Concurrency control — Read-write contention is rare, as updates are randomly spread over the database. However, some pages get read very frequently, e.g., some middle level index pages in the post table. Keeping a count of reading threads requires a mutex, and there is significant contention on this. Since the hot set can be one page, adding more mutexes does not always help. However, hash partitioning the index into many independent trees (as in the case of a cluster) helps for this. There is also contention on a mutex for assigning threads to client requests, as there are large numbers of short operations.  In subsequent posts, we will look at specific queries, what they in fact do, and what their theoretical performance limits would be. In this way we will have a precise understanding of which way SNB can steer the graph DB community.\nSNB Interactive Series  SNB Interactive, Part 1: What is SNB Interactive Really About? SNB Interactive, Part 2: Modeling Choices SNB Interactive, Part 3: Choke Points and Initial Run on Virtuoso  ","permalink":"https://ldbc.github.io/post/snb-interactive-part-3-choke-points-and-initial-run-on-virtuoso/","tags":["developer"],"title":"SNB Interactive Part 3: Choke Points and Initial Run on Virtuoso"},{"categories":null,"contents":"Next 31st of May the GRADES workshop will take place in Melbourne within the ACM/SIGMOD presentation. GRADES started as an initiative of the Linked Data Benchmark Council in the SIGMOD/PODS 2013 held in New York.\nAmong the papers published in this edition we have \u0026ldquo;Graphalytics: A Big Data Benchmark for Graph-Processing Platforms\u0026rdquo;, which presents a new benchmark that uses the Social Network Benchmark data generator of LDBC (that can be found in https://github.com/ldbc) as the base to execute the algorithms used for the benchmark, among which we have BFS, community detection and connected components. We also have \u0026ldquo;Microblogging Queries on Graph Databases: an Introspection\u0026rdquo; which benchmarks two of the most significant Graph Databases in the market, i.e. Neo4j and Sparksee using microblogging queries on top of twitter data. We can finally mention \u0026ldquo;Frappé: Querying the Linux Kernel Dependency Graph\u0026rdquo; which presents a framework for querying and visualising the dependencies of large C/C++ software systems.\nCheck the complete agenda.\nMeet you in Melbourne!\n","permalink":"https://ldbc.github.io/post/snb-and-graphs-related-presentations-at-grades-15/","tags":["GRADES","SIGMOD","Graphalytics","events","snb","data generator"],"title":"SNB and Graphs Related Presentations at GRADES '15"},{"categories":null,"contents":"​SNB Interactive is the wild frontier, with very few rules. This is necessary, among other reasons, because there is no standard property graph data model, and because the contestants support a broad mix of programming models, ranging from in-process APIs to declarative query.\nIn the case of Virtuoso, we have played with SQL and SPARQL implementations. For a fixed schema and well known workload, SQL will always win. The reason for this is that this allows to materialize multi-part indices and data orderings that make sense for the application. In other words, there is transparency into physical design. An RDF application may also have physical design by means ofstructure-aware storage but this is more complex and here we are just concerned with speed and having things work precisely as we intend.\nSchema Design SNB has a regular schema described by a UML diagram. This has a number of relationships of which some have attributes. There are no heterogenous sets, e.g. no need for run-time typed attributes or graph edges with the same label but heterogeneous end points. Translation into SQL or RDF is straightforward. Edges with attributes, e.g. the knows relation between people would end up represented as a subject with the end points and the date since as properties. The relational implementation has a two-part primary key and the date since as a dependent column. A native property graph database would use an edge with an extra property for this, as such are typically supported.\nThe only table-level choice has to do with whether posts and comments are kept in the same or different data structures. The Virtuoso schema has a single table for both, with nullable columns for the properties that occur only in one. This makes the queries more concise. There are cases where only non-reply posts of a given author are accessed. This is supported by having two author foreign key columns each with its own index. There is a single nullable foreign key from the reply to the post/comment being replied to.\nThe workload has some frequent access paths that need to be supported by index. Some queries reward placing extra columns in indices. For example, a common pattern is accessing the most recent posts of an author or group of authors. There, having a composite key of ps_creatorid, ps_creationdate, ps_postid pays off since the to-k on creationdate can be pushed down into the index without needing a reference to the table.\nThe implementation is free to choose data types for attributes, specifically datetimes. The Virtuoso implementation adopts the practice of the Sparksee and Neo4J implementations and represents this is a count of milliseconds since epoch. This is less confusing, faster to compare and more compact than a native datetime datatype that may or may not have timezones etc. Using a built-in datetime seems to be nearly always a bad idea. A dimension table or a number for a time dimension avoids the ambiguities of a calendar or at least makes these explicit.\nThe benchmark allows procedurally maintaining materializations of intermediate results for use by queries as long as these are maintained transaction by transaction. For example, each person could have the 20 newest posts by immediate contacts precomputed. This would reduce Q2 \u0026ldquo;top of the wall\u0026rdquo; to a single lookup. This dows not however appear to be worthwhile. The Virtuoso implementation does do one such materialization for Q14: A connection weight is calculated for every pair of persons that know each other. This is related to the count of replies by one or the other to content generated by the other. If there does not exist a single reply in either direction, the weight is taken to be 0. This weight is precomputed after bulk load and subsequently maintained each time a reply is added. The table for this is the only row-wise structure in the schema and represents a half matrix of connected people, i.e. person1, person2 -\u0026gt; weight. Person1 is by convention the one with the smaller p_personid. Note that comparing id\u0026rsquo;s in this way is useful but not normally supported by RDF systems. RDF would end up comparing strings of URI\u0026rsquo;s with disastrous performance implications unless an implementation specific trick were used.\nIn the next installment we will analyze an actual run.\nSNB Interactive Series  SNB Interactive, Part 1: What is SNB Interactive Really About? SNB Interactive, Part 2: Modeling Choices SNB Interactive, Part 3: Choke Points and Initial Run on Virtuoso  ","permalink":"https://ldbc.github.io/post/snb-interactive-part-2-modeling-choices/","tags":["social network benchmark","snb","Interactive","Workload","Virtuoso"],"title":"SNB Interactive Part 2:  Modeling Choices"},{"categories":null,"contents":"LDBC is presenting two papers at the next edition of the ACM SIGMOD/PODS conference held in Melbourne from May 31st to June 4th, 2015. The annual SCM SIGMOD/PODS conference is a leading international forum for database researchers, practitioners, developers, and users to explore cutting-edge ideas and results, and to exchange techniques, tools and experiences.\nOn the industry track, LDBC will be presenting the Social Network Benchmark Interactive Workload by Orri Erling (OpenLink Software), Alex Averbuch (Neo Technology), Josep Larriba-Pey (Sparsity Technologies, Hassan Chafi (Oracle Labs), Andrey Gubichev (TU Munich), Arnau Prat (Universitat Politècnica de Catalunya), Minh-Duc Pham (VU University Amsterdam) and Peter Boncz (CWI).\nYou can read more about the Social Network Benchmark here and collaborate if you\u0026rsquo;re interested!\nThe other presentation will be at the GRADES workshop within the SIGMOD program regarding Graphalytics: A Big Data Benchmark for Graph-Processing platforms by Mihai Capotă, Tim Hegeman, Alexandru Iosup (Delft University of Technology), Arnau Prat (Universitat Politècnica de Catalunya), Orri Erling (OpenLink Sotware) and Peter Boncz (CWI). We will provide more information about GRADES and this specific presentation in a following post as GRADES is part of the events organized by LDBC.\nDon\u0026rsquo;t forget to check our presentations if you\u0026rsquo;re attending the SIGMOD!\n","permalink":"https://ldbc.github.io/post/ldbc-participates-in-the-36th-edition-of-the-acm-sigmod-pods-conference/","tags":["SIGMOD","ACM","SIGMOD/PODS","grades","snb","Interactive Workload","Graphalytics"],"title":"LDBC Participates in the 36th Edition of the ACM SIGMOD/PODS Conference"},{"categories":null,"contents":"This post is the first in a series of blogs analyzing the LDBC Social Network Benchmark Interactive workload. This is written from the dual perspective of participating in the benchmark design and of building the OpenLink Virtuoso implementation of same.\nWith two implementations of SNB interactive at four different scales, we can take a first look at what the benchmark is really about. The hallmark of a benchmark implementation is that its performance characteristics are understood and even if these do not represent the maximum of the attainable, there are no glaring mistakes and the implementation represents a reasonable best effort by those who ought to know, namely the system vendors.\nThe essence of a benchmark is a set of trick questions or choke points, as LDBC calls them. A number of these were planned from the start. It is then the role of experience to tell whether addressing these is really the key to winning the race. Unforeseen ones will also surface.\nSo far, we see that SNB confronts the implementor with choices in the following areas:\n Data model: Relational, RF, property graph? Physical model, e.g. row-wise vs. column wise storage Materialized data ordering: Sorted projections, composite keys, replicating columns in auxxiliary data structures Maintaining precomputed, materialized intermediate results, e.g. use of materialized views, triggers Query optimization: join order/type, interesting physical data orderings, late projection, top k, etc. Parameters vs. literals: Sometimes different parameter values result in different optimal query plans Predictable, uniform latency: The measurement rules stipulate the the SUT must not fall behind the simulated workload Durability - how to make data durable while maintaining steady throughput? Logging vs. checkpointing.  In the process of making a benchmark implementation, one naturally encounters questions about the validity, reasonability and rationale of the benchmark definition itself. Additionally, even though the benchmark might not directly measure certain aspects of a system, making an implementation will take a system past its usual envelope and highlight some operational aspects.\n Data generation - Generating a mid-size dataset takes time, e.g. 8 hours for 300G. In a cloud situation, keeping the dataset in S3 or similar is necessary, re-generating every time is not an option. Query mix - Are the relative frequencies of the operations reasonable? What bias does this introduce? Uniformity of parameters: Due to non-uniform data distributions in the dataset, there is easily a 100x difference between a \u0026lsquo;fast\u0026rsquo; and \u0026lsquo;slow\u0026rsquo; case of a single query template. How long does one need to run to balance these fluctuations? Working set: Experience shows that there is a large difference between almost warm and steady state of working set. This can be a factor of 1.5 in throughput. Are the latency constraints reasonable? In the present case, a qualifying run must have under 5% of all query executions starting over 1 second late. Each execution is scheduled beforehand and done at the intended time. If the SUT does not keep up, it will have all available threads busy and must finish some work before accepting new work, so some queries will start late. Is this a good criterion for measuring consistency of response time? There are some obvious possibilities of abuse. Is the benchmark easy to implement/run? Perfection is open-ended and optimization possibilities infinite, albeit with diminishing returns. Still, getting startyed should not be too hard. Since systems will be highly diverse, testing that these in fact do the same thing is important. The SNB validation suite is good for this and given publicly available reference implementations, the effort of getting started is not unreasonable. Since a Qualifying run must meet latency constraints while going as fast as possible, setting the performance target involves trial and error. Does the tooling make this easy? Is the durability rule reasonable? Right now, one is not required to do checkpoints but must report the time to roll forward from the last checkpoint or initial state. Incenting vendors to build faster recovery is certainly good, but we are not through with all the implications.\nWhat about redundant clusters?  The following posts will look at the above in light of actual experience.\nSNB Interactive Series  SNB Interactive, Part 1: What is SNB Interactive Really About? SNB Interactive, Part 2: Modeling Choices SNB Interactive, Part 3: Choke Points and Initial Run on Virtuoso  ","permalink":"https://ldbc.github.io/post/snb-interactive-part-1-what-is-snb-interactive-really-about/","tags":["snb","Interactive Workload","social networks benchmark","Virtuoso"],"title":"SNB Interactive Part 1: What Is SNB Interactive Really About?"},{"categories":null,"contents":"TODO fix link below\nIn a previous 3-part blog series we touched upon the difficulties of executing the LDBC SNB Interactive (SNB) workload, while achieving good performance and scalability. What we didn\u0026rsquo;t discuss is why these difficulties were unique to SNB, and what aspects of the way we perform workload execution are scientific contributions - novel solutions to previously unsolved problems. This post will highlight the differences between SNB and more traditional database benchmark workloads. Additionally, it will motivate why we chose to develop a new workload driver as part of this work, rather than using existing tooling that was developed in other database benchmarking efforts. To briefly recap, the task of the driver is to run a transactional database benchmark against large synthetic graph datasets - \u0026ldquo;graph\u0026rdquo; is the word that best captures the novelty and difficulty of this work.\nWorkload Execution - Traditional vs Graph\nTransactional graph workloads differ from traditional relational workloads in several fundamental ways, one of them being the complex dependencies that exist between queries of a graph workload.\nTo understand what is meant by \u0026ldquo;traditional relational workloads\u0026rdquo;, take the classical TPC-C benchmark as an example. In TPC-C Remote Terminal Emulators (emulators) are used to issue update transactions in parallel, where the transactions issued by these emulators do not depend on one another. Note, \u0026ldquo;dependency\u0026rdquo; is used here in the context of scheduling, i.e., one query is dependent on another if it can not start until the other completes. For example, a New-Order transaction does not depend on other orders from this or other users. Naturally, the results of Stock-Level transactions depend on the items that were previously sold, but in TPC-C it is not an emulator\u0026rsquo;s responsibility to enforce any such ordering. The scheduling strategy employed by TPC-C is tailored to the scenario where transactional updates do not depend on one another. In reality, one would expect to also have scheduling dependencies between transactions, e.g., checking the status of the order should only be done after the order is registered in the system. TPC-C, however, does not do this and instead only asks for the status of the last order for a given user. Furthermore, adding such dependencies to TPC-C would make scheduling only slightly more elaborate. Indeed, the Load Tester (LT) would need to make sure a New-Order transaction always precedes the read requests that check its status, but because users (and their orders) are partitioned across LTs, and orders belong to a particular user, this scheduling does not require inter-LT communication.\nA significantly more difficult scheduling problem arises when we consider the SNB benchmark that models a real-world social network. Its domain includes users that form a social friendship graph and which leave posts/comments/likes on each others walls (forums). The update transactions are generated (exported as a log) by the data generator, with assigned timestamps, e.g. user 123 added post 456 to forum 789 at time T. Suppose we partition this workload by user, such that each driver gets all the updates (friendship requests, posts, comments and likes on other user\u0026rsquo;s posts etc) initiated by a given user. Now, if the benchmark is to resemble a real-world social network, the update operations represent a highly connected (and dependent) network: a user should not create comments before she joins the network, a friendship request can not be sent to a non-existent user, a comment can only be added to a post that already exists, etc. Given a user partitioning scheme, most such dependencies would cross the boundaries between driver threads/processes, because the correct execution of update operations requires that the social network is in a particular state, and that state depends on the progress of other threads/processes.\nSuch scheduling dependencies in the SNB workload essentially replicate the underlying graph-like shape of its dataset. That is, every time a user comments on a friend\u0026rsquo;s wall, for example, there is a dependency between two operations that is captured by an edge of the social graph. Partitioning the workload among the LTs therefore becomes equivalent to graph partitioning, a known hard problem.\nBecause it\u0026rsquo;s a graph\nIn short, unlike previous database benchmarking efforts, the SNB workload has necessitated a redefining of the state-of-the-art in workload execution. It is no longer sufficient to rely solely on workload partitioning to safely capture inter-query dependencies in complex database benchmark workloads. The graph-centric nature of SNB introduces new challenges, and novel mechanisms had to be developed to overcome these challenges. To the best of our knowledge, the LDBC SNB Interactive benchmark is the first benchmark that requires a non-trivial partitioning of the workload, among the benchmark drivers. In the context of workload execution, our contribution is therefore the principled design of a driver that executes dependent update operations in a performant and scalable way, across parallel/distributed LTs, while providing repeatable, vendor-independent execution of the benchmark.\n","permalink":"https://ldbc.github.io/post/why-do-we-need-an-ldbc-snb-specific-workload-driver/","tags":["snb","Driver"],"title":"Why Do We Need an LDBC SNB-Specific Workload Driver?"},{"categories":null,"contents":"As discussed in previous posts, one of the features that makes Datagen more realistic is the fact that the activity volume of the simulated Persons is not uniform, but forms spikes. In this blog entry I want to explain more in depth how this is actually implemented inside of the generator.\nFirst of all, I start with a few basics of how Datagen works internally. In Datagen, once the person graph has been created (persons and their relationships), the activity generation starts. Persons are divided into blocks of 10k, in the same way they are during friendship edges generation process. Then, for each person of the block, three types of forums are created:\n The wall of the person The albums of the person The groups where the person is a moderator  We will put our attention to group generation, but the same concepts apply to the other types of forums. Once a group is created, the members of the group are selected. These are selected from either the friends of the moderator, or random persons within the same block.\nAfter assigning the members to the group, the post generation starts. We have two types of post generators, the uniform post generator and the event based post generator. Each post generator is responsible of, given a forum, generate a set of posts for the forum, whose authors are taken from the set of members of the forum. The uniform post generator distributes the dates of the generated posts uniformly in the time line (from the date of the membership until the end of the simulation time). On the other hand, the event based post generator assigns dates to posts, based on what we call “flashmob events”.\nFlashmob events are generated at the beginning of the execution. Their number is predefined by a configuration parameter which is set to 30 events per month of simulation, and the time of the event is distributed uniformly along all the time line. Also, each event has a volume level assigned (between 1 and 20) following a power law distribution, which determines how relevant or important the event is, and a tag representing the concept or topic of the event. Two different events can have the same tag. For example, one of the flashmob events created for SF1 is one related to \u0026ldquo;Enrique Iglesias\u0026rdquo; tag, whose level is 11 and occurs on 29th of May of 2012 at 09:33:47.\nOnce the event based post generation starts for a given group, a subset of the generated flashmob events is extracted. These events must be correlated with the tag/topic of the group, and the set of selected events is restricted by the creation date of the group (in a group one cannot talk about an event previous to the creation of the group). Given this subset of events and their volume level, a cumulative probability distribution (using the events sorted by event date and their level) is computed, which is later used to determine to which event a given post is associated. Therefore, those events with a larger lavel will have a larger probability to receive posts, making their volume larger. Then, post generation starts, which can be summarized as follows:\n Determine the number of posts to generate Select a random member of the group that will generate the post Determine the event the post will be related to given the aforementioned cumulative distribution Assign the date of the post based on the event date  In order to assign the date to the post, based on the date of the event the post is assigned to, we follow the following probability density, which has been extracted from [1]. The shape of the probability density consists of a combination of an exponential function in the 8 hour interval around the peak, while the volume outside this interval follows a logarithmic function. The following figure shows the actual shape of the volume, centered at the date of the event.\nFollowing the example of \u0026ldquo;Enrique Iglesias\u0026rdquo;, the following figure shows the activity volume of posts around the event as generated by Datagen.\nIn this blog entry we have seen how datagen creates event driven user activity. This allows us to reproduce the heterogenous post creation density found in a real social network, where post creation is driven by real world events.\nReferences [1] Jure Leskovec, Lars Backstrom, Jon M. Kleinberg: Meme-tracking and the dynamics of the news cycle. KDD 2009: 497-506\n","permalink":"https://ldbc.github.io/post/event-driven-post-generation-in-datagen/","tags":["datagen","social network","snb","social network benchmark"],"title":"Event Driven Post Generation in Datagen"},{"categories":null,"contents":"This blog entry is about one of the features of DATAGEN that makes it different from other synthetic graph generators that can be found in the literature: the community structure of the graph.\nWhen generating synthetic graphs, one must not only pay attention to quantitative measures such as the number of nodes and edges, but also to other more qualitative characteristics such as the degree distribution, clustering coefficient. Real graphs, and specially social networks, have typically highly skewed degree distributions with a long tail, a moderatelly large clustering coefficient and an appreciable community structure.\nThe first two characteristics are deliberately modeled in DATAGEN. DATAGEN generates persons with a degree distribution that matches that observed in Facebook, and thanks to the attribute correlated edge generation process, we obtain graphs with a moderately large clustering coefficient. But what about the community structure of graphs generated with DATAGEN? The answer can be found in the paper titled “How community-like is the structure of synthetically generated graphs”, which was published in GRADES 2014 [1]. Here we summarize the paper and its contributions and findings.\nExisting synthetic graph generators such as Rmat [1] and Mag [2], are graphs generators designed to produce graphs with long tailed distributions and large clustering coefficient, but completely ignore the fact that real graphs are structured into communities. For this reason, Lancichinetti et al. proposed LFR [3], a graph generator that did not only produced graphs with realistic high level characteristics, but enforced an appreciable community structure. This generator, has become the de facto standard for benchmarking community detection algorithms, as it does not only outputs a graph but also the communities present in that graph, hence it can be used to test the quality of a community detection algorithm.\nHowever, no one studied if the community structure produced by LFR, was in fact realistic compared to real graphs. Even though the community structure in in LFR exhibit interesting properties, such as the expected larger internal density than external, or a longtailed distribution of community sizes, they lack the noise and inhomogeneities present in a real graph. And more importantly, how does the community structure of DATAGEN compares to that exhibited in LFR and reap graphs? Is it more or less realistic? + The authors of [1] set up an experiment where they analized the characteristics of the communities output by LFR, and the groups (groups of people interested in a given topic) output by DATAGEN, and compared them to a set of real graphs with metadata. These real graphs, which can be downloaded from the Snap project website, are graphs that have recently become very popular in the field of community detection, as they contain ground truth communities extracted from their metadata. The ground truth graphs used in this experiment are shown in the following table. For more details about how this ground truth is generated, please refer to [4].\n    Nodes Edges     Amazon 334863 925872   Dblp 317080 1049866   Youtube 1134890 2987624   Livejournal 3997962 34681189    The authors of [1] selected a set of statistical indicators to characterize the communities:\n The clustering coefficient The triangle participation ration (TPR), which is the ratio of nodes that close at least one triangle in the community. The bridge ratio, which is the ratio of edges whose removal disconnects the community. The diameter The conductance The size  The authors start by analyzing each community of the ground truth graphs using the above statistical indicators and ploting the distributions of each of them. The following are the plots of the Livejournal graph. We summarize the findings of the authors regarding real graphs: + Several indicators (Clustering Coefficient, TPR and Bridge ratio) exihibit a multimodal distribution, with two peaks aht their extremes.\n Many of the communities (44%) have a small clustering coefficient between 0 and 0.01. Out of them, 56% have just three vertices. On the other hand, 11% of the communities have a clustering coefficient between 0.99 and 1.0. In between, communities exhibit different values of clustering coefficients. This trend is also observed for TPR and Bridgeratio. This suggests that communities cannot be modeled using a single model. 84% of the communities have a diameter smaller than five, suggesting that ground truth communities are small and compact Ground truth communities are not very isolated, they have a lot of connections pointing outside of the community. Most of the communities are small (10 or less nodes). In general, ground truth communities are, small with a low diameter, not isolated and with different ranges of internal connectivity.               Clustering Coefficient TPR       Bridge Ratio Diameter       Conductance Size    The authors performed the same experiment but for DATAGEN and LFR graphs. They generated a graph of 150k nodes, using their default parameters. In the case of LFR, they tested five different values of the mixing factor, which specifies the ratio of edges of the community pointing outside of the community, They ranged this value from 0 to 0.5. The following are the distributions for DATAGEN.\n             Clustering Coefficient TPR       Bridge Ratio TPRDiameter       Conductance Size    The main conclusions that can be extracted from DATAGEN can be summarized asfollows:\n DATAGEN is able to reproduce the multimodal distribution observed for clustering coefficient, TPR and bridge ratio. The central part of the clustering coefficient is biased towards the left, in a similar way as observed for the youtube and livejournal graphs. Communities of DATAGEN graphs are not, as in real graphs, isolated, but in this case their level of isolation if significantly larger. The diameter is small like in the real graphs. It is significant that communities in DATAGEN graphs are closer to those observed in Youtube and Livejournal, as these are social networks like the graphs produced by DATAGEN. We see that DATAGEN is able to reproduce many of their characteristics.  Finally, the authors repeat the same experiment for LFR graphs. The following are the plots for the LFR graph with mixing ratio 0.3. From them, the authors extract the following conclusions:\n LFR graphs donot show the multimodal distribution observed in real graphs Only the diameter shows a similar shape as in the ground truth.               Clustering Coefficient TPR       Bridge Ratio TPRDiameter       Conductance Size    To better quanify how similar are the distribuions between the different graphs, the authors also show the correlograms for each of the statisticsl indicators. These correlograms, contain the Spearman\u0026rsquo;s correlation coefficient between each pair of graphs for a given statistical indicator. The more blue the color, the better the correlation is. We see that DATAGEN distributions correlate very well with those observed in real graphs, specially as we commented above, with Youtube and Livejournal. On the other hand, LFR only succeds significantly in the case of the Diameter.\n             Clustering Coefficient TPR       Bridge Ratio TPRDiameter       Conductance Size    We see that DATAGEN is able to reproduce a realistics community structure, compared to existing graph generators. This feature, could be potentially exploited to define new benchmakrs to measure the quality of novel community detection algorithms. Stay tuned for future blog posts about his topic!\nReferences [1] Arnau Prat-Pérez, David Domínguez-Sal: How community-like is the structure of synthetically generated graphs? GRADES 2014\n[2] Deepayan Chakrabarti, Yiping Zhan, and ChristosFaloutsos. R-mat: A recursive model for graph mining. SIAM 2014\n[3] Myunghwan Kim and Jure Leskovec. Multiplicative attribute graph model of real-world networks. Internet Mathematics\n[4] Andrea Lancichinetti, Santo Fortunato, and Filippo Radicchi. Benchmark graphs for testing community detection algorithms. Physical Review E 2008.\n","permalink":"https://ldbc.github.io/post/the-ldbc-datagen-community-structure/","tags":["datagen","social network","social network benchmark"],"title":"The LDBC Datagen Community Structure"},{"categories":null,"contents":"Publishing and media businesses are going through transformation I took this picture in June 2010 next to Union Square in San Francisco. I was smoking and wrestling my jetlag in front of Hilton. In the lobby inside the SemTech 2010 conference attendants were watching a game from the FIFA World Cup in South Africa. In the picture, the self-service newspaper stand is empty, except for one free paper. It was not long ago, in the year 2000, this stand was full. Back than the people in the Bay area were willing to pay for printed newspapers. But this is no longer true.\nWhat’s driving this change in publishing and media?\n Widespread and instantaneous distribution of information over the Internet has turned news into somewhat of a \u0026ldquo;commodity\u0026rdquo; and few people are willing to pay for it The wealth of free content on YouTube and similar services spoiled the comfort of many mainstream broadcasters; Open access publishing has limited academic publishers to sell journals and books at prices that were considered fair ten years ago.  Alongside other changes in the industry, publishers figured out that it is critical to add value through better authoring, promotion, discoverability, delivery and presentation of precious content.\nImagine instant news in context, Imagine personal channels, Imagine \u0026hellip; triplestores While plain news can be created repeatedly, premium content and services are not as easy to create. Think of an article that not only tells the new facts, but refers back to previous events and is complemented by an info-box of relevant facts. It allows one to interpret and comprehend news more effectively. This is the well-known journalistic aim to put news in context. It is also well-known that producing such news in \u0026ldquo;near real time\u0026rdquo; is difficult and expensive using legacy processes and content management technology.\nAnother example would be a news feed that delivers good coverage of information relevant to a narrow subject – for example a company, a story line or a region. Judging by the demand for intelligent press clipping services like Factiva, such channels are in demand but are not straightforward to produce with today’s technology. Despite the common perception that automated recommendations for related content and personalized news are technology no-brainers, suggesting truly relevant content is far from trivial.\nFinally, if we use an example in life sciences, the ability to quickly find scientific articles discussing asthma and x-rays, while searching for respiration disorders and radiation, requires a search service that is not easy to deliver.\nMany publishers have been pressed to advance their business. This, in turn, had led to quest to innovate. And semantic technology can help publishers in two fundamental ways:\n Generation of rich and \u0026ldquo;meaningful\u0026rdquo; (trying not to use \u0026ldquo;semantic\u0026rdquo; :-) metadata descriptions; Dynamic retrieval of content, based on this rich metadata, enabling better delivery.  In this post I write about \u0026ldquo;semantic annotation\u0026rdquo; and how it enables application scenarios like BBC’s Dynamic Semantic Publishing (DSP). I will also present the business case behind DSP. The final part of the post is about triplestores – semantic graph database engines, used in DSP. To be more concrete I write about the Semantic Publishing Benchmark (SPB), which evaluates the performance of triplestores in DSP scenarios.\nSemantic Annotation produces Rich Metadata Descriptions – the fuel for semantic publishing The most popular meaning of \u0026ldquo;semantic annotation\u0026rdquo; is the process of enrichment of text with links to (descriptions of) concepts and entities mentioned in the text. This usually means tagging either the entire document or specific parts of it with identifiers of entities. These identifiers allow one to retrieve descriptions of the entities and relations to other entities – additional structured information that fuels better search and presentation.\nThe concept of using text-mining for automatic semantic annotation of text with respect to very large datasets, such as DBPedia, emerged in early 2000. In practical terms it means using such large datasets as a sort of gigantic gazetteer (name lookup tool) and the ability to disambiguate. Figuring out whether \u0026ldquo;Paris\u0026rdquo; in the text refers to the capital of France or to Paris, Texas, or to Paris Hilton is crucial in such context. Sometimes this is massively difficult – try to instruct a computer how to guess whether \u0026ldquo;Hilton\u0026rdquo; in the second sentence of this post refers to a hotel from the chain founded by her grandfather or that I had the chance to meet Paris Hilton in person on the street in San Francisco.\nToday there are plenty of tools (such as the Ontotext Media and Publishing platform and DBPedia Spotlight) and services (such as Thomson Reuter’s OpenCalais and Ontotext’s S4) that offer automatic semantic annotation. Although text-mining cannot deliver 100% correct annotations, there are plenty of scenarios, where technology like this would revoluntionize a business. This is the case with the Dynamic Semantic Publishing scenario described below.\nThe BBC’s Dynamic Semantic Publishing (DSP) Dynamic Semantic Publishing is a model for using semantic technology in media developed by a group led by John O’Donovan and Jem Rayfield at the BBC. The implementation of DSP behind BBC’s FIFA World Cup 2010 website was the first high-profile success story for usage of semantic technology in media. It is also the basis for the SPB benchmark – sufficient reasons to introduce this use case at length below.\nBBC Future Media \u0026amp; Technology department have transformed the BBC relational content management model and static publishing framework to a fully dynamic semantic publishing architecture. With minimal journalistic management, media assets are being enriched with links to concepts, semantically described in a triplestore. This novel semantic approach provides improved navigation, content re-use and re-purposing through automatic aggregation and rendering of links to relevant stories. At the end of the day DSP improves the user experience on BBC’s web site.\n\u0026ldquo;A high-performance dynamic semantic publishing framework facilitates the publication of automated metadata-driven web pages that are light-touch, requiring minimal journalistic management, as they automatically aggregate and render links to relevant stories\u0026rdquo;. \u0026ndash; Jem Rayfield, Senior Technical Architect, BBC News and Knowledge\nThe Dynamic Semantic Publishing (DSP) architecture of the BBC curates and publishes content (e.g. articles or images) based on embedded Linked Data identifiers, ontologies and associated inference. It allows for journalists to determine levels of automation (\u0026ldquo;edited by exception\u0026rdquo;) and support semantic advertisement placement for audiences outside of the UK. The following quote explains the workflow when a new article gets into BBC’s content management system.\n\u0026ldquo;In addition to the manual selective tagging process, journalist-authored content is automatically analysed against the World Cup ontology. A natural language and ontological determiner process automatically extracts World Cup concepts embedded within a textual representation of a story. The concepts are moderated and, again, selectively applied before publication. Moderated, automated concept analysis improves the depth, breadth and quality of metadata publishing.\nJournalist-published metadata is captured and made persistent for querying using the resource description framework (RDF) metadata representation and triple store technology. A RDF triplestore and SPARQL approach was chosen over and above traditional relational database technologies due to the requirements for interpretation of metadata with respect to an ontological domain model. The high level goal is that the domain ontology allows for intelligent mapping of journalist assets to concepts and queries. The chosen triplestore provides reasoning following the forward-chaining model and thus implied inferred statements are automatically derived from the explicitly applied journalist metadata concepts. For example, if a journalist selects and applies the single concept \u0026ldquo;Frank Lampard\u0026rdquo;, then the framework infers and applies concepts such as \u0026ldquo;England Squad\u0026rdquo;, \u0026ldquo;Group C\u0026rdquo; and \u0026ldquo;FIFA World Cup 2010\u0026rdquo; \u0026hellip;\u0026quot; \u0026ndash; Jem Rayfield\nOne can consider each of the \u0026ldquo;aggregation pages\u0026rdquo; of BBC as a sort of feed or channel serving content related to a specific topic. If you take this perspective, with its World Cup 2010 website BBC was able to provide more than 700 thematic channels.\n\u0026ldquo;The World Cup site is a large site with over 700 aggregation pages (called index pages) designed to lead you on to the thousands of story pages and content\n…we are not publishing pages, but publishing content as assets which are then organized by the metadata dynamically into pages, but could be re-organized into any format we want much more easily than we could before.\n… The index pages are published automatically. This process is what assures us of the highest quality output, but still save large amounts of time in managing the site and makes it possible for us to efficiently run so many pages for the World Cup.\u0026quot; \u0026ndash; John O\u0026rsquo;Donovan, Chief Technical Architect, BBC Future Media \u0026amp; Technology\nTo get a real feeling about the load of the triplestore behind BBC\u0026rsquo;s World Cup web site, here are some statistics:\n 800+ aggregation pages (Player, Team, Group, etc.), generated through SPARQL queries; Average unique page requests/day: 2 million; Average SPARQL queries/day: 1 million; 100s repository updates/inserts per minute with OWL 2 RL reasoning; Multi data center that is fully resilient, clustered 6 node triplestore.  The Semantic Publishing Benchmark LDBC\u0026rsquo;s Semantic Publishing Benchmark (SPB) measures the performance of an RDF database in a load typical for metadata-based content publishing, such as the BBC Dynamic Semantic Publishing scenario. Such load combines tens of updates per second (e.g. adding metadata about new articles) with even higher volumes of read requests (SPARQL queries collecting recent content and data to generate web pages on a specific subject, e.g. Frank Lampard).\nSPB simulates a setup for media that deals with large volumes of streaming content, e.g. articles, pictures, videos. This content is being enriched with metadata that describes it through links to reference knowledge:\n Reference knowledge: taxonomies and databases that include relevant concepts, entities and factual information (e.g. sport statistics); Metadata for each individual piece of content allows publishers to efficiently produce live streams of content relevant to specific subjects.  In this scenario the triplestore holds both reference knowledge and metadata. The main interactions with the repository are of two types:\n Aggregation queries retrieve content according to various criteria. There are two sets (mixes) of aggregation queries. The basic one includes interactive queries that involve retrieval of concrete pieces of content, as well as aggregation functions, geo-spatial and full-text search constraints. The analytical query mix includes analytical queries, faceted search and drill-down queries; Updates, adding new metadata or updating the reference knowledge. It is important that such updates should immediately impact the results of the aggregation queries. Imagine a fan checking the page for Frank Lampard right after he scored a goal – she will be very disappointed to see out of date statistics there.  SPB v.1.0 directly reproduces the DSP setup at the BBC. The reference dataset consists of BBC Ontologies (Core, Sport, News), BBC datasets (list of F1 teams, MPs, etc.) and an excerpt from Geonames for the UK. The benchmark is packed with metadata generator that allows one to set up experiments at different scales. The metadata generator produces 19 statements per Creative Work (BBC’s slang for all sorts of media assets). The standard scale factor is 50 million statements.\nTODO links below\nA more technical introduction to SPB can be found in this post. Results from experiments with SPB on different hardware configurations, including AWS instances, are available in this post. An interesting discovery is that given the current state of the technology (particularly the GraphDB v.6.1 engine) and today’s cloud infrastructure, the load of BBC’s World Cup 2010 website can be handled at AWS by a cluster that costs only $81/day.\nDespite the fact that SPB v.1.0 follows closely the usage scenario for triplestores in BBC’s DSP incarnations, it is relevant to a wide range of media and publishing scenarios, where large volumes of \u0026ldquo;fast flowing\u0026rdquo; content need to be \u0026ldquo;dispatched\u0026rdquo; to serve various information needs of a huge number of consumers. The main challenges can be summarized as follows:\n The Triplestore is used as operational database serving a massive number of read queries (hundreds of queries per second) in parallel with tens of update transactions per second. Transactions need to be handled instantly and in a reliable and consistent manner; Reasoning is needed to map content descriptions to queries in a flexible manner; There are specific requirements, such as efficient handling of full-text search, geo-spatial and temporal constraints.  SPB v.2.0 – steeper for the engines, closer to the publishers We are in the final testing of the new version 2.0 of SPB. The benchmark has evolved to allow for retrieval of semantically relevant content in a more advanced manner and at the same time to demonstrate how triplestores can offer simplified and more efficient querying.\nThe major changes in SPB v.2.0 can be summarized as follows:\n Much bigger reference dataset: from 170 thousand to 22 million statements. Now it includes GeoNames data about all of Europe (around 7 million statements) and DBPedia data about companies, people and events (14 million statements). This way we can simulate media archives described against datasets with good global coverage for specific types of objects. Such large reference sets also provide a better testing ground for experiments with very large content archives – think of 50 million documents (1 billion statements) or more; Better interconnected reference data: more than 5 million links between entities, including 500,000 owl:sameAs links between DBPedia and Geonames descriptions. The latter evaluates the capabilities of the engine to deal with data coming from multiple sources, which use different identifiers for one and the same entity; Retrieval of relevant content through links in the reference data, including inferred ones. To this end it is important than SPB v.2.0 involves much more comprehensive inference, particularly with respect to transitive closure of parent-company and geographic nesting chains.  ","permalink":"https://ldbc.github.io/post/industry-relevance-of-the-semantic-publishing-benchmark/","tags":["industry"],"title":"Industry Relevance of the Semantic Publishing Benchmark"},{"categories":null,"contents":"The Linked Data paradigm has become the prominent enabler for sharing huge volumes of data using Semantic Web technologies, and has created novel challenges for non-relational data management systems, such as RDF and graph engines. Efficient data access through queries is perhaps the most important data management task, and is enabled through query optimization techniques, which amount to the discovery of optimal or close to optimal execution plans for a given query.\nIn this post, we propose a different approach to query optimization, which is meant to complement (rather than replace) the standard optimization methodologies for SPARQL queries. Our approach is based on the use of schema information, encoded using OWL constructs, which often accompany Linked Data.\nOWL adopts the Open World Assumption and hence OWL axioms are perceived primarily to infer new knowledge. Nevertheless, ontology designers consider OWL as an expressive schema language used to express constraints for validating the datasets, hence following the Closed World Assumption when interpreting OWL ontologies. Such constraints include disjointness/equivalence of classes/properties, cardinality constraints, domain and range restrictions for properties and others.\nThis richness of information carried over by OWL axioms can be the basis for the development of schema-aware techniques that will allow significant improvements in the performance of existing RDF query engines when used in tandem with data statistics or even other heuristics based on patterns found in SPARQL queries. As a simple example, a cardinality constraint at the schema level can provide a hint on the proper join ordering, even if data statistics are missing or incomplete.\nThe aim of this post is to show that the richness of information carried over by OWL axioms under the Close World Assumption can be the basis for the development of schema-aware optimization techniques that will allow considerable improvement for query processing. To attain this objective, we discuss a small set of interesting cases of OWL axioms; a full list can be found here.\nSchema-Based Optimization Techniques Here we provide some examples of queries, which, when combined with specific schema constraints expressed in OWL, can help the optimizer in formulating the (near to) optimal query plans.\nA simple first case is the case of constraint violation. Consider the query below, which returns all instances of class \u0026lt;A\u0026gt; which are fillers of a specific property \u0026lt;P\u0026gt;. If the underlying schema contains the information that the range of \u0026lt;P\u0026gt; is class \u0026lt;B\u0026gt;, and that class \u0026lt;B\u0026gt; is disjoint from class \u0026lt;A\u0026gt;, then this query should return the empty result, with no further evaluation (assuming that the constraints associated with the schema are satisfied by the data). An optimizer that takes into account schema information should return an empty result in constant time instead of trying to optimize or evaluate the large star join.\nSELECT ?v WHERE { ?v rdf : type \u0026lt;A\u0026gt; . ?u \u0026lt;P\u0026gt; ?v . ?u \u0026lt;P\u0026gt; ?v1 . ?u \u0026lt;P1 \u0026gt; ?v2 . ?u \u0026lt;P2 \u0026gt; ?v3 . ?u \u0026lt;P3 \u0026gt; ?v4 . ?u \u0026lt;P4 \u0026gt; ?v5} Schema-aware optimizers could also prune the search space by eliminating results that are known a priori not to be in the answer set of a query. The query above is an extreme such example (where all potential results are pruned), but other cases are possible, such as the case of the query below, where all subclasses of class \u0026lt;A1\u0026gt; can immediately be identified as not being in the answer set.\nSELECT ?c WHERE { ?x rdf: type ?c . ?x \u0026lt;P\u0026gt; ?y . FILTER NOT EXISTS \\{ ?x rdf: type \u0026lt;A1 \u0026gt; }} Another category of schema-empowered optimizations has to do with improved selectivity estimation. In this respect, knowledge about the cardinality (minimum cardinality, maximum cardinality, exact cardinality, functionality) of a property can be exploited to formulate better query plans, even if data statistics are incomplete, missing or erroneous.\nSimilarly, taking into account class hierarchies, or the definition of classes/properties via set theoretic constructs (union, intersection) at the schema level, can provide valuable information on the selectivity of certain triple patterns, thus facilitating the process of query optimization. Similar effects can be achieved using information about properties (functionality, transitivity, symmetry etc).\nAs an example of these patterns, consider the query below, where class \u0026lt;C\u0026gt; is defined as the intersection of classes \u0026lt;C1\u0026gt;, \u0026lt;C2\u0026gt;. Thus, the triple pattern (?x rdf:type \u0026lt;C\u0026gt;) is more selective than (?y rdf:type \u0026lt;C1\u0026gt;) and (?z rdf:type \u0026lt;C2\u0026gt;) and this should be immediately recognizable by the optimizer, without having to resort to cost estimations. This example shows also how unnecessary triple patterns can be pruned from a query to reduce the number of necessary joins. Figure 1 illustrates the query plan obtained when the OWL intersectionOf construct is used.\nSELECT ?x WHERE { ?x rdf: type \u0026lt;C\u0026gt; . ?x \u0026lt;P1 \u0026gt; ?y . ?y rdf : type \u0026lt;C1 \u0026gt; . ?y \u0026lt;P2 \u0026gt; ?z . ?z rdf : type \u0026lt;C2 \u0026gt; } Schema information can also be used by the query optimizer to rewrite SPARQL queries to equivalent ones that are found in a form for which already known optimization techniques are easily applicable. For example, the query below could easily be transformed into a classical star-join query if we know (from the schema) that property P4 is a symmetric property.\nSELECT ?y ?y1 ?y2 ?y3 WHERE { ?x \u0026lt;P1 \u0026gt; ?y . ?x \u0026lt;P2 \u0026gt; ?y1 . ?x \u0026lt;P3 \u0026gt; ?y2 . ?y3 \u0026lt;P4 \u0026gt; ?x } Conclusion In this post we argued that OWL-empowered optimization techniques can be beneficial for SPARQL query optimization when used in tandem with standard heuristics based on statistics. We provided some examples which showed the power of such optimizations in various cases, namely:\n Cases where the search space can be pruned due to the schema and the associated constraints; an extreme special sub-case is the identification of queries that violate schema constraints and thus produce no results. Cases where the schema can help in the estimation of triple pattern selectivity, even if statistics are incomplete or missing. Cases where the schema can identify redundant triple patterns that do not affect the result and can be safely eliminated from the query. Cases where the schema can be used for rewriting a query in an equivalent form that would facilitate optimization using well-known optimization techniques.  This list is by no means complete, as further cases can be identified by optimizers. Our aim in this post was not to provide a complete listing, but to demonstrate the potential of the idea in various directions.\n","permalink":"https://ldbc.github.io/post/owl-empowered-sparql-query-optimization/","tags":["developer","industry"],"title":"OWL-Empowered SPARQL Query Optimization"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confwww-pham-peb-15/","tags":[],"title":"Deriving an Emergent Relational Schema from RDF Data"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-journalsws-loizou-ag-15/","tags":[],"title":"On the formulation of performant SPARQL queries"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confsigmod-erling-alcgppb-15/","tags":[],"title":"The LDBC Social Network Benchmark: Interactive Workload"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confsigmod-guisado-gamez-p-14/","tags":[],"title":"Understanding Graph Structure of Wikipedia for Query Expansion"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-journalsercim-angles-pb-14/","tags":[],"title":"Benchmarking Linked Open Data Management Systems"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-bookscrclinked-14-boncz-ep-14/","tags":[],"title":"Experiences with Virtuoso Cluster RDF Column Store"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confedbt-gubichev-014/","tags":[],"title":"Exploiting the query structure for efficient join ordering in SPARQL queries"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confsigmod-gubichev-t-14/","tags":[],"title":"Graph Pattern Matching - Do We Have to Reinvent the Wheel?"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confwww-prat-perez-dl-14/","tags":[],"title":"High quality, scalable and parallel community detection for large real graphs"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confsigmod-prat-d-14/","tags":[],"title":"How community-like is the structure of synthetically generated graphs?"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-conficde-gubichev-ab-14/","tags":[],"title":"How to generate query parameters in RDF benchmarks?"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confrweb-larriba-pey-md-14/","tags":[],"title":"Introduction to Graph Databases"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-journalsercim-pham-b-14/","tags":[],"title":"MonetDB/RDF: Discovering and Exploiting the Emergent Schema of RDF Data"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-conftpctc-gubichev-b-14/","tags":[],"title":"Parameter Curation for Benchmark Queries"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-journalssigmod-angles-blf-0-enmkt-14/","tags":[],"title":"The Linked Data Benchmark Council: A graph and RDF industry benchmarking effort"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confsigmod-martinez-bazan-d-14/","tags":[],"title":"Using semijoin programs to solve traversal queries in graph databases"},{"categories":null,"contents":"The LDBC consortium is pleased to announce the third Technical User Community (TUC) meeting!\nThis will be a one day event in London on the 19 November 2013 running in collaboration with the GraphConnect event (18/19 November). Registered TUC participants that would like a free pass to all of GraphConnect should register for GraphConnect using this following coupon code: LDBCTUC.\nThe TUC event will include:\n Introduction to the objectives and progress of the LDBC project Description of the progress of the benchmarks being evolved through Task Forces Users explaining their use-cases and describing the limitations they have found in current technology Industry discussions on the contents of the benchmarks  We will also be launching the LDBC non-profit organization, so anyone outside the EU project will be able to join as a member.\nWe will kick off new benchmark development task forces in the coming year, and talks at this coming TUC will play an important role in deciding the use case scenarios that will drive those benchmarks.\nAll users of RDF and graph databases are welcome to attend. If you are interested, please contact: ldbc AT ac DOT upc DOT edu\n Agenda Logistics LDBC/TUC Background  Social Network Benchmark Semantic Publishing Benchmark     Agenda November 19th - Public TUC Meeting\n8:00 Breakfast and registration will open for Graph Connect/TUC at 8:00 am (Dexter House)\nshort LDBC presentation (Peter Boncz) during GraphConnect keynote by Emil Eifrem (09:00-09:30 Dexter House)\nNOTE: the TUC meeting is at the Tower Hotel, nearby Dexter House.\n10:00 TUC Meeting Opening (Peter Boncz)\n10:10 TUC Presentations (RDF Application Descriptions)\n Johan Hjerling (BBC): BBC Linked Data and the Semantic Publishing Benchmark Andreas Both (Unister): Ontology-driven applications in an e-commerce context Nuno Carvalho (Fujitsu Laboratories Europe): Fujitsu RDF use cases and benchmarking requirements Robina Clayphan (Europeana): Europeana and Open Data  11:30 Semantic Publishing Benchmark (SPB)\n Venelin Kotsev (Ontotext - LDBC): Semantic Publishing Benchmark Task Force Update and Report  12:00-13:00 Lunch at the Graph Connect venue\nTalks During Lunch:\n [Pedro Furtado, Jorge Bernardino (Univ. Coimbra): KEYSTONE Cost Action]  13:00 TUC Presentations (Graph Application Descriptions)\n Minqi Zhou / Weining Qian (East China Normal University): Elastic and realistic social media data generation Andrew Sherlock (Shapespace): Shapespace Use Case Sebastian Verheughe (Telenor): Real-time Resource Authorization  14:00 Social Network Benchmark (SNB)\n Norbert Martinez (UPC - LDBC): Social Network Benchmark Task Force Update and Report  14:30 Break\n14:45 TUC Presentations (Graph Analytics)\n Keith Houck (IBM): Benchmarking experiences with [System G Native Store (tentative title)] Abraham Bernstein (University of Zurich): Streams and Advanced Processing: Benchmarking RDF querying beyond the Standard SPARQL Triple Store Luis Ceze (University of Washington): Grappa and GraphBench Status Update  15:45 Break\n16:00 TUC Presentations* (Possible Future RDF Benchmarking Topics)*\n Christian-Emil Ore (Unit for Digital Documentation, University of Oslo, Norway): CIDOC-CRM [Atanas Kiryakov (Ontotext): Large-scale Reasoning with a Complex Cultural Heritage Ontology (CIDOC CRM) Kostis Kyzirakos (National and Kapodistrian University of Athens / CWI): Geographica: A Benchmark for Geospatial RDF Stores Xavier Lopez (Oracle): W3C Property Graph progress Thomas Scharrenbach (University Zurich) PCKS: Benchmarking Semantic Flow Processing Systems  17:20 Meeting Conclusion (Josep Larriba Pey)\n17:30 End of TUC meeting\n19:00 Social dinner\nNovember 20th - Internal LDBC Meeting\n10:00 Start\n12:30 End of meeting\n coffee and lunch provided  Logistics Date\n19th November 2013\nLocation\nThe TUC meeting will be held in The Tower hotel (Google Maps link) approximately 4 minutes walk from the GraphConnect conference in London.\nGetting there\n From City Airport is the easiest: short ride on the DLR to Tower Gateway. Easy. From London Heathrow: first need to take the Heathrow Express to Paddington. Then take the Circle line to Tower Hill. See attached.  Accomodation\nTower Hill is nice http://www.booking.com/hotel/gb/sleep-inn-city-of-london.en-us.html - book early to get a good rate\nSocial Dinner\nThe social dinner will take place at 7 pm on Nov 19.** TODO more details**\nTravel costs\n[There is some small budget available that can be used to assist some attendees that are otherwise unable to fund their trip. Please contact us using the following email address if you would like more information: ldbcgrants AT ac DOT upc DOT edu]\nLDBC/TUC Background Looking back, we have been working on two benchmarks for the past year: a Social Network Benchmark (SNB) and a Semantic Publishing Benchmark (SPB). While below we provide a short summary, all the details of the work on these benchmark development efforts can be found in the first yearly progress reports:\n LDBC_SNB_Report_Nov2013.pdf LDBC_SPB_Report_Nov2013.pdf  A summary of these efforts can be read below or, for a more detailed account, please refer to: The Linked Data Benchmark Council: a Graph and RDF industry benchmarking effort\nAnnual reports about the progress, results, and future work of these two efforts will soon be available for download here, and will be discussed in depth at the TUC.\nSocial Network Benchmark The Social Network Benchmark (SNB) is designed for evaluating a broad range of technologies for tackling graph data management workloads. The systems targeted are quite broad: from graph, RDF, and relational database systems to Pregel-like graph compute frameworks. The social network scenario was chosen with the following goals in mind:\n it should be understandable, and the relevance of managing such data should be understandable it should cover the complete range of interesting challenges, according to the benchmark scope the queries should be realistic, i.e., similar data and workloads are encountered in practice  SNB includes a data generator for creation of synthetic social network data with the following characteristics:\n data schema is representative of real social networks data generated includes properties occurring in real data, e.g. irregular structure, structure/value correlations, power-law distributions the software generator is easy-to-use, configurable and scalable  SNB is intended to cover a broad range of aspects of social network data management, and therefore includes three distinct workloads:\n Interactive  Tests system throughput with relatively simple queries and concurrent updates, it is designed to test ACID features and scalability in an online operational setting. The targeted systems are expected to be those that offer transactional functionality.   Business Intelligence  Consists of complex structured queries for analyzing online behavior of users for marketing purposes, it is designed to stress query execution and optimization. The targeted systems are expected to be those that offer an abstract query language.   Graph Analytics  Tests the functionality and scalability of systems for graph analytics, which typically cannot be expressed in a query language. Analytics is performed on most/all of the data in the graph as a single operation and produces large intermediate results, and it is not not expected to be transactional or need isolation. The targeted systems are graph compute frameworks though database systems may compete, for example by using iterative implementations that repeatedly execute queries and keep intermediate results in temporary data structures.    Semantic Publishing Benchmark The Semantic Publishing Benchmark (SPB) simulates the management and consumption of RDF metadata that describes media assets, or creative works.\nThe scenario is a media organization that maintains RDF descriptions of its catalogue of creative works \u0026ndash; input was provided by actual media organizations which make heavy use of RDF, including the BBC. The benchmark is designed to reflect a scenario where a large number of aggregation agents provide the heavy query workload, while at the same time a steady stream of creative work description management operations are in progress. This benchmark only targets RDF databases, which support at least basic forms of semantic inference. A tagging ontology is used to connect individual creative work descriptions to instances from reference datasets, e.g. sports, geographical, or political information. The data used will fall under the following categories: reference data, which is a combination of several Linked Open Data datasets, e.g. GeoNames and DBpedia; domain ontologies, that are specialist ontologies used to describe certain areas of expertise of the publishing, e.g., sport and education; publication asset ontologies, that describe the structure and form of the assets that are published, e.g., news stories, photos, video, audio, etc.; and tagging ontologies and the metadata, that links assets with reference/domain ontologies.\nThe data generator is initialized by using several ontologies and datasets. The instance data collected from these datasets are then used at several points during the execution of the benchmark. Data generation is performed by generating SPARQL fragments for create operations on creative works and executing them against the RDF database system.\nTwo separate workloads are modeled in SPB:\n Editorial Simulates creating, updating and deleting creative work metadata descriptions. Media companies use both manual and semi-automated processes for efficiently and correctly managing asset descriptions, as well as annotating them with relevant instances from reference ontologies. Aggregation Simulates the dynamic aggregation of content for consumption by the distribution pipelines (e.g. a web-site). The publishing activity is described as \u0026ldquo;dynamic\u0026rdquo;, because the content is not manually selected and arranged on, say, a web page. Instead, templates for pages are defined and the content is selected when a consumer accesses the page.  Attachments: article.pdf (application/pdf)\nldbc_tuc_london.pdf (application/pdf)\nLDBC_SPB_Report_Nov2013.pdf (application/pdf)\nLDBC_SNB_Report_Nov2013.pdf (application/pdf)\nLDBC_SPB_Report_Nov2013.pdf (application/pdf)\nldbc_tuc_19-11-2013_Europeana-Robina_Clayphan.ppt (application/vnd.ms-powerpoint)\nLDBC-TUC-Fujitsu-Final.pdf (application/pdf)\nLDBC London 19 Nov 2013 - Telenor Resource Authorization.pdf (application/pdf)\nKyzirakos-Geographica.pdf (application/pdf)\nBBC-JohanHjerling-LDBC-TUC-3.pdf (application/pdf)\nLDBC_TUC_wnqian.pdf (application/pdf)\nLondonLDBCKEYSTONE.pdf (application/pdf)\nLDBC-CRM-reasoning-Alexiev2013-AK.pdf (application/pdf)\nLDBC_TUC3_SNB.pdf (application/download)\nLDBC_Status of the Semantic Publishing Benchmark.pdf (application/download)\nandreas-both__linked-data-benchmark-concil__3rd-technical-user-meeting_2013.pdf (application/download)\\\n","permalink":"https://ldbc.github.io/event/third-tuc-meeting/","tags":["TUC Meeting"],"title":"Third TUC Meeting"},{"categories":null,"contents":"The LDBC consortium are pleased to announce the second Technical User Community (TUC) meeting.\nThis will be a two day event in Munich on the 22/23rd April 2013.\nThe event will include:\n Introduction to the objectives and progress of the LDBC project. Description of the progress of the benchmarks being evolved through Task Forces. Users explaining their use-cases and describing the limitations they have found in current technology. Industry discussions on the contents of the benchmarks.  All users of RDF and graph databases are welcome to attend. If you are interested, please contact: ldbc AT ac DOT upc DOT edu\nNews: due to the airline strikes at Lufthansa and El Al, Herman Ravkin (BIG project) and Jesús Lanchas (ACCESO) and Thomas Scharrenbach (University of Zurich) cannot come, and Norbert Martinez will arrive only tomorrow. The schedule has been adapted, and may change again, keep an eye on it.\n Agenda Slides Logistics  Date Location Venue  Getting to the TUM Campus from the Munich city center: Subway (U-Bahn) Getting to the TUM Campus from the Munich Airport Getting to the TUM Campus from Garching: U-Bahn   Getting there Accomodation Social Dinner Travel costs    Agenda April 22nd\n10:00 Registration.\n10:30 Josep Lluis Larriba Pey (UPC) - Welcome and Introduction.\n10:30 Peter Boncz (VUA): LDBC: goals and status\n *Social Network Use Cases (with discussion moderated by Josep  Lluis Larriba Pey)*\n11:00 Josep Lluis Larriba Pey (UPC): Social Network Benchmark Task Force\n11:30 Gustavo González (Mediapro): Graph-based User Modeling through Real-time Social Streams12:00 Klaus Großmann (Dshini): Neo4j at Dshini\n12:30 Lunch\n *Semantic Publishing Use Cases (with discussion moderated by  Barry Bishop)*\n13:30 Barry Bishop (Ontotext): Semantic Publishing Benchmark Task Force14:00 Dave Rogers (BBC): Linked Data Platform at the BBC14:30 Edward Thomas (Wolters Kluwer): Semantic Publishing at Wolters Kluwer\n15:00 coffee breakProjects Related to LDBC\n15:30 Fabian Suchanek (MPI): \u0026ldquo;YAGO: A large knowledge base from Wikipedia and WordNet\u0026rdquo;\n16:00 Antonis Loziou (VUA): The OpenPHACTS approach to data integration16:30 Mirko Kämpf (Brox): \u0026ldquo;GeoKnow - Spatial Data Web project and Supply Chain Use Case\u0026rdquo;17:00 End of first day\n19:00 Social dinner\nApril 23rd\n *Industry \u0026amp; Hardware Aspects*  10:00 Xavier Lopez (Oracle): Graph Database Performance an Oracle Perspective.pdf\n10:30 Pedro Trancoso (University of Cyprus): \u0026ldquo;Benchmarking and computer architecture: the research side\u0026rdquo;\n11:00 coffee break11:30 Peter Boncz (VUA) moderates: \u0026ldquo;next steps in the Social Networking Task Force\u0026rdquo;\n12:00 Barry Bishop (Ontotext) moderates: \u0026ldquo;next steps in the Semantic Publishing Task Force\u0026rdquo;\n12:30 End of meetingSlides The slides of the presentations during the meeting will we posted here.\nLogistics Date 22nd and 23th April 2013\nLocation The TUC meeting will be held at LE009 room at LRZ (Leibniz-Rechenzentrum) located inside the TU Munich campus in Garching, Germany. The address is:\nLRZ (Leibniz-Rechenzentrum)Boltzmannstraße 185748 Garching, Germany\nVenue To reach the campus, there are several options, including Taxi and Subway Ubahn\nGetting to the TUM Campus from the Munich city center: Subway (U-Bahn) Take the U-bahn line U6 in the direction of Garching-Forschungszentrum, exit at the end station. Take the south exit to MI-Building and LRZ on the Garching Campus. The time of the journey from the city center is approx.25-30 minutes. In order to get here from the City Center, you need the Munich XXL ticket that costs around 7.50 euros and covers all types of transportation for one day. The ticket has to be validated before ride.\nGetting to the TUM Campus from the Munich Airport  (except weekends) S-Bahn S8 line in the direction of (Hauptbahnhof) Munich Central Station until the third stop, Ismaning (approx. 13 minutes). From here Bus Nr. 230 until stop MI-Building on the Garching Campus. Alternatively: S1 line until Neufahrn, then with the Bus 690, which stops at Boltzmannstraße. S-Bahn lines S8 or S1 towards City Center until Marienplatz stop. Then change to U-bahn U6 line towards Garching-Forschungszentrum, exit at the last station. Take the south exit to MI-Building and LRZ. Taxi: fare is ca,30-40 euros.  For cases 1 and 2, before the trip get the One-day Munich Airport ticket and validate it. It will cover all public transportation for that day.\nGetting to the TUM Campus from Garching: U-Bahn The city of Garching is located on the U6 line, one stop before the Garching-Forschungszentrum**.** In order to get from Garching to Garching-Forschungszentrum with the U-bahn, a special one-way ticket called Kurzstrecke (1.30 euros) can be purchased.\nFinding LRZ@TUM\nOpenStreetMap link\nGoogleMaps link\nGetting there Flying: Munich airport is located 28.5 km northeast of Munich. There are two ways to get from the airport to the city center: suburban train (S-bahn) and Taxi.\nS-Bahn:\nS-bahn lines S1 and S8 will get you from the Munich airport to the city center, stopping at both Munich Central Station (Hauptbahnhof) and Marienplatz. One-day Airport-City ticket costs 11.20 euros and is valid for the entire Munich area public transportation during the day of purchase (the tickets needs to be validated before the journey). S-bahn leaves every 5-20 minutes and reaches the city center in approx.40 minutes\nTaxi: taxi from the airport to the city center costs approximately 50 euros\nAccomodation The following hotels are recommended. The first two are located near the TUM campus in this google map\nHotel Koenig Ludwig II\nBürgerplatz 3,\nGarching, Germany\nTel: +49 89 320 50 46\nhttp://www.hkl.de/home-en/ Hotel Am Park\nBürgermeister-Amon-Straße 2,\nGarching, Germany\nTel: +49 89 320 40 84\nhttp://hotel-am-park.com/ Hotel-Pension Carolin\nKaulbachstraße 42\nMunich, Germany\nTel: +49 89 34 57 57\nhttp://www.pension-carolin.com/ Cosmopolitan Hotel\nHohenzollernstrasse 5\nMunich, Germany\nTel: +49 89 38 38 10\nhttp://www.cosmopolitanhotel.de/en/ Hotel Pullmann\nTheodor-Dombart-Straße 4\nMunich, Germany\nTel: +49 89 360 99 0\nhttp://www.pullman-hotel-munich.com/default-en.htmlSocial Dinner The social dinner will take place at 7 pm on April 22 in Hofbräuhaus (second floor)\nAddress: Hofbräuhaus, Platzl 9, Munich\nGoogleMaps link\nTravel costs There is some small budget available that can be used to assist some attendees that are otherwise unable to fund their trip. Please contact us using the following email address if you would like more information: ldbcgrants AT ac DOT upc DOT edu\nAttachments: e19940cef9.png (image/png)\ngetBuildingMap.gif (image/gif)\nmapbigger.gif (image/gif)\n5d84f31b1f.png (image/png)\nBBC_LDBC_presentation_Dave_Rogers.pptx (application/vnd.openxmlformats-officedocument.presentationml.presentation)\nDshini_LDBC_Klaus_Grossman.pdf (application/pdf)\nOntotext_LDBC_Barry_Bishop_Publishing.pptx (application/vnd.openxmlformats-officedocument.presentationml.presentation)\nMediaPro_LDBC_Gustavo_Gonzalez-Sanchez.pdf (application/pdf)\nUPC_LDBC_Norbert_Martinez.pdf (application/pdf)\nCWI_LDBC_Peter_Boncz.pptx (application/vnd.openxmlformats-officedocument.presentationml.presentation)\nWolters_Kluwer_LDBC_Edward_Thomas.pdf (application/pdf)\nOpenPhacts_LDBC_Antonis_Loizou.pptx (application/vnd.openxmlformats-officedocument.presentationml.presentation)\nGraph Database Performance an Oracle Perspective.pdf (application/pdf)\n","permalink":"https://ldbc.github.io/event/second-tuc-meeting/","tags":["TUC Meeting"],"title":"Second TUC Meeting"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confamw-angles-br-13/","tags":[],"title":"A Practical Query Language for Graph DBs"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confsigmod-angles-pdl-13/","tags":[],"title":"Benchmarking database systems for social network applications"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confsigmod-ma-wqyxz-13/","tags":[],"title":"On benchmarking online social media analytical queries"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-journalscorrabs-1301-5121/","tags":[],"title":"Partitioning Graph Databases - A Quantitative Evaluation"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-conficde-pham-13/","tags":[],"title":"Self-organizing structured RDF in MonetDB"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confsigmod-gubichev-bs-13/","tags":[],"title":"Sparqling Kleene: fast property paths in RDF-3X"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-journalsdbsk-boncz-fgl-013/","tags":[],"title":"The Linked Data Benchmark Council Project"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confsigmod-cattuto-qpa-13/","tags":[],"title":"Time-varying social networks in a graph database: a Neo4j use case"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-conftpctc-boncz-ne-13/","tags":[],"title":"TPC-H Analyzed: Hidden Messages and Lessons Learned from an Influential Benchmark"},{"categories":null,"contents":"The LDBC consortium are pleased to announce the first Technical User Community (TUC) meeting. This will be a two day event in Barcelona on the 19/20th November 2012.\nSo far more than six commercial consumers of graph/RDF database technology have expressed an interest in attending the event and more are welcome. The proposed format of the event wil include:\n Introduction by the coordinator and technical director explaining the objectives of the LDBC project Invitation to users to explain their use-cases and describe the limitations they have found in current technology Brain-storming session for identifying trends and mapping out strategies to tackle existing choke-points  The exact agenda will be published here as things get finalised before the event.\nAll users of RDF and graph databases are welcome to attend. If you are interested, please contact: ldbc AT ac DOT upc DOT edu\n Agenda Slides Logistics  Date Location Venue Getting there Accomodation Travel costs    Agenda We will start at 9:00 on Monday for a full day, followed by a half a day on Tuesday to allow attendees to travel home on the evening of the 20th.\nDay 1\n09:00 Welcome (Location: Aula Master)\n09:30 Project overview (Enphasis on task forces?) + Questionnaire results?\n10:30 Coffee break\n11:00 User talks (To gather information for use cases?)\n13:00 Lunch\n14:00 User talks (cont.)\n15:00 Use case discussions (based on questionnaire results + consortium proposal + user talks).\n16:00 Task force proposals (consortium)\n17:00 Finish first day\n20:00 Social dinner\nDay 2\n10:00 Task force discussion (consortium + TUC)\n11:00 Coffe break\n11:30 Task force discussion (consortium + TUC)\n12:30 Summaries (Task forces, use cases, \u0026hellip;) and actions\n13:00 Lunch and farewell\n15:00 LDBC Internal meeting\nSlides Opening session:\n CWI - Peter Boncz - Objectives UPC - Larri - Questionnaire  User stories:\n BBC - Jem Rayfield CA Technologies - Victor Muntés Connected Discovery (Open Phacts) - Bryn Williams-Jones Elsevier - Alan Yagoda ERA7 Bioinformatics- Eduardo Pareja Press Association - Jarred McGinnis RJLee - David Neuer Yale - Lec Maj  Benchmark proposals:\n Publishing benchmark proposal - Ontotext - Barry Bishop Social Network Benchmark Proposal - UPC - Larri  Logistics Date 19th and 20th November 2012\nLocation The TUC meeting will be held at “Aula Master” at A3 building located inside the “Campus Nord de la UPC” in Barcelona. The address is:\nAula Master\nEdifici A3, Campus Nord UPC\nC. Jordi Girona, 1-3\n08034 Barcelona, Spain\nVenue To reach the campus, there are several options, including Taxi, Metro and Bus.\nFinding UPC\nFinding the meeting room\nGetting there Flying: Barcelona airport is situated 12 km from the city. There are several ways of getting from the airport to the centre of Barcelona, the cheapest of which is to take the train located outside just a few minutes walking distance past the parking lots at terminal 2 (there is a free bus between terminal 1 and terminal 2, see this map of the airport. It is possible to buy 10 packs of train tickets which makes it cheaper. Taking the bus to the centre of town is more convenient as they leave directly from terminal 1 and 2, however it is more expensive than the train.\nRail: The Renfe commuter train leaves the airport every 30 minutes from 6.13 a.m. to 11.40 p.m. Tickets cost around 3€ and the journey to the centre of Barcelona (Sants or Plaça Catalunya stations) takes 20 minutes.\nBus: The Aerobus leaves the airport every 12 minutes, from 6.00 a.m. to 24.00, Monday to Friday, and from 6.30 a.m. to 24.00 on Saturdays, Sundays and public holidays. Tickets cost 6€ and the journey ends in Plaça Catalunya in the centre of Barcelona.\nTaxi: From the airport, you can take one of Barcelona\u0026rsquo;s typical black and yellow taxis. Taxis may not take more than four passengers. Unoccupied taxis display a green light and have a clearly visible sign showing LIBRE or LLIURE. The trip to Sants train station costs approximately €16 and trips to other destinations in the city cost approximately €18.\n**Train and bus: **Barcelona has two international train stations: Sants and França. Bus companies have different points of arrival in different parts of the city. You can find detailed information in the following link: http://www.barcelona-airport.com/eng/transport_eng.htm\nThe locations of the airport and the city centre\nAccomodation The following hotels are recommended. The two first are located near the UPC campus and they take 10-15 min by foot to reach the TUC meeting location. The two last are located at the city center. They require about 30 min (taking metro L3 at plaça Catalunya) to reach the TUC meeting location. You can see the hotel locations in this google map.\nHotel Husa Pedralbes\nFontcoberta, 4\n8034 Barcelona\n932 037 112\nhttp://www.hotelhusapedralbes.com/\n Hotel Bonanova Park\nCapita Arenas, 51\n08034 Barcelona\n932 04 09 00\nhttp://www.hotelbonanovapark.com/\n Hotel Inglaterra Barcelona\nCarrer de Pelai, 14\n08001 Barcelona\n935 05 11 00\nhttp://www.hotel-inglaterra.com/\n Hotel Jazz\nCarrer de Pelai, 3\n08001 Barcelona\n935 52 96 96\nhttp://www.hoteljazz.com/\nIn addition to the hotels above, there is the possibility to stay at the \u0026ldquo;Torre Girona\u0026rdquo; residence. It is the closest and cheapest option available. You can find detailed information here. Basically, it costs 53 euros for a single room and 60 to 88 euros for a double room depending if it is occupied by one or two people. Currently, there are 20 individual and 20 double rooms free for these days. If anyone is interested in this option, you should send an email to torregirona@resa.es asking for a reservation.\nTorre Girona Residence Hall\nPasseig dels Tillers, 19\n08034 Barcelona\nTelephone: 0034 93 390 43 00\nFax: 0034 93 205 69 10\nE-mail: torregirona@resa.es\nTravel costs There is some small budget available that can be used to assist some attendees that are otherwise unable to fund their trip. Please contact us using the following email address if you would like more information: ldbcgrants AT ac DOT upc DOT edu\nAttachments: upc_map.jpg (image/jpeg)\nbarcelona_map.jpg (image/jpeg)\nbus_map.jpg (image/jpeg)\nmeeting_room_map.jpg (image/jpeg)\nldbc_tuc_19-11-2012_Ontotext-Barry_Bishop.pdf (application/force-download)\nldbc_tuc_19-11-2012_Yale-Lec_Maj.pdf (application/force-download)\nldbc_tuc_19-11-2012_CWI-Peter_Boncz.pptx (application/vnd.openxmlformats-officedocument.presentationml.presentation)\nldbc_tuc_19-11-2012_BBC-Jem_Rayfield.pdf (application/force-download)\nERA7_BIOINFORMATICS_Bio4j_LDBC_TUC_meeting_Nov_2012.pptx (application/vnd.openxmlformats-officedocument.presentationml.presentation)\nldbc_tuc_19-11-2012_Questionnaire.pdf (application/pdf)\nldbc_tuc_19-11-2012_social_network_taskforce_proposal.pdf (application/pdf)\nldbc_tuc_19-11-2012_Elsevier-Alan_Yagoda.pptx (application/vnd.openxmlformats-officedocument.presentationml.presentation)\nldbc_tuc_19-11-2012_RJLee-David_Neuer.pptx (application/vnd.openxmlformats-officedocument.presentationml.presentation)\nldbc_tuc_19-11-2012_Press_Association-Jarred_McGinnis.pptx (application/vnd.openxmlformats-officedocument.presentationml.presentation)\n","permalink":"https://ldbc.github.io/event/first-tuc-meeting/","tags":["TUC Meeting"],"title":"First TUC Meeting"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confcikm-gubichev-n-12/","tags":[],"title":"Fast approximation of Steiner trees in large graphs"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confedbt-tsialiamanis-sfcb-12/","tags":[],"title":"Heuristics-based query optimisation for SPARQL"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-confsemweb-phuoc-dpbef-12/","tags":[],"title":"Linked Stream Data Processing Engines: Facts and Figures"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-conftpctc-pham-be-12/","tags":[],"title":"S3G2: A Scalable Structure-Correlated Social Graph Generator"},{"categories":[],"contents":"","permalink":"https://ldbc.github.io/publication/dblp-journalsdebu-erling-12/","tags":[],"title":"Virtuoso, a Hybrid RDBMS/Graph Column Store"},{"categories":null,"contents":"This file is here so that Hugo generates the member page.\n","permalink":"https://ldbc.github.io/current-members/placeholder/","tags":null,"title":""},{"categories":null,"contents":"Redirecting\u0026hellip;\n","permalink":"https://ldbc.github.io/events/","tags":null,"title":""},{"categories":null,"contents":"","permalink":"https://ldbc.github.io/pages/page-name/","tags":null,"title":""},{"categories":null,"contents":"Redirecting\u0026hellip;\n","permalink":"https://ldbc.github.io/posts/","tags":null,"title":""},{"categories":null,"contents":"Benefits The benefits of LDBC membership are:\n Access to the internal LDBC information via its wiki and mailing lists, which includes access to discussion documents, graph user scenario\u0026rsquo;s, datasets, draft benchmark specifications and software, results and discussions. Right to membership of LDBC task forces. Currently, there are task forces on the Semantic Publishing Benchmark, the Social Network Benchmark Interactive, BI and the Graphalytics workloads, as well as the Graph Query Language task force. Access to research resources at academic partners. This includes the ability to look into research agenda\u0026rsquo;s provide feedback and establish working relationships with students, as well as arrange targeted internships with MSc and PhD students provided by these partners.  Access fillable membership form here 2021 one-year membership dues  Individual Member: EUR 150 (reduced rate: EUR 10) Non-Profit Organization Member: EUR 1000 Company Member: EUR 2000 Sponsor Company Member: EUR 8000  ","permalink":"https://ldbc.github.io/becoming-a-member/","tags":null,"title":"Becoming a Member"},{"categories":null,"contents":"This page is only a placeholder, contents will be added later.\nLicensing Will be added later.\nContributor Agreements Will be added later.\n","permalink":"https://ldbc.github.io/developer-community/","tags":null,"title":"Developer Community"},{"categories":null,"contents":"The Graphalytics benchmark is an industrial-grade benchmark for graph analysis platforms such as Giraph. It consists of six core algorithms, standard datasets, synthetic dataset generators, and reference outputs, enabling the objective comparison of graph analysis platforms.\nThe design of the benchmark process takes into account that graph-processing is impeded by three dimensions of diversity: platform, algorithms and dataset.\nThe benchmark harness consists of a core component, which is extendable by a driver for each different platform implementation. The choice of the six algorithms:\n breadth-first search, PageRank, weakly connected components, community detection using label propagation, local clustering coefficient, and single-source shortest paths  was carefully motivated, using the LDBC TUC and extensive literature surveys to ensure good coverage of scenarios. The standard datasets include both real and synthetic datasets, which are classified into intuitive “T-shirt” sizes (e.g., S, M, L, XL).\nEach experiment set in Graphalytics consists of multiple platform runs (a platform executes an algorithm on a dataset), and diverse set of experiments are carried out to evaluate different performance characteristics of a system-under-test.\nAll completed benchmarks must go through a strict validation process to ensure the integrity of the performance results.\nThe Graphalytics benchmarking process is made future-proof, through a renewal process that takes place regularly to ensure that the benchmark process meets the state-of-the-art development in the field of graph analytics.\nTo enhance the depth of the benchmark process, Graphalytics also facilitates a plugin-architecture, which allows external software tools to be added to the benchmark harness. For instance, it is possible to also use SNB Datagen (the data generator of the LDBC Social Network Benchmark), an advanced synthetic dataset generator to create synthetic graphs for custom test scenarios, or to use Granula, a fine-grained performance evaluation tool to obtain enriched performance results.\nThe development of Graphalytics is supported by many active vendors in the field of large-scale graph analytics. Currently, Graphalytics already facilitates benchmark for a large number of graph analytics platforms, such as Giraph, GraphX, GraphMat, OpenG, and PGXD, allowing comparison of the state-of-the-art system performance of both community-driven and industrial-driven platforms. To get started, the details of the Graphalyics documentation and its software components are described below.\n","permalink":"https://ldbc.github.io/benchmarks/graphalytics/","tags":null,"title":"LDBC Graphalytics Benchmark (LDBC-Graphalytics)"},{"categories":null,"contents":"The Semantic Publishing Benchmark (SPB) is an LDBC benchmark for testing the performance of RDF engines inspired by the Media/Publishing industry. In particular, LDBC worked with British Broadcasting Corporation BBC to define this benchmark, for which BBC donated workloads, ontologies and data. The publishing industry is an area where significant adoption of RDF is taking place.\nThere have been many academic benchmarks for RDF but none of these are truly industrial-grade. The SPB combines a set of complex queries under inference with continuous updates and special failover tests for systems implementing replication.\nSPB performance is measured by producing a workload of CRUD (Create, Read, Update, Delete) operations which are executed simultaneously. The benchmark offers a data generator that uses real reference data to produce datasets of various sizes and tests the scalability aspect of RDF systems. The benchmark workload consists of (a) editorial operations that add new data, alter or delete existing (b) aggregation operations that retrieve content according to various criteria. The benchmark also tests conformance for various rules inside the OWL2-RL rule-set.\nThe SPB specification contains the description of the benchmark and the data generator and all information about its software components can be found on the SPB developer page.\nSemantic Publishing Benchmark (SPB) Audited Results for Scale Factors SF1 - 64M, SF3 - 256M and SF5 - 1G triples are shown below.    Scale Factor Interactive (Q/s) Updates (ops/sec) Analytical Cost Software Hardware Test Sponsor Date Full Disclosure Report     1 100.85 10.19 n.a. €37,504 GraphDB EE6.2 Xeon1650v3 6-core 3.5Ghz 96GB RAM ONTOTEXT AD 2015/04/26 Full Disclosure Report   1 142.7588 10.6725 n.a €35,323 GraphDB SE 6.3 alpha CPU Intel Xeon E5-1650 v3 3.5Ghz,15MB L3 cache, s2011 ONTOTEXT AD 2015/06/10 Full Disclosure Report   3 29.90 9.50 n.a. €37,504 GraphDB EE6.2 Xeon1650v3 6-core 3.5Ghz 96GB RAM ONTOTEXT AD 2015/04/26 Full Disclosure Report   3 54.6364 9.4967 n.a €35,323 GraphDB SE 6.3 alpha CPU Intel Xeon E5-1650 v3 3.5Ghz,15MB L3 cache, s2011 ONTOTEXT AD 2015/06/10 Full Disclosure Report   1 149.0385 156.8325 n.a. $20,213 (€17,801 rate of 21/06/2015) Virtuoso Opensource Version 7.50.3213 Intel Xeon E5-2630, 6x 2.30GHz, Sockel 2011, boxed, 192 GB RAM OpenLink Software 2015/06/09 Full Disclosure Report   3 80.6158 92.7072 n.a. $20,213 (€17,801 rate of 21/06/2015) Virtuoso Opensource Version 7.50.3213 Intel Xeon E5-2630, 6x 2.30GHz, Sockel 2011, boxed, 192 GB RAM OpenLink Software 2015/06/09 Full Disclosure Report   3 115.3838 109.8517 n.a. $24,528 (€21,601 rate 21/06/2015) Virtuoso Opensource Version 7.50.3213 Amazon EC2, r3.8xlarge OpenLink Software 2015/06/09 Full Disclosure Report   5 32.2789 72.7192 n.a. $20,213 (€17,801 rate 21/06/2015) Virtuoso Opensource Version 7.50.3213 Intel Xeon E5-2630, 6x 2.30GHz, Sockel 2011, boxed, 192 GB RAM OpenLink Software 2015/06/09 Full Disclosure Report   5 45.8101 55.4467 n.a $24,528 (€21,601 rate 21/06/2015) Virtuoso Opensource Version 7.50.3213 Amazon EC2, r3.8xlarge OpenLink Software 2015/06/10 Full Disclosure Report    ","permalink":"https://ldbc.github.io/benchmarks/spb/","tags":null,"title":"LDBC Semantic Publishing Benchmark (LDBC-SPB)"},{"categories":null,"contents":"The Social Network Benchmark consists in fact of three distinct benchmarks on a common dataset, since there are three different workloads. Each workload produces a single metric for performance at the given scale and a price/performance metric at the scale. The full disclosure further breaks down the composition of the metric into its constituent parts, e.g. single query execution times.\n The Social Network Benchmark's Interactive workload, a benchmark focusing on transactional graph processing with complex read queries that access the neighbourhood of a given node in the graph and update operations that continuously insert new data in the graph. The Social Network Benchmark's Business Intelligence workload, a benchmark that focuses on aggregation- and join-heavy complex queries touching a large portion of the graph with microbatches of insert/delete update operations. This workload is not yet finalized.  Social Network Benchmark Interactive (version 0.3.2) audited results TuGraph TuGraph was audited in July 2020. TuGraph is owned by Ant Group now.\n   SF Throughput (ops/sec) Cost Software Hardware Test Sponsor Date Full Disclosure Report     30 5,436.47 $280,650 TuGraph 1.10 AWS r5d.12xlarge instance, 48*Intel Xeon Platinum 8175M @ 2.5GHz, 374GB RAM FMA 2020/07/26 Full Disclosure Report   100 5,010.77 $280,650 TuGraph 1.10 AWS r5d.12xlarge instance, 48*Intel Xeon Platinum 8175M @ 2.5GHz, 374GB RAM FMA 2020/07/26 Full Disclosure Report   300 4,855.52 $280,650 TuGraph 1.10 AWS r5d.12xlarge instance, 48*Intel Xeon Platinum 8175M @ 2.5GHz, 374GB RAM FMA 2020/07/26 Full Disclosure Report    Supplementary material for the TuGraph audits:\n Executive summary Signatures Attachments  Social Network Benchmark Interactive (version 0.2.2) audited results    SF Throughput (ops/sec) Cost Software Hardware Test Sponsor Date Full Disclosure Report     10 101.20 €30,427 Sparksee 5.1.1 2*Xeon 2630v3 8-core 2.4GHz, 256GB RAM Sparsity Technologies SA 2015/04/27 Full Disclosure Report   30 1287.17 €20,212 Virtuoso 07.50.3213 v7fasttrack 2*Xeon2630 6-core 2.4GHz, 192GB RAM OpenLink Software 2015/04/27 Full Disclosure Report   30 86.50 €30,427 Sparksee 5.1.1 2*Xeon 2630v3 8-core 2.4GHz, 256GB RAM Sparsity Technologies SA 2015/04/27 Full Disclosure Report   100 1200.00 €20,212 Virtuoso 07.50.3213 v7fasttrack 2*Xeon2630 6-core 2.4GHz, 192GB RAM OpenLink Software 2015/04/27 Full Disclosure Report   100 81.70 €37,927 Sparksee 5.1.1 2*Xeon 2630v3 8-core 2.4GHz, 256GB RAM Sparsity Technologies SA 2015/04/27 Full Disclosure Report   300 635 €20,212 Virtuoso 07.50.3213 v7fasttrack 2*Xeon2630 6-core 2.4GHz, 192GB RAM OpenLink Software 2015/04/27 Full Disclosure Report    ","permalink":"https://ldbc.github.io/benchmarks/snb/","tags":null,"title":"LDBC Social Network Benchmark (LDBC-SNB)"}]