<!DOCTYPE html>

<html lang="en-us"><head>
  <meta charset="utf-8">
  <title>Speeding Up LDBC SNB Datagen</title>

  <!-- mobile responsive meta -->
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Linked Data Benchmark Council Website">
  
  <meta name="author" content="LDBC">
  <meta name="generator" content="Hugo 0.80.0" />

  <!-- plugins -->
  
  <link rel="stylesheet" href="https://ldbc.github.io/plugins/bootstrap/bootstrap.min.css">
  
  <link rel="stylesheet" href="https://ldbc.github.io/plugins/slick/slick.css">
  
  <link rel="stylesheet" href="https://ldbc.github.io/plugins/fontawesome/font-awesome.min.css">
  
  <link rel="stylesheet" href="https://ldbc.github.io/plugins/animate/animate.css">
  
  <link rel="stylesheet" href="https://ldbc.github.io/plugins/venobox/venobox.css">
  

  <!-- Main Stylesheet -->
  
  <link rel="stylesheet" href="https://ldbc.github.io/scss/style.min.css" media="screen">

  <!--Favicon-->
  <link rel="shortcut icon" href="https://ldbc.github.io/images/favicon.png " type="image/x-icon">
  <link rel="icon" href="https://ldbc.github.io/images/favicon.png " type="image/x-icon">



  <!-- Custom js -->
  <script type="module" type="text/javascript" src="https://ldbc.github.io/js/isotope.pkgd.min.js"></script><script type="module" type="text/javascript" src="https://ldbc.github.io/js/wowchemy.js"></script>

</head><body>
<!-- preloader start -->
<div class="preloader">
  
</div>
<!-- preloader end -->
<!-- header -->
<header>
  

  <!-- navigation -->
  <div class="navigation bg-white position-relative">
    <div class="container">
      <nav class="navbar navbar-expand-lg navbar-light bg-white">
        <a class="navbar-brand" href="/"><img class="img-fluid pb-lg-3" src="https://ldbc.github.io/images/ldbc-title.png" alt="Linked Data Benchmark Council"></a>
        <button class="navbar-toggler border-0" type="button" data-toggle="collapse" data-target="#navigation"
          aria-controls="navigation" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>

        <div class="collapse navbar-collapse text-center" id="navigation">
          <ul class="navbar-nav ml-auto">
            <li class="nav-item">
              <a class="nav-link" href="/">Home</a>
            </li>
            
            
            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" href="#" role="button" data-toggle="dropdown" aria-haspopup="true"
                aria-expanded="false">
                Benchmarks
              </a>
              <div class="dropdown-menu" >
                
                <a class="dropdown-item" href="/benchmark-graphalytics">Graphalytics Benchmark</a>
                
                <a class="dropdown-item" href="/benchmark-spb">Semantic Publishing Benchmark</a>
                
                <a class="dropdown-item" href="/benchmark-snb">Social Network Benchmark</a>
                
              </div>
            </li>
            
            
            
            <li class="nav-item dropdown">
              <a class="nav-link dropdown-toggle" href="#" role="button" data-toggle="dropdown" aria-haspopup="true"
                aria-expanded="false">
                Membership
              </a>
              <div class="dropdown-menu" >
                
                <a class="dropdown-item" href="/current-members">Current Members</a>
                
                <a class="dropdown-item" href="/becoming-a-member">Becoming a Member</a>
                
              </div>
            </li>
            
            
            
            <li class="nav-item">
              <a class="nav-link" href="/event">Events</a>
            </li>
            
            
            
            <li class="nav-item">
              <a class="nav-link" href="/post">Posts</a>
            </li>
            
            
            
            <li class="nav-item">
              <a class="nav-link" href="/publication">Publications</a>
            </li>
            
            
          </ul>

          
          

          
          
        </div>
      </nav>
    </div>
  </div>
  <!-- /navigation -->
</header>
<!-- /header -->

<!-- page title -->
<section class="section bg-cover overlay"> 
  <div class="container">
    <div class="row">
      <div class="col-12">
        
        
        <h2 class="text-white mb-3">Speeding Up LDBC SNB Datagen</h2>
        
        <!-- breadcrumb -->
        <nav aria-label="breadcrumb">
          <ol class="breadcrumb bg-transparent p-0">
            <li class="breadcrumb-item font-weight-semebold"><a class="text-white" href="/">Home</a></li>
        
            <li class="breadcrumb-item font-weight-semebold active text-primary" aria-current="page">Speeding Up LDBC SNB Datagen</li>
        
          </ol>
        </nav>
        
      </div>
    </div>
  </div>
</section>
<!-- /page title -->


<!-- blog details -->
<section class="section">
  <div class="container">
    <div class="row">
      <div class="col-lg-8">
        <!-- post thumb -->
        
        <div class="card-meta text-uppercase mb-2">by <strong class="text-color"><a href="https://www.linkedin.com/in/dszakallas/">Dávid Szakállas</a></strong>/ on <strong class="text-dark">24 Jun 2021</strong></div>
        
        
          <i>Guest post.</i>
        
        
        <div class="position-relative mb-5">
          
          
        </div>
        <div class="content">
          <p>LDBC&rsquo;s <a href="#snb">Social Network Benchmark</a> (LDBC SNB) is an industrial and
academic initiative, formed by principal actors in the field of
graph-like data management. Its goal is to define a framework where
different graph-based technologies can be fairly tested and compared,
that can drive the identification of systems' bottlenecks and required
functionalities, and can help researchers open new frontiers in
high-performance graph data management.</p>
<p>LDBC SNB provides <a href="https://github.com/ldbc/ldbc_snb_datagen">Datagen</a>
(Data Generator), which produces synthetic datasets, mimicking a social
network&rsquo;s activity during a period of time. Datagen is defined by the
charasteristics of realism, scalability, determinism and usability. To
address scalability in particular, Datagen has been implemented on the
MapReduce computation model to enable scaling out across a distributed
cluster. However, since its inception in the early 2010s there has been
a tremendous amount of development in the big data landscape, both in
the sophistication of distributed processing platforms, as well as
public cloud IaaS offerings. In the light of this, we should reevaluate
this implementation, and in particular, investigate if Apache Spark
would be a more cost-effective solution for generating datasets on the
scale of tens of terabytes, on public clouds such as Amazon Web Services
(AWS).</p>
<h2 id="overview">Overview</h2>
<p>The benchmark&rsquo;s specification describes a social network <a href="https://github.com/ldbc/ldbc_snb_docs/blob/dev/figures/schema.pdf">data
model</a>
which divides its components into two broad categories: static and
dynamic. The dynamic element consists of an evolving network where
people make friends, post in forums, comment or like each others posts,
etc. In contrast, the static component contains related attributes such
as countries, universities and organizations and are fixed values. For
the detailed specifications of the benchmark and the Datagen component,
see <a href="#References">References</a>.</p>
<p>Datasets are generated in a multi-stage process captured as a sequence
of MapReduce steps (shown in the diagram below).</p>
<p><img src="/post/speeding-up-ldbc-snb-datagen/datagen_flow.png" alt=""></p>
<p><em>Figure 1. LDBC SNB Datagen Process on Hadoop</em></p>
<p>In the initialization phase dictionaries are populated and distributions
are initialized. In the first generation phase persons are synthesized,
then relationships are wired between them along 3 dimensions
(university, interest and random). After merging the graph of person
relationships, the resulting dataset is output. Following this,
activities such as forum posts, comments, likes and photos are generated
and output. Finally, the static components are output.</p>
<p><em>Note: The diagram shows the call sequence as implemented. All steps are
sequential &ndash; including the relationship generation &ndash;, even in cases
when the data dependencies would allow for parallelization.</em></p>
<p>Entities are generated by procedural Java code and are represented as
POJOs in memory and as sequence files on disk. Most entities follow a
shallow representation, i.e foreign keys (in relational terms) are
mapped to integer ids, which makes serialization
straightforward.<a href="#fn1">^1^</a> A notable exception is the Knows edge which
contains only the target vertex, and is used as a navigation property on
the source Person. The target Person is replaced with only the foreign
key augmented with some additional information in order to keep the
structure free of cycles. Needless to say, this <em>edge as property</em>
representation makes the data harder to handle in SQL than it would be
with a flat join table.</p>
<p>Entity generation amounts to roughly one fifth of the main codebase. It
generates properties drawn from several random distributions using
mutable pRNGs. Determinism is achieved by initializing the pRNGs to
seeds that are fully defined by the configuration with constants, and
otherwise having no external state in the logic.<a href="#fn2">^2^</a></p>
<p>Serialization is done by hand-written serializers for the supported
output formats (e.g. CSV) and comprises just a bit less than one third
of the main codebase. Most of the output is created by directly
interacting with low-level HDFS file streams. Ideally, this code should
be migrated to higher-level writers that handle faults and give
consistent results when the task has to be restarted.</p>
<h2 id="motivations-for-the-migration">Motivations for the migration</h2>
<p>The application is written using Hadoop MapReduce, which is now largely
superseded by more modern distributed batch processing platforms,
notably Apache Spark. For this reason, it was proposed to migrate
Datagen to Spark. The migration provides the following benefits:</p>
<ul>
<li>
<p><strong>Better memory utilization:</strong> MapReduce is disk-oriented, i.e. it
writes the output to disk after each reduce stage which is then read
by the next MapReduce job. As public clouds provide virtual machines
with sufficient RAM to encapsulate any generated dataset, time and
money are wasted by the overhead this unnecessary disk I/O incurs.
Instead, the intermediate results should be cached in memory where
possible. The lack of support for this is a well-known limitation of
MapReduce.</p>
</li>
<li>
<p><strong>Smaller codebase:</strong> The Hadoop MapReduce library is fairly
ceremonial and boilerplatey. Spark provides a higher-level
abstraction that is simpler to work with, while still providing
enough control on the lower-level details required for this
workload.</p>
</li>
<li>
<p><strong>Small entry cost:</strong> Spark and MapReduce are very close
conceptually, they both utilise HDFS under the hood, and run on the
JVM. This means that a large chunk of the existing code can be
reused, and migration to Spark can, therefore, be completed with
relatively small effort. Additionally, MapReduce and Spark jobs can
be run on AWS EMR using basically the same HW/SW configuration,
which facilitates straightforward performance comparisons.</p>
</li>
<li>
<p><strong>Incremental improvements:</strong> Spark exposes multiple APIs for
different workloads and operating on different levels of
abstraction. Datagen may initially utilise the lower-level,
Java-oriented RDDs (which offer the clearest 1 to 1 mapping when
coming from MapReduce) and gradually move towards DataFrames to
support Parquet output in the serializers and maybe unlock some SQL
optimization capabilities in the generators later down the road.</p>
</li>
<li>
<p><strong>OSS, commodity:</strong> Spark is one of the most widely used open-source
big data platforms. Every major public cloud provides a managed
offering for Spark. Together these mean that the migration increases
the approachability and portability of the code.</p>
</li>
</ul>
<h2 id="first-steps">First steps</h2>
<p>The first milestone is a successful run of LDBC Datagen on Spark while
making the minimum necessary amount of code alterations. This entails
the migration of the Hadoop wrappers around the generators and
serializers. The following bullet-points summarize the key notions that
cropped up during the process.</p>
<ul>
<li>
<p><strong>Use your memory:</strong> A strong focus was placed on keeping the call
sequence intact, so that the migrated code evaluates the same steps
in the same order, but with data passed as RDDs. It was hypothesised
that the required data could be either cached in memory entirely at
all times, or if not, regenerating them would still be faster than
involving the disk I/O loop (e.g by using MEMORY_AND_DISK). In
short, the default caching strategy was used everywhere.</p>
</li>
<li>
<p><strong>Regression tests:</strong> Lacking tests apart from an id uniqueness
check, meant there were no means to detect bugs introduced by the
migration. Designing and implementing a comprehensive test suite was
out of scope, so instead, regression testing was utilised, with the
MapReduce output as the baseline. The original output mostly
consists of Hadoop sequence files which can be read into Spark,
allowing comparisons to be drawn with the output from the RDD
produced by the migrated code.</p>
</li>
<li>
<p><strong>Thread-safety concerns:</strong> Soon after migrating the first generator
and running the regression tests, there were clear discrepancies in
the output. These only surfaced when the parallelization level was
set greater than 1. This indicated the presence of potential race
conditions. Thread-safety wasn&rsquo;t a concern in the original
implementation due to the fact that MapReduce doesn&rsquo;t use
thread-based parallelization for mappers and reducers.<a href="#fn3">^3^</a> In
Spark however, tasks are executed by parallel threads in the same
JVM application, so the code is required to be thread-safe. After
some debugging, a bug was discovered originating from the shared use
of java.text.SimpleDateFormat (notoriously known to be not
thread-safe) in the serializers. This was resolved simply by
changing to java.time.format.DateTimeFormatter. There were multiple
instances of some static field on an object being mutated
concurrently. In some cases this was a temporary buffer and was
easily resolved by making it an instance variable. In another case a
shared context variable was used, which was resolved by passing
dedicated instances as function arguments. Sadly, the Java language
has the same syntax for accessing locals, fields and statics,
<a href="#fn4">^4^</a> which makes it somewhat harder to find potential
unguarded shared variables.</p>
</li>
</ul>
<h2 id="case-study-person-ranking">Case study: Person ranking</h2>
<p>Migrating was rather straightforward, however, the so-called person
ranking step required some thought. The goal of this step is to organize
persons so that similar ones appear close to each other in a
deterministic order. This provides a scalable way to cluster persons
according to a similarity metric, as introduced in the <a href="#s3g2">S3G2
paper</a>.</p>
<h3 id="the-original-mapreduce-version">The original MapReduce version</h3>
<p><br>
<img src="http://ldbcouncil.org/sites/default/files/person_ranking.svg" alt=""></p>
<p><em>Figure 2. Diagram of the MapReduce code for ranking persons</em></p>
<p>The implementation, shown in pseudocode above, works as follows:</p>
<ol>
<li>The equivalence keys are mapped to each person and fed into
TotalOrderPartitioner which maintains an order sensitive
partitioning while trying to emit more or less equal sized groups to
keep the data skew low.</li>
<li>The reducer keys the partitions with its own task id and a counter
variable which has been initialized to zero and incremented on each
person, establishing a local ranking inside the group. The final
state of the counter (which is the total number of persons in that
group) is saved to a separate &ldquo;side-channel&rdquo; file upon the
completion of a reduce task.</li>
<li>In a consecutive reduce-only stage, the global order is established
by reading all of these previously emitted count files in the order
of their partition number in each reducer, then creating an ordered
map from each partition number to the corresponding cumulative count
of persons found in all preceding ones. This is done in the setup
phase. In the reduce function, the respective count is incremented
and assigned to each person.</li>
</ol>
<p>Once this ranking is done, the whole range is sliced up into equally
sized blocks, which are processed independently. For example, when
wiring relationships between persons, only those appearing in the same
block are considered.</p>
<h3 id="the-migrated-version">The migrated version</h3>
<p>Spark provides a sortBy function which takes care of the first step
above in a single line. The gist of the problem remains collecting the
partition sizes and making them available in a later step. While the
MapReduce version uses a side output, in Spark the partition sizes are
collected in a separate job and passed into the next phase using a
broadcast variable. The resulting code size is a fraction of the
original one.</p>
<h2 id="benchmarks">Benchmarks</h2>
<p>Benchmarks were carried out on AWS <a href="https://aws.amazon.com/emr/">EMR</a>,
originally utilising
<a href="https://aws.amazon.com/ec2/instance-types/i3/">i3.xlarge</a> instances
because of their fast NVMe SSD storage and ample amount of RAM.</p>
<p>The application parameter hadoop.numThreads controls the number of
reduce threads in each Hadoop job for the MapReduce version and the
number of partitions in the serialization jobs in the Spark one. For
MapReduce, this was set to n_nodes, i.e. the number of machines;
experimentation yield slowdowns for higher values. The Spark version on
the other hand, performed better with this parameter set to n_nodes *
v_cpu. The scale factor (SF) parameter determines the output size. It
is defined so that one SF unit generates around 1 GB of data. That is,
SF10 generates around 10 GB, SF30 around 30 GB, etc. It should be noted
however, that incidentally the output was only 60% of this in these
experiments, stemming from two reasons. One, update stream serialization
was not migrated to Spark, due to problems in the original
implementation. Of course, for the purpose of faithful comparison the
corresponding code was removed from the MapReduce version as well before
executing the benchmarks. This explains a 10% reduction from the
expected size. The rest can be attributed to incorrectly tuned
parameters.<a href="#fn5">^5^</a> The MapReduce results were as follows:</p>
<p>SF    workers   Platform    Instance Type   runtime (min)   runtime * worker/SF (min)</p>
<hr>
<p>10    1         MapReduce   i3.xlarge       16              1.60
30    1         MapReduce   i3.xlarge       34              1.13
100   3         MapReduce   i3.xlarge       40              1.20
300   9         MapReduce   i3.xlarge       44              1.32</p>
<p>It can be observed that the runtime per scale factor only increases
slowly, which is good. The metric charts show an underutilized, bursty
CPU. The bursts are supposedly interrupted by the disk I/O parts when
the node is writing the results of a completed job. It can also be seen
that the memory only starts to get consumed after 10 minutes of the run
have passed.</p>
<p><br>
<img src="http://ldbcouncil.org/sites/default/files/mr_sf100_cpu_load.png" alt=""></p>
<p><em>Figure 3. CPU Load for the Map Reduce cluster is bursty and less than
50% on average (SF100, 2nd graph shows master)</em></p>
<p><em><img src="http://ldbcouncil.org/sites/default/files/mr_sf100_mem_free.png" alt=""></em></p>
<p><em>Figure 4. The job only starts to consume memory when already 10 minutes
into the run (SF100, 2nd graph shows master)</em></p>
<p>Let&rsquo;s see how Spark fares.</p>
<p>SF     workers   Platform   Instance Type   runtime (min)   runtime * worker/SF (min)</p>
<hr>
<p>10     1         Spark      i3.xlarge       10              1.00
30     1         Spark      i3.xlarge       21              0.70
100    3         Spark      i3.xlarge       27              0.81
300    9         Spark      i3.xlarge       36              1.08
1000   30        Spark      i3.xlarge       47              1.41
3000   90        Spark      i3.xlarge       47              1.41</p>
<p>A similar trend here, however the run times are around 70% of the
MapReduce version. It can be seen that the larger scale factors (SF1000
and SF3000) yielded a long runtime than expected. On the metric charts
of SF100 the CPU shows full utilization, except at the end, when the
results are serialized in one go and the CPU is basically idle (the
snapshot of the diagram doesn&rsquo;t include this part unfortunately). Spark
can be seen to have used up all memory pretty fast even in case of
SF100. In case of SF1000 and SF3000, the nodes are running so low on
memory that most probably some of the RDDs have to be calculated
multiple times (no disk level serialization was used here), which seem
to be the most plausible explanation for the slowdowns experienced. In
fact, the OOM errors encountered when running SF3000 supports this
hypothesis even further. It was thus proposed to scale up the RAM in the
instances. The CPU utilization hints that adding some extra vCPUs as
well can further yield speedup.</p>
<p><br>
<img src="http://ldbcouncil.org/sites/default/files/spark_sf100_cpu_load.png" alt=""></p>
<p><em>Figure 5. Full CPU utilization for Spark (SF100, last graph shows
master)</em></p>
<p><br>
<img src="http://ldbcouncil.org/sites/default/files/spark_sf100_mem_free.png" alt=""></p>
<p><em>Figure 6. Spark eats up memory fast (SF100, 2nd graph shows master)</em></p>
<p>i3.2xlarge would have been the most straightforward option for scaling
up the instances, however the humongous 1.9 TB disk of this image is
completely unnecessary for the job. Instead the cheaper r5d.2xlarge
instance was utilised, largely identical to i3.2xlarge, except it <em>only</em>
has a 300 GB SSD.</p>
<p>SF      workers   Platform   Instance Type   runtime (min)   runtime * worker/SF (min)</p>
<hr>
<p>100     3         Spark      r5d.2xlarge     16              0.48
300     9         Spark      r5d.2xlarge     21              0.63
1000    30        Spark      r5d.2xlarge     26              0.78
3000    90        Spark      r5d.2xlarge     25              0.75
10000   303       Spark      r5d.2xlarge     25              0.75</p>
<p>The last column clearly demonstrates our ability to keep the cost per
scale factor unit constant.</p>
<h2 id="next-steps">Next steps</h2>
<p>The next improvement is refactoring the serializers so they use Spark&rsquo;s
high-level writer facilities. The most compelling benefit is that it
will make the jobs fault-tolerant, as Spark maintains the integrity of
the output files in case the task that writes it fails. This makes
Datagen more resilient and opens up the possibility to run on less
reliable hardware configuration (e.g. EC2 spot nodes on AWS) for
additional cost savings. They will supposedly also yield some speedup on
the same cluster configuration.</p>
<p>As already mentioned, the migration of the update stream serialization
was ignored due to problems with the original code. Ideally, they should
be implemented with the new serializers.</p>
<p>The Spark migration also serves as an important building block for the
next generation of LDBC benchmarks. As part of extending the SNB
benchmark suite, the SNB task force has recently extended Datagen with
support for <a href="#deletes">generating delete operations</a>. The next step for
the task force is to fine-tune the temporal distributions of these
deletion operations to ensure that the emerging sequence of events is
realistic, i.e. the emerging distribution resembles what a database
system would experience when serving a real social network.</p>
<h2 id="acknowledgements">Acknowledgements</h2>
<p>This work is based upon the work of Arnau Prat, Gábor Szárnyas, Ben
Steer, Jack Waudby and other LDBC contributors. Thanks for your help and
feedback!</p>
<h2 id="references">References</h2>
<ul>
<li>[]{#deletes}<a href="http://ldbcouncil.org/sites/default/files/datagen-deletions-grades-nda-2020.pdf">Supporting Dynamic Graphs and Temporal Entity Deletions
in the LDBC Social Network Benchmark&rsquo;s Data
Generator</a></li>
<li>[]{#datagen}<a href="https://www.youtube.com/watch?v=ZQOLuCOOpSI">9th TUC Meeting &ndash; LDBC SNB Datagen Update &ndash; Arnau
Prat (UPC)</a> -
<a href="http://wiki.ldbcouncil.org/pages/viewpage.action?pageId=59277315&amp;preview=/59277315/75431942/datagen_in_depth.pdf">slides</a></li>
<li>[]{#s3g2}<a href="https://research.vu.nl/en/publications/s3g2-a-scalable-structure-correlated-social-graph-generator">S3G2: a Scalable Structure-correlated Social Graph
Generator</a></li>
<li>[]{#snb}<a href="https://arxiv.org/abs/2001.02299">The LDBC Social Network
Benchmark</a></li>
<li><a href="http://www.ldbcouncil.org/">LDBC</a> - <a href="https://github.com/ldbc">LDBC GitHub
organization</a></li>
</ul>
<p>[^1^]{#fn1} Also makes it easier to map to a tabular format thus it is a
SQL friendly representation.</p>
<p>[^2^]{#fn2} It&rsquo;s hard to imagine this done declaratively in SQL.</p>
<p>[^3^]{#fn3} Instead, multiple YARN containers have to be used if you
want to parallelize on the same machine.</p>
<p>[^4^]{#fn4} Although editors usually render these using different font
styles.</p>
<p>[^5^]{#fn5} With the addition of deletes, entities often get inserted
and deleted during the simulation (which is normal in a social network).
During serialization, we check for such entities and omit them. However
we forgot to calculate this when determining the output size, which we
will amend when tuning the distributions.</p>

        </div>
        <!-- tags -->
        <div class="mb-3">
          <h5 class="d-inline-block mr-3">Tags:</h5>
          <ul class="list-inline d-inline-block">
            LDBC DATAGEN
            , SNB
            
          </ul>
        </div>
        
        
      </div>
      <!-- sidebar -->
<aside class="col-lg-4 order-1 order-lg-2">
  <!-- latest post -->
  <div class="bg-white px-4 py-5 box-shadow mb-5">
    <h4 class="mb-4">Latest updates</h4>
    <!-- post-item -->
    
    <div class="media border-bottom border-color pb-3 mb-3">
      <div class="media-body">
        <a href="https://ldbc.github.io/post/speeding-up-ldbc-snb-datagen/">
          <h5 class="mt-0">Speeding Up LDBC SNB Datagen</h5>
        </a>
        24 Jun 2021
      </div>
      
      
    </div>
    
    <div class="media border-bottom border-color pb-3 mb-3">
      <div class="media-body">
        <a href="https://ldbc.github.io/event/12th-tuc-meeting-at-sigmod-2019-amsterdam/">
          <h5 class="mt-0">12th TUC Meeting at SIGMOD 2019 Amsterdam</h5>
        </a>
        24 Jun 2021
      </div>
      
      
    </div>
    
  </div>
  <!-- tags -->
  <div class="bg-white px-4 py-5 box-shadow mb-5">
    <h4 class="mb-4"></h4>
    <ul class="list-inline tag-list">
      <li class="list-inline-item"><a class="hover-ripple" href="/tags/ldbc-datagen">Ldbc datagen</a></li>
      <li class="list-inline-item"><a class="hover-ripple" href="/tags/snb">Snb</a></li>
      <li class="list-inline-item"><a class="hover-ripple" href="/tags/tuc-meeting">Tuc meeting</a></li>
    </ul>
  </div>
  
  
</aside>
<!-- /sidebar -->
    </div>
  </div>
</section>
<!-- /blog details -->


<footer>
  
  <div class="section bg-secondary">
    <div class="container">
      <div class="row justify-content-between">
        
        <div class="col-lg-5 mb-5 mb-lg-0">
          
          <a class="mb-4 d-inline-block" href="/"><img class="img-fluid"
              src="https://ldbc.github.io/images/ldbc.png" alt="Linked Data Benchmark Council"></a>
          <p class="text-light mb-5">LDBC came out of an EU FP7 project and is now a non-profit organization sustained by its members. Contact us at info AT ldbcouncil DOT org.</p>
          <h4 class="text-white mb-4"></h4>
          
          <ul class="list-inline social-icon-alt">
            
            <li class="list-inline-item">
              <a class="hover-ripple" href="https://www.facebook.com/ldbcouncil"><i class="fa fa-facebook"></i></a>
            </li>
            
            <li class="list-inline-item">
              <a class="hover-ripple" href="https://twitter.com/LDBCouncil"><i class="fa fa-twitter"></i></a>
            </li>
            
            <li class="list-inline-item">
              <a class="hover-ripple" href="https://github.com/ldbc"><i class="fa fa-github"></i></a>
            </li>
            
          </ul>
        </div>
        <div class="col-lg-6">
          <div class="row">
            
            
            
            
            <div class="col-12">

  <!-- Twitter timeline -->
  
  
  <a class="twitter-timeline" data-height="450" data-theme="dark"  href="https://twitter.com/LDBCouncil?ref_src=twsrc%5Etfw" style="background-color: #252d39;">Tweets by LDBCouncil</a> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script> 
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
  
  <div class="bg-secondary-darken py-4">
    <div class="container">
      <div class="row">
        <div class="col-md-6 text-center text-md-left mb-3 mb-md-0">
          <p class="mb-0 text-white">© Copyright LDBC 2021</p>
        </div>
        <div class="col-md-6 text-center text-md-right">
          <ul class="list-inline">
            
            <li class="list-inline-item mx-0"><a class="d-inline-block px-3 text-white" href="/docs/LDBC.Byelaws.1.3.ADOPTED.2021-01-14.pdf" class="text-white">LDBC Byelaws Document</a></li>
            
          </ul>
        </div>
      </div>
    </div>
  </div>
</footer>




<!-- Google Map API -->




<!-- JS Plugins -->

<script src="https://ldbc.github.io/plugins/jQuery/jquery.min.js"></script>

<script src="https://ldbc.github.io/plugins/bootstrap/bootstrap.min.js"></script>

<script src="https://ldbc.github.io/plugins/slick/slick.min.js"></script>

<script src="https://ldbc.github.io/plugins/google-map/gmap.js"></script>

<script src="https://ldbc.github.io/plugins/venobox/venobox.min.js"></script>

<script src="https://ldbc.github.io/plugins/filterizr/jquery.filterizr.min.js"></script>

<script src="https://ldbc.github.io/plugins/search/fuse.min.js"></script>

<script src="https://ldbc.github.io/plugins/search/mark.js"></script>

<script src="https://ldbc.github.io/plugins/search/search.js"></script>


<!-- Main Script -->

<script src="https://ldbc.github.io/js/script.min.js"></script>

<!-- google analitycs -->


<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title"></h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 btn-sm js-copy-cite" href="#" target="_blank">
          
          COPY
        </a>
        <a class="btn btn-outline-primary my-1 btn-sm js-download-cite" href="#" target="_blank">
          
          DOWNLOAD
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>


</body>

</html>