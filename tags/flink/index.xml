<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Flink on Linked Data Benchmark Council</title>
    <link>https://ldbc.github.io/tags/flink/</link>
    <description>Recent content in Flink on Linked Data Benchmark Council</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; Copyright LDBC 2021</copyright>
    <lastBuildDate>Mon, 16 Nov 2015 14:47:00 +0000</lastBuildDate><atom:link href="https://ldbc.github.io/tags/flink/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Speeding Up LDBC SNB Datagen</title>
      <link>https://ldbc.github.io/post/speeding-up-ldbc-snb-datagen/</link>
      <pubDate>Fri, 12 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://ldbc.github.io/post/speeding-up-ldbc-snb-datagen/</guid>
      <description>&lt;p&gt;LDBC&amp;rsquo;s &lt;a href=&#34;#references&#34;&gt;Social Network Benchmark [4]&lt;/a&gt; (LDBC SNB) is an industrial and
academic initiative, formed by principal actors in the field of
graph-like data management. Its goal is to define a framework where
different graph-based technologies can be fairly tested and compared,
that can drive the identification of systems&#39; bottlenecks and required
functionalities, and can help researchers open new frontiers in
high-performance graph data management.&lt;/p&gt;
&lt;p&gt;LDBC SNB provides &lt;a href=&#34;https://github.com/ldbc/ldbc_snb_datagen&#34;&gt;Datagen&lt;/a&gt;
(Data Generator), which produces synthetic datasets, mimicking a social
network&amp;rsquo;s activity during a period of time. Datagen is defined by the
charasteristics of realism, scalability, determinism and usability. To
address scalability in particular, Datagen has been implemented on the
MapReduce computation model to enable scaling out across a distributed
cluster. However, since its inception in the early 2010s there has been
a tremendous amount of development in the big data landscape, both in
the sophistication of distributed processing platforms, as well as
public cloud IaaS offerings. In the light of this, we should reevaluate
this implementation, and in particular, investigate if Apache Spark
would be a more cost-effective solution for generating datasets on the
scale of tens of terabytes, on public clouds such as Amazon Web Services
(AWS).&lt;/p&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;The benchmark&amp;rsquo;s specification describes a social network &lt;a href=&#34;https://github.com/ldbc/ldbc_snb_docs/blob/dev/figures/schema.pdf&#34;&gt;data
model&lt;/a&gt;
which divides its components into two broad categories: static and
dynamic. The dynamic element consists of an evolving network where
people make friends, post in forums, comment or like each others posts,
etc. In contrast, the static component contains related attributes such
as countries, universities and organizations and are fixed values. For
the detailed specifications of the benchmark and the Datagen component,
see &lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Datasets are generated in a multi-stage process captured as a sequence
of MapReduce steps (shown in the diagram below).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;datagen_flow.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;
&lt;em&gt;Figure 1. LDBC SNB Datagen Process on Hadoop&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In the initialization phase dictionaries are populated and distributions
are initialized. In the first generation phase persons are synthesized,
then relationships are wired between them along 3 dimensions
(university, interest and random). After merging the graph of person
relationships, the resulting dataset is output. Following this,
activities such as forum posts, comments, likes and photos are generated
and output. Finally, the static components are output.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note: The diagram shows the call sequence as implemented. All steps are
sequential &amp;ndash; including the relationship generation &amp;ndash;, even in cases
when the data dependencies would allow for parallelization.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Entities are generated by procedural Java code and are represented as
POJOs in memory and as sequence files on disk. Most entities follow a
shallow representation, i.e foreign keys (in relational terms) are
mapped to integer ids, which makes serialization
straightforward.&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; A notable exception is the Knows edge which
contains only the target vertex, and is used as a navigation property on
the source Person. The target Person is replaced with only the foreign
key augmented with some additional information in order to keep the
structure free of cycles. Needless to say, this &lt;em&gt;edge as property&lt;/em&gt;
representation makes the data harder to handle in SQL than it would be
with a flat join table.&lt;/p&gt;
&lt;p&gt;Entity generation amounts to roughly one fifth of the main codebase. It
generates properties drawn from several random distributions using
mutable pRNGs. Determinism is achieved by initializing the pRNGs to
seeds that are fully defined by the configuration with constants, and
otherwise having no external state in the logic.&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Serialization is done by hand-written serializers for the supported
output formats (e.g. CSV) and comprises just a bit less than one third
of the main codebase. Most of the output is created by directly
interacting with low-level HDFS file streams. Ideally, this code should
be migrated to higher-level writers that handle faults and give
consistent results when the task has to be restarted.&lt;/p&gt;
&lt;h2 id=&#34;motivations-for-the-migration&#34;&gt;Motivations for the migration&lt;/h2&gt;
&lt;p&gt;The application is written using Hadoop MapReduce, which is now largely
superseded by more modern distributed batch processing platforms,
notably Apache Spark. For this reason, it was proposed to migrate
Datagen to Spark. The migration provides the following benefits:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Better memory utilization:&lt;/strong&gt; MapReduce is disk-oriented, i.e. it
writes the output to disk after each reduce stage which is then read
by the next MapReduce job. As public clouds provide virtual machines
with sufficient RAM to encapsulate any generated dataset, time and
money are wasted by the overhead this unnecessary disk I/O incurs.
Instead, the intermediate results should be cached in memory where
possible. The lack of support for this is a well-known limitation of
MapReduce.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Smaller codebase:&lt;/strong&gt; The Hadoop MapReduce library is fairly
ceremonial and boilerplatey. Spark provides a higher-level
abstraction that is simpler to work with, while still providing
enough control on the lower-level details required for this
workload.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Small entry cost:&lt;/strong&gt; Spark and MapReduce are very close
conceptually, they both utilise HDFS under the hood, and run on the
JVM. This means that a large chunk of the existing code can be
reused, and migration to Spark can, therefore, be completed with
relatively small effort. Additionally, MapReduce and Spark jobs can
be run on AWS EMR using basically the same HW/SW configuration,
which facilitates straightforward performance comparisons.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Incremental improvements:&lt;/strong&gt; Spark exposes multiple APIs for
different workloads and operating on different levels of
abstraction. Datagen may initially utilise the lower-level,
Java-oriented RDDs (which offer the clearest 1 to 1 mapping when
coming from MapReduce) and gradually move towards DataFrames to
support Parquet output in the serializers and maybe unlock some SQL
optimization capabilities in the generators later down the road.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;OSS, commodity:&lt;/strong&gt; Spark is one of the most widely used open-source
big data platforms. Every major public cloud provides a managed
offering for Spark. Together these mean that the migration increases
the approachability and portability of the code.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;first-steps&#34;&gt;First steps&lt;/h2&gt;
&lt;p&gt;The first milestone is a successful run of LDBC Datagen on Spark while
making the minimum necessary amount of code alterations. This entails
the migration of the Hadoop wrappers around the generators and
serializers. The following bullet-points summarize the key notions that
cropped up during the process.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Use your memory:&lt;/strong&gt; A strong focus was placed on keeping the call
sequence intact, so that the migrated code evaluates the same steps
in the same order, but with data passed as RDDs. It was hypothesised
that the required data could be either cached in memory entirely at
all times, or if not, regenerating them would still be faster than
involving the disk I/O loop (e.g by using MEMORY_AND_DISK). In
short, the default caching strategy was used everywhere.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Regression tests:&lt;/strong&gt; Lacking tests apart from an id uniqueness
check, meant there were no means to detect bugs introduced by the
migration. Designing and implementing a comprehensive test suite was
out of scope, so instead, regression testing was utilised, with the
MapReduce output as the baseline. The original output mostly
consists of Hadoop sequence files which can be read into Spark,
allowing comparisons to be drawn with the output from the RDD
produced by the migrated code.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Thread-safety concerns:&lt;/strong&gt; Soon after migrating the first generator
and running the regression tests, there were clear discrepancies in
the output. These only surfaced when the parallelization level was
set greater than 1. This indicated the presence of potential race
conditions. Thread-safety wasn&amp;rsquo;t a concern in the original
implementation due to the fact that MapReduce doesn&amp;rsquo;t use
thread-based parallelization for mappers and reducers.&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt; In
Spark however, tasks are executed by parallel threads in the same
JVM application, so the code is required to be thread-safe. After
some debugging, a bug was discovered originating from the shared use
of java.text.SimpleDateFormat (notoriously known to be not
thread-safe) in the serializers. This was resolved simply by
changing to java.time.format.DateTimeFormatter. There were multiple
instances of some static field on an object being mutated
concurrently. In some cases this was a temporary buffer and was
easily resolved by making it an instance variable. In another case a
shared context variable was used, which was resolved by passing
dedicated instances as function arguments. Sadly, the Java language
has the same syntax for accessing locals, fields and statics,
&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt; which makes it somewhat harder to find potential
unguarded shared variables.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;case-study-person-ranking&#34;&gt;Case study: Person ranking&lt;/h2&gt;
&lt;p&gt;Migrating was rather straightforward, however, the so-called person
ranking step required some thought. The goal of this step is to organize
persons so that similar ones appear close to each other in a
deterministic order. This provides a scalable way to cluster persons
according to a similarity metric, as introduced in the &lt;a href=&#34;#references&#34;&gt;S3G2
paper [3]&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;the-original-mapreduce-version&#34;&gt;The original MapReduce version&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;person_ranking.svg&#34; alt=&#34;&#34;&gt;&lt;br&gt;
&lt;em&gt;Figure 2. Diagram of the MapReduce code for ranking persons&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The implementation, shown in pseudocode above, works as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The equivalence keys are mapped to each person and fed into
TotalOrderPartitioner which maintains an order sensitive
partitioning while trying to emit more or less equal sized groups to
keep the data skew low.&lt;/li&gt;
&lt;li&gt;The reducer keys the partitions with its own task id and a counter
variable which has been initialized to zero and incremented on each
person, establishing a local ranking inside the group. The final
state of the counter (which is the total number of persons in that
group) is saved to a separate &amp;ldquo;side-channel&amp;rdquo; file upon the
completion of a reduce task.&lt;/li&gt;
&lt;li&gt;In a consecutive reduce-only stage, the global order is established
by reading all of these previously emitted count files in the order
of their partition number in each reducer, then creating an ordered
map from each partition number to the corresponding cumulative count
of persons found in all preceding ones. This is done in the setup
phase. In the reduce function, the respective count is incremented
and assigned to each person.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Once this ranking is done, the whole range is sliced up into equally
sized blocks, which are processed independently. For example, when
wiring relationships between persons, only those appearing in the same
block are considered.&lt;/p&gt;
&lt;h3 id=&#34;the-migrated-version&#34;&gt;The migrated version&lt;/h3&gt;
&lt;p&gt;Spark provides a sortBy function which takes care of the first step
above in a single line. The gist of the problem remains collecting the
partition sizes and making them available in a later step. While the
MapReduce version uses a side output, in Spark the partition sizes are
collected in a separate job and passed into the next phase using a
broadcast variable. The resulting code size is a fraction of the
original one.&lt;/p&gt;
&lt;h2 id=&#34;benchmarks&#34;&gt;Benchmarks&lt;/h2&gt;
&lt;p&gt;Benchmarks were carried out on AWS &lt;a href=&#34;https://aws.amazon.com/emr/&#34;&gt;EMR&lt;/a&gt;,
originally utilising
&lt;a href=&#34;https://aws.amazon.com/ec2/instance-types/i3/&#34;&gt;i3.xlarge&lt;/a&gt; instances
because of their fast NVMe SSD storage and ample amount of RAM.&lt;/p&gt;
&lt;p&gt;The application parameter hadoop.numThreads controls the number of
reduce threads in each Hadoop job for the MapReduce version and the
number of partitions in the serialization jobs in the Spark one. For
MapReduce, this was set to n_nodes, i.e. the number of machines;
experimentation yield slowdowns for higher values. The Spark version on
the other hand, performed better with this parameter set to n_nodes *
v_cpu. The scale factor (SF) parameter determines the output size. It
is defined so that one SF unit generates around 1 GB of data. That is,
SF10 generates around 10 GB, SF30 around 30 GB, etc. It should be noted
however, that incidentally the output was only 60% of this in these
experiments, stemming from two reasons. One, update stream serialization
was not migrated to Spark, due to problems in the original
implementation. Of course, for the purpose of faithful comparison the
corresponding code was removed from the MapReduce version as well before
executing the benchmarks. This explains a 10% reduction from the
expected size. The rest can be attributed to incorrectly tuned
parameters.&lt;sup id=&#34;fnref:5&#34;&gt;&lt;a href=&#34;#fn:5&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;5&lt;/a&gt;&lt;/sup&gt; The MapReduce results were as follows:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;SF&lt;/th&gt;
&lt;th&gt;workers&lt;/th&gt;
&lt;th&gt;Platform&lt;/th&gt;
&lt;th&gt;Instance Type&lt;/th&gt;
&lt;th&gt;runtime (min)&lt;/th&gt;
&lt;th&gt;runtime * worker/SF (min)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;MapReduce&lt;/td&gt;
&lt;td&gt;i3.xlarge&lt;/td&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;td&gt;1.60&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;30&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;MapReduce&lt;/td&gt;
&lt;td&gt;i3.xlarge&lt;/td&gt;
&lt;td&gt;34&lt;/td&gt;
&lt;td&gt;1.13&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;100&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;MapReduce&lt;/td&gt;
&lt;td&gt;i3.xlarge&lt;/td&gt;
&lt;td&gt;40&lt;/td&gt;
&lt;td&gt;1.20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;300&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;MapReduce&lt;/td&gt;
&lt;td&gt;i3.xlarge&lt;/td&gt;
&lt;td&gt;44&lt;/td&gt;
&lt;td&gt;1.32&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It can be observed that the runtime per scale factor only increases
slowly, which is good. The metric charts show an underutilized, bursty
CPU. The bursts are supposedly interrupted by the disk I/O parts when
the node is writing the results of a completed job. It can also be seen
that the memory only starts to get consumed after 10 minutes of the run
have passed.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;mr_sf100_cpu_load.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;
&lt;em&gt;Figure 3. CPU Load for the Map Reduce cluster is bursty and less than
50% on average (SF100, 2nd graph shows master)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;mr_sf100_mem_free.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;
&lt;em&gt;Figure 4. The job only starts to consume memory when already 10 minutes
into the run (SF100, 2nd graph shows master)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s see how Spark fares.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;SF&lt;/th&gt;
&lt;th&gt;workers&lt;/th&gt;
&lt;th&gt;Platform&lt;/th&gt;
&lt;th&gt;Instance Type&lt;/th&gt;
&lt;th&gt;runtime (min)&lt;/th&gt;
&lt;th&gt;runtime * worker/SF (min)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;Spark&lt;/td&gt;
&lt;td&gt;i3.xlarge&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;1.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;30&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;Spark&lt;/td&gt;
&lt;td&gt;i3.xlarge&lt;/td&gt;
&lt;td&gt;21&lt;/td&gt;
&lt;td&gt;0.70&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;100&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;Spark&lt;/td&gt;
&lt;td&gt;i3.xlarge&lt;/td&gt;
&lt;td&gt;27&lt;/td&gt;
&lt;td&gt;0.81&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;300&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;Spark&lt;/td&gt;
&lt;td&gt;i3.xlarge&lt;/td&gt;
&lt;td&gt;36&lt;/td&gt;
&lt;td&gt;1.08&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1000&lt;/td&gt;
&lt;td&gt;30&lt;/td&gt;
&lt;td&gt;Spark&lt;/td&gt;
&lt;td&gt;i3.xlarge&lt;/td&gt;
&lt;td&gt;47&lt;/td&gt;
&lt;td&gt;1.41&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3000&lt;/td&gt;
&lt;td&gt;90&lt;/td&gt;
&lt;td&gt;Spark&lt;/td&gt;
&lt;td&gt;i3.xlarge&lt;/td&gt;
&lt;td&gt;47&lt;/td&gt;
&lt;td&gt;1.41&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;A similar trend here, however the run times are around 70% of the
MapReduce version. It can be seen that the larger scale factors (SF1000
and SF3000) yielded a long runtime than expected. On the metric charts
of SF100 the CPU shows full utilization, except at the end, when the
results are serialized in one go and the CPU is basically idle (the
snapshot of the diagram doesn&amp;rsquo;t include this part unfortunately). Spark
can be seen to have used up all memory pretty fast even in case of
SF100. In case of SF1000 and SF3000, the nodes are running so low on
memory that most probably some of the RDDs have to be calculated
multiple times (no disk level serialization was used here), which seem
to be the most plausible explanation for the slowdowns experienced. In
fact, the OOM errors encountered when running SF3000 supports this
hypothesis even further. It was thus proposed to scale up the RAM in the
instances. The CPU utilization hints that adding some extra vCPUs as
well can further yield speedup.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;spark_sf100_cpu_load.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;
&lt;em&gt;Figure 5. Full CPU utilization for Spark (SF100, last graph shows
master)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;spark_sf100_mem_free.png&#34; alt=&#34;&#34;&gt;&lt;br&gt;
&lt;em&gt;Figure 6. Spark eats up memory fast (SF100, 2nd graph shows master)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;i3.2xlarge would have been the most straightforward option for scaling
up the instances, however the humongous 1.9 TB disk of this image is
completely unnecessary for the job. Instead the cheaper r5d.2xlarge
instance was utilised, largely identical to i3.2xlarge, except it &lt;em&gt;only&lt;/em&gt;
has a 300 GB SSD.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;SF&lt;/th&gt;
&lt;th&gt;workers&lt;/th&gt;
&lt;th&gt;Platform&lt;/th&gt;
&lt;th&gt;Instance Type&lt;/th&gt;
&lt;th&gt;runtime (min)&lt;/th&gt;
&lt;th&gt;runtime * worker/SF (min)&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;100&lt;/td&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;Spark&lt;/td&gt;
&lt;td&gt;r5d.2xlarge&lt;/td&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;td&gt;0.48&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;300&lt;/td&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;Spark&lt;/td&gt;
&lt;td&gt;r5d.2xlarge&lt;/td&gt;
&lt;td&gt;21&lt;/td&gt;
&lt;td&gt;0.63&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1000&lt;/td&gt;
&lt;td&gt;30&lt;/td&gt;
&lt;td&gt;Spark&lt;/td&gt;
&lt;td&gt;r5d.2xlarge&lt;/td&gt;
&lt;td&gt;26&lt;/td&gt;
&lt;td&gt;0.78&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3000&lt;/td&gt;
&lt;td&gt;90&lt;/td&gt;
&lt;td&gt;Spark&lt;/td&gt;
&lt;td&gt;r5d.2xlarge&lt;/td&gt;
&lt;td&gt;25&lt;/td&gt;
&lt;td&gt;0.75&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10000&lt;/td&gt;
&lt;td&gt;303&lt;/td&gt;
&lt;td&gt;Spark&lt;/td&gt;
&lt;td&gt;r5d.2xlarge&lt;/td&gt;
&lt;td&gt;25&lt;/td&gt;
&lt;td&gt;0.75&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The last column clearly demonstrates our ability to keep the cost per
scale factor unit constant.&lt;/p&gt;
&lt;h2 id=&#34;next-steps&#34;&gt;Next steps&lt;/h2&gt;
&lt;p&gt;The next improvement is refactoring the serializers so they use Spark&amp;rsquo;s
high-level writer facilities. The most compelling benefit is that it
will make the jobs fault-tolerant, as Spark maintains the integrity of
the output files in case the task that writes it fails. This makes
Datagen more resilient and opens up the possibility to run on less
reliable hardware configuration (e.g. EC2 spot nodes on AWS) for
additional cost savings. They will supposedly also yield some speedup on
the same cluster configuration.&lt;/p&gt;
&lt;p&gt;As already mentioned, the migration of the update stream serialization
was ignored due to problems with the original code. Ideally, they should
be implemented with the new serializers.&lt;/p&gt;
&lt;p&gt;The Spark migration also serves as an important building block for the
next generation of LDBC benchmarks. As part of extending the SNB
benchmark suite, the SNB task force has recently extended Datagen with
support for &lt;a href=&#34;#references&#34;&gt;generating delete operations [1]&lt;/a&gt;. The next step for
the task force is to fine-tune the temporal distributions of these
deletion operations to ensure that the emerging sequence of events is
realistic, i.e. the emerging distribution resembles what a database
system would experience when serving a real social network.&lt;/p&gt;
&lt;h2 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;This work is based upon the work of Arnau Prat, Gábor Szárnyas, Ben
Steer, Jack Waudby and other LDBC contributors. Thanks for your help and
feedback!&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] &lt;a href=&#34;http://ldbcouncil.org/sites/default/files/datagen-deletions-grades-nda-2020.pdf&#34;&gt;Supporting Dynamic Graphs and Temporal Entity Deletions
in the LDBC Social Network Benchmark&amp;rsquo;s Data
Generator&lt;/a&gt;&lt;br&gt;
[2] &lt;a href=&#34;https://www.youtube.com/watch?v=ZQOLuCOOpSI&#34;&gt;9th TUC Meeting &amp;ndash; LDBC SNB Datagen Update &amp;ndash; Arnau
Prat (UPC)&lt;/a&gt; -
&lt;a href=&#34;http://wiki.ldbcouncil.org/pages/viewpage.action?pageId=59277315&amp;amp;preview=/59277315/75431942/datagen_in_depth.pdf&#34;&gt;slides&lt;/a&gt;&lt;br&gt;
[3] &lt;a href=&#34;https://research.vu.nl/en/publications/s3g2-a-scalable-structure-correlated-social-graph-generator&#34;&gt;S3G2: a Scalable Structure-correlated Social Graph
Generator&lt;/a&gt;&lt;br&gt;
[4] &lt;a href=&#34;https://arxiv.org/abs/2001.02299&#34;&gt;The LDBC Social Network
Benchmark&lt;/a&gt;&lt;br&gt;
[5] &lt;a href=&#34;http://www.ldbcouncil.org/&#34;&gt;LDBC&lt;/a&gt; - &lt;a href=&#34;https://github.com/ldbc&#34;&gt;LDBC GitHub
organization&lt;/a&gt;&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Also makes it easier to map to a tabular format thus it is a
SQL friendly representation. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;It&amp;rsquo;s hard to imagine this done declaratively in SQL. &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Instead, multiple YARN containers have to be used if you
want to parallelize on the same machine. &lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Although editors usually render these using different font
styles. &lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:5&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;With the addition of deletes, entities often get inserted
and deleted during the simulation (which is normal in a social network).
During serialization, we check for such entities and omit them. However
we forgot to calculate this when determining the output size, which we
will amend when tuning the distributions. &lt;a href=&#34;#fnref:5&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Twelfth TUC Meeting at SIGMOD 2019 Amsterdam</title>
      <link>https://ldbc.github.io/event/twelfth-tuc-meeting-at-sigmod-2019-amsterdam/</link>
      <pubDate>Fri, 05 Jul 2019 08:30:00 +0100</pubDate>
      
      <guid>https://ldbc.github.io/event/twelfth-tuc-meeting-at-sigmod-2019-amsterdam/</guid>
      <description>&lt;p&gt;Read more information and how to
register &lt;a href=&#34;http://wiki.ldbcouncil.org/pages/viewpage.action?pageId=106233859&#34;&gt;HERE&lt;/a&gt; [CLOSED]&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CALL FOR CONTRIBUTIONS:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;[CLOSED] The LDBC meeting will feature updates on activities of LDBC,
but is also seeking contributions by community members and graph
practitioners and researchers. These proposals can be emailed to Peter
Boncz (&lt;a href=&#34;mailto:boncz@cwi.nl&#34;&gt;boncz@cwi.nl&lt;/a&gt;)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;FINAL PROGRAM (UPDATED):&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;08:30-10:30 LDBC Board Meeting (non-public)&lt;/li&gt;
&lt;li&gt;10:30-11:00 Coffee&lt;/li&gt;
&lt;li&gt;11:00-12:45 Session 1: Graph Benchmarks
&lt;ul&gt;
&lt;li&gt;11:00-11:05 Welcome &amp;amp; introduction&lt;/li&gt;
&lt;li&gt;11:05-11:45 Gabor Szarnyas (BME), Benjamin Steer (QMUL), Jack
Waudby (Newcastle University): Business Intelligence workload:
Progress report and roadmap&lt;/li&gt;
&lt;li&gt;11:45-12:00 Frank McSherry (Materialize): Experiences
implementing LDBC queries in a dataflow system&lt;/li&gt;
&lt;li&gt;12:00-12:25 Vasileios Trigonakis (Oracle): Evaluating a new
distributed graph query engine with LDBC: Experiences and
limitations&lt;/li&gt;
&lt;li&gt;12:25-12:45 Ahmed Musaafir (VU Amsterdam): LDBC
Graphalytics&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;12:45-14:00 Lunch&lt;/li&gt;
&lt;li&gt;14:00-16:05 Session 2: Graph Query Languages
&lt;ul&gt;
&lt;li&gt;14:00-14:25 Juan Sequeda (Capsenta): Property Graph Schema
Working Group: A progress report&lt;/li&gt;
&lt;li&gt;4:25-14:50 Stefan Plantikow (Neo4j): GQL: Scope and
features&lt;/li&gt;
&lt;li&gt;14:50-15:15 Vasileios Trigonakis (Oracle): Property graph
extensions for the SQL standard&lt;/li&gt;
&lt;li&gt;15:15-15:40 Alin Deutsch (TigerGraph): Modern graph analytics
support in GSQL, TigerGraph&#39;s query language&lt;/li&gt;
&lt;li&gt;15:40-16:05 Jan Posiadała (Nodes and Edges, Poland): Executable
semantics of graph query language.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;16:05-16:30 Coffee&lt;/li&gt;
&lt;li&gt;16:30-17:50 Session 3: Graph System Performance
&lt;ul&gt;
&lt;li&gt;16:30-16:50 Per Fuchs (CWI): Fast, scalable WCOJ
graph-pattern matching on in-memory graphs in Spark&lt;/li&gt;
&lt;li&gt;16:50-17:10 Semih Salihoglu (University of Waterloo): Optimizing
subgraph queries with a mix of tradition and modernity&lt;/li&gt;
&lt;li&gt;17:10-17:30 Roi Lipman (RedisGraph): Evaluating Cypher queries
and procedures as algebraic operations within RedisGraph&lt;/li&gt;
&lt;li&gt;17:30-17:50 Alexandru Uta (VU Amsterdam): Low-latency Spark
queries on updatable data&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>LDBC Is Proud to Announce the New LDBC Graphalytics Benchmark Draft Specification</title>
      <link>https://ldbc.github.io/post/ldbc-is-proud-to-announce-the-new-ldbc-graphalytics-benchmark-draft-specification/</link>
      <pubDate>Tue, 06 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>https://ldbc.github.io/post/ldbc-is-proud-to-announce-the-new-ldbc-graphalytics-benchmark-draft-specification/</guid>
      <description>&lt;p&gt;LDBC is proud to announce the new LDBC Graphalytics Benchmark draft
specification.&lt;/p&gt;
&lt;p&gt;LDBC Graphalytics is the first industry-grade graph data management
benchmark for graph analysis platforms such as Giraph. It consists of
six core algorithms, standard datasets, synthetic dataset generators,
and reference outputs, enabling the objective comparison of graph
analysis platforms.  It has strong industry support from Oracle, Intel,
Huawei and IBM, and was tested and optimized on the best industrial and
open-source systems.&lt;/p&gt;
&lt;p&gt;Tim Hegeman of &lt;a href=&#34;http://www.tudelft.nl&#34;&gt;TU Delft&lt;/a&gt; is today presenting the
technical paper describing LDBC Graphalytics at the important
&lt;a href=&#34;http://www.vldb.org/conference.html&#34;&gt;VLDB&lt;/a&gt; (Very Large DataBases)
conference in New Delhi, where his talk also marks the release by LDBC
of Graphalytics as a benchmark draft. Practitioners are invited to read
the PVLDB paper, download the software and try running it.&lt;/p&gt;
&lt;p&gt;LDBC is eager to use any feedback for its future adoption of LDBC
Graphalytics.&lt;/p&gt;
&lt;p&gt;Learn more: &lt;a href=&#34;http://ldbcouncil.org/ldbc-graphalytics&#34;&gt;http://ldbcouncil.org/ldbc-graphalytics&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;GitHub: &lt;a href=&#34;https://github.com/tudelft-atlarge/graphalytics&#34;&gt;https://github.com/tudelft-atlarge/graphalytics&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>LDBC and Apache Flink</title>
      <link>https://ldbc.github.io/post/ldbc-and-apache-flink/</link>
      <pubDate>Mon, 16 Nov 2015 14:47:00 +0000</pubDate>
      
      <guid>https://ldbc.github.io/post/ldbc-and-apache-flink/</guid>
      <description>&lt;p&gt;Apache Flink &lt;a href=&#34;#references&#34;&gt;[1]&lt;/a&gt; is an open source platform
for distributed stream and batch data processing. Flink&amp;rsquo;s core is a
streaming dataflow engine that provides data distribution,
communication, and fault tolerance for distributed computations over
data streams. Flink also builds batch processing on top of the streaming
engine, overlaying native iteration support, managed memory, and program
optimization.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://flink.apache.org/img/flink-stack-small.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Flink offers multiple APIs to process data from various data sources
(e.g. HDFS, HBase, Kafka and JDBC). The DataStream and DataSet APIs
allow the user to apply general-purpose data operations, like map,
reduce, groupBy and join, on streams and static data respectively. In
addition, Flink provides libraries for machine learning (Flink ML),
graph processing (Gelly) and SQL-like operations (Table). All APIs can
be used together in a single Flink program which enables the definition
of powerful analytical workflows and the implementation of distributed
algorithms.&lt;/p&gt;
&lt;p&gt;The following snippet shows how a wordcount program can be expressed in
Flink using the DataSet API:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-java&#34; data-lang=&#34;java&#34;&gt;DataSet&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;String&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; text &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; env&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;fromElements&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;
  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;He who controls the past controls the future.&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;,&lt;/span&gt;
  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;He who controls the present controls the past.&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;);&lt;/span&gt;

DataSet&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;Tuple2&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;String&lt;span style=&#34;color:#f92672&#34;&gt;,&lt;/span&gt; Integer&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; wordCounts &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; text
  &lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;flatMap&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;new&lt;/span&gt; LineSplitter&lt;span style=&#34;color:#f92672&#34;&gt;())&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;// splits the line and outputs (word,1)
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
tuples&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;groupBy&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;0&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;// group by word
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;sum&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;1&lt;span style=&#34;color:#f92672&#34;&gt;);&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;// sum the 1&amp;#39;s
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
wordCounts&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;print&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;();&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;At the Leipzig University, we use Apache Flink as execution layer for
our graph analytics platform Gradoop
&lt;a href=&#34;#references&#34;&gt;[2]&lt;/a&gt;. The LDBC datagen helps us
to evaluate the scalability of our algorithms and operators in a
distributed execution environment. To use the generated graph data in
Flink, we wrote a tool that transforms the LDBC output files into Flink
data sets for further processing &lt;a href=&#34;#references&#34;&gt;[3]&lt;/a&gt;. Using the class
&lt;code&gt;LDBCToFlink&lt;/code&gt;, LDBC output files can be read directly from HDFS or from
the local file system:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-java&#34; data-lang=&#34;java&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;final&lt;/span&gt; ExecutionEnvironment env &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;
  ExecutionEnvironment&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;getExecutionEnvironment&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;();&lt;/span&gt;

&lt;span style=&#34;color:#66d9ef&#34;&gt;final&lt;/span&gt; LDBCToFlink ldbcToFlink &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;new&lt;/span&gt; LDBCToFlink&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;
  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;hdfs:///ldbc_snb_datagen/social_network&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;// or &amp;#34;/path/to/social_network&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  env&lt;span style=&#34;color:#f92672&#34;&gt;);&lt;/span&gt;

DataSet&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;LDBCVertex&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; vertices &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ldbcToFlink&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;getVertices&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;();&lt;/span&gt;

DataSet&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;LDBCEdge&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; edges &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ldbcToFlink&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;getEdges&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;();&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The tuple classes &lt;code&gt;LDBCVertex&lt;/code&gt; and &lt;code&gt;LDBCEdge&lt;/code&gt; hold the information generated
by the LDBC datagen and are created directly from its output files.
During the transformation process, globally unique vertex identifiers
are created based on the LDBC identifier and the vertex class. When
reading edge files, source and target vertex identifiers are computed in
the same way to ensure consistent linking between vertices.&lt;/p&gt;
&lt;p&gt;Each &lt;code&gt;LDBCVertex&lt;/code&gt; instance contains:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;an identifier, which is unique among all vertices&lt;/li&gt;
&lt;li&gt;a vertex label (e.g. &lt;code&gt;Person&lt;/code&gt;, &lt;code&gt;Comment&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;a key-value map of properties including also multivalued properties
(e.g. &lt;code&gt;Person.email&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each &lt;code&gt;LDBCEdge&lt;/code&gt; instance contains:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;an identifier, which is unique among all edges&lt;/li&gt;
&lt;li&gt;an edge label (e.g. &lt;code&gt;knows&lt;/code&gt;, &lt;code&gt;likes&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;a source vertex identifier&lt;/li&gt;
&lt;li&gt;a target vertex identifier&lt;/li&gt;
&lt;li&gt;a key-value map of properties&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The resulting datasets can be used by the DataSet API and all libraries
that are built on top of it (i.e. Flink ML, Gelly and Table). In the
following example, we load the LDBC graph from HDFS, filter vertices
with the label &lt;code&gt;Person&lt;/code&gt; and edges with the label &lt;code&gt;knows&lt;/code&gt; and use
Gelly to compute the connected components of that subgraph. The full
source code is available on GitHub &lt;a href=&#34;#references&#34;&gt;[4]&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-java&#34; data-lang=&#34;java&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;final&lt;/span&gt; ExecutionEnvironment env &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;
  ExecutionEnvironment&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;getExecutionEnvironment&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;();&lt;/span&gt;

&lt;span style=&#34;color:#66d9ef&#34;&gt;final&lt;/span&gt; LDBCToFlink ldbcToFlink &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;new&lt;/span&gt; LDBCToFlink&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;
  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/home/s1ck/Devel/Java/ldbc_snb_datagen/social_network&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;,&lt;/span&gt;
  env&lt;span style=&#34;color:#f92672&#34;&gt;);&lt;/span&gt;

&lt;span style=&#34;color:#75715e&#34;&gt;// filter vertices with label “Person”
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;DataSet&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;LDBCVertex&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; ldbcVertices &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ldbcToFlink&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;getVertices&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;()&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;filter&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;new&lt;/span&gt; VertexLabelFilter&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;LDBCConstants&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;VERTEX_CLASS_PERSON&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;));&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;// filter edges with label “knows”
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;DataSet&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;LDBCEdge&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; ldbcEdges &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ldbcToFlink&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;getEdges&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;()&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;filter&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;new&lt;/span&gt; EdgeLabelFilter&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;LDBCConstants&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;EDGE_CLASS_KNOWS&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;));&lt;/span&gt;

&lt;span style=&#34;color:#75715e&#34;&gt;// create Gelly vertices suitable for connected components
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;DataSet&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;Vertex&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;Long&lt;span style=&#34;color:#f92672&#34;&gt;,&lt;/span&gt; Long&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; vertices &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ldbcVertices&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;map&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;new&lt;/span&gt; VertexInitializer&lt;span style=&#34;color:#f92672&#34;&gt;());&lt;/span&gt;

&lt;span style=&#34;color:#75715e&#34;&gt;// create Gelly edges suitable for connected components
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;DataSet&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;Edge&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;Long&lt;span style=&#34;color:#f92672&#34;&gt;,&lt;/span&gt; NullValue&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; edges &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ldbcEdges&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;map&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;new&lt;/span&gt; EdgeInitializer&lt;span style=&#34;color:#f92672&#34;&gt;());&lt;/span&gt;

&lt;span style=&#34;color:#75715e&#34;&gt;// create Gelly graph
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;Graph&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;Long&lt;span style=&#34;color:#f92672&#34;&gt;,&lt;/span&gt; Long&lt;span style=&#34;color:#f92672&#34;&gt;,&lt;/span&gt; NullValue&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; g &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Graph&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;fromDataSet&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;vertices&lt;span style=&#34;color:#f92672&#34;&gt;,&lt;/span&gt; edges&lt;span style=&#34;color:#f92672&#34;&gt;,&lt;/span&gt; env&lt;span style=&#34;color:#f92672&#34;&gt;);&lt;/span&gt;

&lt;span style=&#34;color:#75715e&#34;&gt;// run connected components on the subgraph for 10 iterations
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;DataSet&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;Vertex&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;Long&lt;span style=&#34;color:#f92672&#34;&gt;,&lt;/span&gt; Long&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; components &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;
  g&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;run&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;new&lt;/span&gt; ConnectedComponents&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;Long&lt;span style=&#34;color:#f92672&#34;&gt;,&lt;/span&gt; NullValue&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;(&lt;/span&gt;10&lt;span style=&#34;color:#f92672&#34;&gt;));&lt;/span&gt;

&lt;span style=&#34;color:#75715e&#34;&gt;// print the component id of the first 10 vertices
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;components&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;first&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;10&lt;span style=&#34;color:#f92672&#34;&gt;).&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;print&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;();&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The ldbc-flink-import tool is available on Github
&lt;a href=&#34;#references&#34;&gt;[3]&lt;/a&gt; and licensed under the
GNU GPLv3. If you have any questions regarding the tool please feel free
to contact me on GitHub. If you find bugs or have any ideas for
improvements, please create an issue or a pull request.&lt;/p&gt;
&lt;p&gt;If you want to learn more about Apache Flink, a good starting point is
the main documentation
&lt;a href=&#34;#references&#34;&gt;[5]&lt;/a&gt; and
if you have any question feel free to ask the official mailing lists.
There is also a nice set of videos
&lt;a href=&#34;#references&#34;&gt;[6]&lt;/a&gt; available
from the latest Flink Forward conference.&lt;/p&gt;
&lt;h5 id=&#34;references&#34;&gt;References&lt;/h5&gt;
&lt;p&gt;[1]
&lt;a href=&#34;http://flink.apache.org/&#34;&gt;http://flink.apache.org/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2]
&lt;a href=&#34;https://github.com/dbs-leipzig/gradoop&#34;&gt;https://github.com/dbs-leipzig/gradoop&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[3]
&lt;a href=&#34;https://github.com/s1ck/ldbc-flink-import&#34;&gt;https://github.com/s1ck/ldbc-flink-import&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[4]
&lt;a href=&#34;https://gist.github.com/s1ck/b33e6a4874c15c35cd16&#34;&gt;https://gist.github.com/s1ck/b33e6a4874c15c35cd16&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[5]
&lt;a href=&#34;https://ci.apache.org/projects/flink/flink-docs-release-0.10/&#34;&gt;https://ci.apache.org/projects/flink/flink-docs-release-0.10/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[6]
&lt;a href=&#34;https://www.youtube.com/channel/UCY8_lgiZLZErZPF47a2hXMA&#34;&gt;https://www.youtube.com/channel/UCY8_lgiZLZErZPF47a2hXMA&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Elements of Instance Matching Benchmarks: a Short Overview</title>
      <link>https://ldbc.github.io/post/elements-of-instance-matching-benchmarks-a-short-overview/</link>
      <pubDate>Tue, 16 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>https://ldbc.github.io/post/elements-of-instance-matching-benchmarks-a-short-overview/</guid>
      <description>&lt;p&gt;The number of datasets published in the Web of Data as part of the
Linked Data Cloud is constantly increasing. The Linked Data paradigm is
based on the unconstrained publication of information by different
publishers, and the interlinking of web resources through “same-as”
links which specify that two URIs correspond to the same real world
object. In the vast number of data sources participating in the Linked
Data Cloud, this information is not explicitly stated but is discovered
using &lt;strong&gt;instance matching&lt;/strong&gt; techniques and tools. Instance matching is
also known as &lt;strong&gt;record linkage&lt;/strong&gt; [[1]], &lt;strong&gt;duplicate detection&lt;/strong&gt;
&lt;a href=&#34;#references&#34;&gt;[2]&lt;/a&gt;, &lt;strong&gt;entity resolution&lt;/strong&gt; &lt;a href=&#34;#references&#34;&gt;[3]&lt;/a&gt; and &lt;strong&gt;object
identification&lt;/strong&gt; &lt;a href=&#34;#references&#34;&gt;[4]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For instance, a search in Geonames
(&lt;a href=&#34;http://www.geonames.org/&#34;&gt;http://www.geonames.org/&lt;/a&gt;) for &amp;ldquo;Athens&amp;rdquo; would
return a resource (i.e., URI) accompanied with a map of the area and
information about the place; additional information for the city of
Athens can be found in other datasets such as for instance DBpedia
(&lt;a href=&#34;http://dbpedia.org/&#34;&gt;http://dbpedia.org/&lt;/a&gt;) or Open Government Datasets (&lt;a href=&#34;http://data.gov.gr/)&#34;&gt;http://data.gov.gr/)&lt;/a&gt;.
To exploit all obtain all necessary information about the city of Athens
we need to establish that the retrieved resources refer to the same real
world object.&lt;/p&gt;
&lt;p&gt;Web resources are published by &amp;ldquo;autonomous agents&amp;rdquo; who choose their
preferred information representation or the one that best fits the
application of interest. Furthermore, different representations of the
same real world entity are due to data acquisition errors or different
acquisition techniques used to process scientific data. Moreover, real
world entities evolve and change over time, and sources need to keep
track of these developments, a task that is very hard and often not
possible. Finally, when integrating data from multiple sources, the
process itself may add new erroneous data. Clearly, these reasons are
not limited to problems that did arise in the era of Web Data, it is
thus not surprising that instance matching systems have been around for
several years &lt;a href=&#34;#references&#34;&gt;[2]&lt;/a&gt;&lt;a href=&#34;#references&#34;&gt;[5]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It is though essential at this point to develop, along with instance and
entity matching systems, &lt;em&gt;instance matching benchmarks to determine the
weak and strong points of those systems, as well as their overall
quality in order to support users in deciding the system to use for
their needs&lt;/em&gt;. Hence, well defined, and good quality benchmarks are
important for comparing the performance of the available or under
development instance matching systems. Benchmarks are used not only to
inform users of the strengths and weaknesses of systems, but also to
motivate developers, researchers and technology vendors to deal with the
weak points of their systems and to ameliorate their performance and
functionality. They are also useful for identifying the settings in
which each of the systems has optimal performance. Benchmarking aims at
providing an objective basis for such assessments.&lt;/p&gt;
&lt;p&gt;An instance matching benchmark for Linked Data consists of a &lt;em&gt;source&lt;/em&gt;
and &lt;em&gt;target dataset&lt;/em&gt; implementing a set of &lt;em&gt;test-cases&lt;/em&gt;, where each test
case addresses a different kind of requirement regarding instance
matching, a &lt;em&gt;ground truth&lt;/em&gt; or &lt;em&gt;gold standard&lt;/em&gt; and finally the
&lt;em&gt;evaluation metrics&lt;/em&gt; used to &lt;em&gt;assess the benchmark.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Datasets are the raw material of a benchmark. A benchmark comprises of a
&lt;em&gt;source&lt;/em&gt; and &lt;em&gt;target&lt;/em&gt; dataset and the objective of an instance matching
system is to discover the matches of the two. Datasets are characterized
by (a) their &lt;em&gt;nature&lt;/em&gt; (&lt;em&gt;real&lt;/em&gt; or &lt;em&gt;synthetic&lt;/em&gt;), (b) the
&lt;em&gt;schemas/ontologies&lt;/em&gt; they use,  (c) their &lt;em&gt;domains&lt;/em&gt;,  (d) the
&lt;em&gt;languages&lt;/em&gt; they are written in, and (e) the
&lt;em&gt;variations/heterogeneities&lt;/em&gt; of the datasets. Real datasets are widely
used in benchmarks since they offer realistic conditions for
heterogeneity problems and they have realistic distributions. &lt;em&gt;Synthetic
datasets&lt;/em&gt; are generated using automated data generators and  are useful
because they offer fully controlled test conditions, have accurate gold
standards and allow setting the focus on specific types of heterogeneity
problems in a systematic manner&lt;/p&gt;
&lt;p&gt;Datasets (and benchmarks) may contain different &lt;em&gt;kinds of variations&lt;/em&gt;
that correspond to &lt;em&gt;different test cases&lt;/em&gt;. According to Ferrara et.al.
&lt;a href=&#34;#references&#34;&gt;[6]&lt;/a&gt;&lt;a href=&#34;#references&#34;&gt;[7]&lt;/a&gt;, three kinds of variations exist for Linked
Data, namely &lt;em&gt;data variations&lt;/em&gt;, &lt;em&gt;structural variations&lt;/em&gt; and &lt;em&gt;logical
variations&lt;/em&gt;. The first refers mainly to differences due to typographical
errors, differences in the employed data formats, language etc. The
second refers to the differences in the structure of the employed Linked
Data schemas. Finally, the third  type derives from the use of
semantically rich RDF and OWL constructs that enable one to define
hierarchies and equivalence of classes and properties, (in)equality of
instances, complex class definitions through union and intersection
among others.&lt;/p&gt;
&lt;p&gt;The common case in real benchmarks is that the datasets to be matched
contain different kinds (combinations) of variations. On the other hand,
synthetic datasets may be purposefully designed to contain specific
types (or combinations) of variations (e.g., only structural), or may be
more general in an effort to illustrate all the common cases of
discrepancies that appear in reality between individual descriptions.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;gold standard&lt;/em&gt; is considered as the “correct answer sheet” of the
benchmark, and is used to judge the completeness and soundness of the
result sets of the benchmarked systems. For instance matching benchmarks
employing synthetic datasets, the gold standard is always automatically
generated, as the errors (variations) that are added into the datasets
are known and systematically created. When it comes to real datasets,
the gold standard can be either manually curated or (semi-)
automatically generated. In the first case, domain experts manually mark
the matches between the datasets, whereas in the second, supervised and
crowdsourcing techniques aid the process of finding the matches, a
process that is often time consuming and error prone.&lt;/p&gt;
&lt;p&gt;Last, an instance matching benchmark uses &lt;em&gt;evaluation metrics&lt;/em&gt; to
determine and assess the systems’ output quality and performance. For
instance matching tools, performance is not a critical aspect.  On the
other hand, an instance matching tool should return all and only the
correct answers. So, what matters most is returning the relevant
matches, rather than returning them quickly. For this reason, the
evaluation metrics that are dominantly employed for instance matching
benchmarks are the standard &lt;em&gt;precision&lt;/em&gt;, &lt;em&gt;recall&lt;/em&gt; and &lt;em&gt;f-measure&lt;/em&gt;
metrics.&lt;/p&gt;
&lt;h5 id=&#34;references&#34;&gt;References&lt;/h5&gt;
&lt;p&gt;[1] Li, C., Jin, L., and Mehrotra, S. (2006) Supporting efficient
record linkage for large data sets using mapping techniques. WWW 2006.&lt;/p&gt;
&lt;p&gt;[2] Dragisic, Z., Eckert, K., Euzenat, J., Faria, D., Ferrara, A.,
Granada, R., Ivanova, V.,Jimenez-Ruiz, E., Oskar Kempf, A., Lambrix, P.,
Montanelli, S., Paulheim, H., Ritze, D., Shvaiko, P., Solimando, A.,
Trojahn, C., Zamaza, O., and Cuenca Grau,  B. (2014) Results of the
Ontology Alignment Evaluation Initiative 2014. Proc. 9th ISWC workshop
on ontology matching (OM 2014).&lt;/p&gt;
&lt;p&gt;[3] Bhattacharya, I. and Getoor, L. (2006) Entity resolution in
graphs. Mining Graph Data. Wiley and Sons 2006.&lt;/p&gt;
&lt;p&gt;[4] Noessner, J., Niepert, M., Meilicke, C., and Stuckenschmidt,
H. (2010) Leveraging Terminological Structure for Object Reconciliation.
In ESWC 2010.&lt;/p&gt;
&lt;p&gt;[5] Flouris, G., Manakanatas, D., Kondylakis, H., Plexousakis, D.,
Antoniou, G. Ontology Change: Classification and Survey (2008) Knowledge
Engineering Review (KER 2008), pages 117-152.&lt;/p&gt;
&lt;p&gt;[6] Ferrara, A., Lorusso, D., Montanelli, S., and Varese, G.
(2008) Towards a Benchmark for Instance Matching. Proc. 3th ISWC
workshop on ontology matching (OM 2008).&lt;/p&gt;
&lt;p&gt;[7] Ferrara, A., Montanelli, S., Noessner, J., and Stuckenschmidt,
H. (2011) Benchmarking Matching Applications on the Semantic Web. In
ESWC, 2011.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SNB Interactive Part 3: Choke Points and Initial Run on Virtuoso</title>
      <link>https://ldbc.github.io/post/snb-interactive-part-3-choke-points-and-initial-run-on-virtuoso/</link>
      <pubDate>Wed, 10 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>https://ldbc.github.io/post/snb-interactive-part-3-choke-points-and-initial-run-on-virtuoso/</guid>
      <description>&lt;p&gt;In this post we will look at running the &lt;a href=&#34;http://ldbcouncil.org/developer/snb&#34;&gt;LDBC SNB&lt;/a&gt; on &lt;a href=&#34;http://virtuoso.openlinksw.com/&#34;&gt;Virtuoso&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;First, let&amp;rsquo;s recap what the benchmark is about:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;fairly frequent short updates, with no update contention worth
mentioning&lt;/li&gt;
&lt;li&gt;short random lookups&lt;/li&gt;
&lt;li&gt;medium complex queries centered around a person&amp;rsquo;s social environment&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The updates exist so as to invalidate strategies that rely too heavily
on precomputation. The short lookups exist for the sake of realism;
after all, an online social application does lookups for the most part.
The medium complex queries are to challenge the DBMS.&lt;/p&gt;
&lt;p&gt;The DBMS challenges have to do firstly with query optimization, and
secondly with execution with a lot of non-local random access patterns.
Query optimization is not a requirement, &lt;em&gt;per se,&lt;/em&gt; since imperative
implementations are allowed, but we will see that these are no more free
of the laws of nature than the declarative ones.&lt;/p&gt;
&lt;p&gt;The workload is arbitrarily parallel, so intra-query parallelization is
not particularly useful, if also not harmful. There are latency
constraints on operations which strongly encourage implementations to
stay within a predictable time envelope regardless of specific query
parameters. The parameters are a combination of person and date range,
and sometimes tags or countries. The hardest queries have the potential
to access all content created by people within 2 steps of a central
person, so possibly thousands of people, times 2000 posts per person,
times up to 4 tags per post. We are talking in the millions of key
lookups, aiming for sub-second single-threaded execution.&lt;/p&gt;
&lt;p&gt;The test system is the same as used in
the &lt;a href=&#34;http://www.openlinksw.com/weblog/oerling/?id=1739&#34;&gt;TPC-H series&lt;/a&gt;:
dual Xeon E5-2630, 2x6 cores x 2 threads, 2.3GHz, 192 GB RAM. The
software is the &lt;a href=&#34;https://github.com/v7fasttrack/virtuoso-opensource/tree/feature/analytics&#34;&gt;feature/analytics branch&lt;/a&gt; of &lt;a href=&#34;https://github.com/v7fasttrack/virtuoso-opensource/&#34;&gt;v7fasttrack, available from www.github.com&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The dataset is the SNB 300G set, with:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;1,136,127&lt;/th&gt;
&lt;th&gt;persons&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;125,249,604&lt;/td&gt;
&lt;td&gt;knows edges&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;847,886,644&lt;/td&gt;
&lt;td&gt;posts, including replies&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1,145,893,841&lt;/td&gt;
&lt;td&gt;tags of posts or replies&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1,140,226,235&lt;/td&gt;
&lt;td&gt;likes of posts or replies&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As an initial step, we run the benchmark as fast as it will go. We use
32 threads on the driver side for 24 hardware threads.&lt;/p&gt;
&lt;p&gt;Below are the numerical quantities for a 400K operation run after 150K
operations worth of warmup.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Duration:&lt;/strong&gt; 10:41.251&lt;br&gt;
&lt;strong&gt;Throughput:&lt;/strong&gt; 623.71 (op/s)&lt;/p&gt;
&lt;p&gt;The statistics that matter are detailed below, with operations ranked in
order of descending client-side wait-time. All times are in
milliseconds.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;% of total&lt;/th&gt;
&lt;th&gt;total_wait&lt;/th&gt;
&lt;th&gt;name&lt;/th&gt;
&lt;th&gt;count&lt;/th&gt;
&lt;th&gt;mean&lt;/th&gt;
&lt;th&gt;min&lt;/th&gt;
&lt;th&gt;max&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;20 %&lt;/td&gt;
&lt;td&gt;&amp;ldquo;4,231,130&amp;rdquo;&lt;/td&gt;
&lt;td&gt;LdbcQuery5&lt;/td&gt;
&lt;td&gt;656&lt;/td&gt;
&lt;td&gt;&amp;ldquo;6,449.89&amp;rdquo;&lt;/td&gt;
&lt;td&gt;245&lt;/td&gt;
&lt;td&gt;&amp;ldquo;10,311&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;11 %&lt;/td&gt;
&lt;td&gt;&amp;ldquo;2,272,954&amp;rdquo;&lt;/td&gt;
&lt;td&gt;LdbcQuery8&lt;/td&gt;
&lt;td&gt;&amp;ldquo;18,354&amp;rdquo;&lt;/td&gt;
&lt;td&gt;123.84&lt;/td&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;td&gt;&amp;ldquo;2,240&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10 %&lt;/td&gt;
&lt;td&gt;&amp;ldquo;2,200,718&amp;rdquo;&lt;/td&gt;
&lt;td&gt;LdbcQuery3&lt;/td&gt;
&lt;td&gt;388&lt;/td&gt;
&lt;td&gt;&amp;ldquo;5,671.95&amp;rdquo;&lt;/td&gt;
&lt;td&gt;468&lt;/td&gt;
&lt;td&gt;&amp;ldquo;17,368&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;7.3 %&lt;/td&gt;
&lt;td&gt;&amp;ldquo;1,561,382&amp;rdquo;&lt;/td&gt;
&lt;td&gt;LdbcQuery14&lt;/td&gt;
&lt;td&gt;&amp;ldquo;1,124&amp;rdquo;&lt;/td&gt;
&lt;td&gt;&amp;ldquo;1,389.13&amp;rdquo;&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;&amp;ldquo;5,724&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;6.7 %&lt;/td&gt;
&lt;td&gt;&amp;ldquo;1,441,575&amp;rdquo;&lt;/td&gt;
&lt;td&gt;LdbcQuery12&lt;/td&gt;
&lt;td&gt;&amp;ldquo;1,252&amp;rdquo;&lt;/td&gt;
&lt;td&gt;&amp;ldquo;1,151.42&amp;rdquo;&lt;/td&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;td&gt;&amp;ldquo;3,273&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;6.5 %&lt;/td&gt;
&lt;td&gt;&amp;ldquo;1,396,932&amp;rdquo;&lt;/td&gt;
&lt;td&gt;LdbcQuery10&lt;/td&gt;
&lt;td&gt;&amp;ldquo;1,252&amp;rdquo;&lt;/td&gt;
&lt;td&gt;&amp;ldquo;1,115.76&amp;rdquo;&lt;/td&gt;
&lt;td&gt;13&lt;/td&gt;
&lt;td&gt;&amp;ldquo;4,743&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5 %&lt;/td&gt;
&lt;td&gt;&amp;ldquo;1,064,457&amp;rdquo;&lt;/td&gt;
&lt;td&gt;LdbcShortQuery3PersonFriends&lt;/td&gt;
&lt;td&gt;&amp;ldquo;46,285&amp;rdquo;&lt;/td&gt;
&lt;td&gt;22.9979&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;&amp;ldquo;2,287&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4.9 %&lt;/td&gt;
&lt;td&gt;&amp;ldquo;1,047,536&amp;rdquo;&lt;/td&gt;
&lt;td&gt;LdbcShortQuery2PersonPosts&lt;/td&gt;
&lt;td&gt;&amp;ldquo;46,285&amp;rdquo;&lt;/td&gt;
&lt;td&gt;22.6323&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;&amp;ldquo;2,156&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4.1 %&lt;/td&gt;
&lt;td&gt;&amp;ldquo;885,102&amp;rdquo;&lt;/td&gt;
&lt;td&gt;LdbcQuery6&lt;/td&gt;
&lt;td&gt;&amp;ldquo;1,721&amp;rdquo;&lt;/td&gt;
&lt;td&gt;514.295&lt;/td&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;&amp;ldquo;5,227&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3.3 %&lt;/td&gt;
&lt;td&gt;&amp;ldquo;707,901&amp;rdquo;&lt;/td&gt;
&lt;td&gt;LdbcQuery1&lt;/td&gt;
&lt;td&gt;&amp;ldquo;2,117&amp;rdquo;&lt;/td&gt;
&lt;td&gt;334.389&lt;/td&gt;
&lt;td&gt;28&lt;/td&gt;
&lt;td&gt;&amp;ldquo;3,467&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2.4 %&lt;/td&gt;
&lt;td&gt;&amp;ldquo;521,738&amp;rdquo;&lt;/td&gt;
&lt;td&gt;LdbcQuery4&lt;/td&gt;
&lt;td&gt;&amp;ldquo;1,530&amp;rdquo;&lt;/td&gt;
&lt;td&gt;341.005&lt;/td&gt;
&lt;td&gt;49&lt;/td&gt;
&lt;td&gt;&amp;ldquo;2,774&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2.1 %&lt;/td&gt;
&lt;td&gt;&amp;ldquo;440,197&amp;rdquo;&lt;/td&gt;
&lt;td&gt;LdbcShortQuery4MessageContent&lt;/td&gt;
&lt;td&gt;&amp;ldquo;46,302&amp;rdquo;&lt;/td&gt;
&lt;td&gt;9.50708&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;&amp;ldquo;2,015&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.9 %&lt;/td&gt;
&lt;td&gt;&amp;ldquo;407,450&amp;rdquo;&lt;/td&gt;
&lt;td&gt;LdbcUpdate5AddForumMembership&lt;/td&gt;
&lt;td&gt;&amp;ldquo;14,338&amp;rdquo;&lt;/td&gt;
&lt;td&gt;28.4175&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;&amp;ldquo;2,008&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.9 %&lt;/td&gt;
&lt;td&gt;&amp;ldquo;405,243&amp;rdquo;&lt;/td&gt;
&lt;td&gt;LdbcShortQuery7MessageReplies&lt;/td&gt;
&lt;td&gt;&amp;ldquo;46,302&amp;rdquo;&lt;/td&gt;
&lt;td&gt;8.75217&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;&amp;ldquo;2,112&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.9 %&lt;/td&gt;
&lt;td&gt;&amp;ldquo;404,002&amp;rdquo;&lt;/td&gt;
&lt;td&gt;LdbcShortQuery6MessageForum&lt;/td&gt;
&lt;td&gt;&amp;ldquo;46,302&amp;rdquo;&lt;/td&gt;
&lt;td&gt;8.72537&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;&amp;ldquo;1,968&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.8 %&lt;/td&gt;
&lt;td&gt;&amp;ldquo;387,044&amp;rdquo;&lt;/td&gt;
&lt;td&gt;LdbcUpdate3AddCommentLike&lt;/td&gt;
&lt;td&gt;&amp;ldquo;12,659&amp;rdquo;&lt;/td&gt;
&lt;td&gt;30.5746&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;&amp;ldquo;2,060&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.7 %&lt;/td&gt;
&lt;td&gt;&amp;ldquo;361,290&amp;rdquo;&lt;/td&gt;
&lt;td&gt;LdbcShortQuery1PersonProfile&lt;/td&gt;
&lt;td&gt;&amp;ldquo;46,285&amp;rdquo;&lt;/td&gt;
&lt;td&gt;7.80577&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;&amp;ldquo;2,015&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1.6 %&lt;/td&gt;
&lt;td&gt;&amp;ldquo;334,409&amp;rdquo;&lt;/td&gt;
&lt;td&gt;LdbcShortQuery5MessageCreator&lt;/td&gt;
&lt;td&gt;&amp;ldquo;46,302&amp;rdquo;&lt;/td&gt;
&lt;td&gt;7.22234&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;&amp;ldquo;2,055&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;1 %&lt;/td&gt;
&lt;td&gt;&amp;ldquo;220,740&amp;rdquo;&lt;/td&gt;
&lt;td&gt;LdbcQuery2&lt;/td&gt;
&lt;td&gt;&amp;ldquo;1,488&amp;rdquo;&lt;/td&gt;
&lt;td&gt;148.347&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;&amp;ldquo;2,504&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.96 %&lt;/td&gt;
&lt;td&gt;&amp;ldquo;205,910&amp;rdquo;&lt;/td&gt;
&lt;td&gt;LdbcQuery7&lt;/td&gt;
&lt;td&gt;&amp;ldquo;1,721&amp;rdquo;&lt;/td&gt;
&lt;td&gt;119.646&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;td&gt;&amp;ldquo;2,295&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.93 %&lt;/td&gt;
&lt;td&gt;&amp;ldquo;198,971&amp;rdquo;&lt;/td&gt;
&lt;td&gt;LdbcUpdate2AddPostLike&lt;/td&gt;
&lt;td&gt;&amp;ldquo;5,974&amp;rdquo;&lt;/td&gt;
&lt;td&gt;33.3062&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;&amp;ldquo;1,987&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.88 %&lt;/td&gt;
&lt;td&gt;&amp;ldquo;189,871&amp;rdquo;&lt;/td&gt;
&lt;td&gt;LdbcQuery11&lt;/td&gt;
&lt;td&gt;&amp;ldquo;2,294&amp;rdquo;&lt;/td&gt;
&lt;td&gt;82.7685&lt;/td&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;&amp;ldquo;2,219&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.85 %&lt;/td&gt;
&lt;td&gt;&amp;ldquo;182,964&amp;rdquo;&lt;/td&gt;
&lt;td&gt;LdbcQuery13&lt;/td&gt;
&lt;td&gt;&amp;ldquo;2,898&amp;rdquo;&lt;/td&gt;
&lt;td&gt;63.1346&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;&amp;ldquo;2,201&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.74 %&lt;/td&gt;
&lt;td&gt;&amp;ldquo;158,188&amp;rdquo;&lt;/td&gt;
&lt;td&gt;LdbcQuery9&lt;/td&gt;
&lt;td&gt;78&lt;/td&gt;
&lt;td&gt;&amp;ldquo;2,028.05&amp;rdquo;&lt;/td&gt;
&lt;td&gt;&amp;ldquo;1,108&amp;rdquo;&lt;/td&gt;
&lt;td&gt;&amp;ldquo;4,183&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.67 %&lt;/td&gt;
&lt;td&gt;&amp;ldquo;143,457&amp;rdquo;&lt;/td&gt;
&lt;td&gt;LdbcUpdate7AddComment&lt;/td&gt;
&lt;td&gt;&amp;ldquo;3,986&amp;rdquo;&lt;/td&gt;
&lt;td&gt;35.9902&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;&amp;ldquo;1,912&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.26 %&lt;/td&gt;
&lt;td&gt;&amp;ldquo;54,947&amp;rdquo;&lt;/td&gt;
&lt;td&gt;LdbcUpdate8AddFriendship&lt;/td&gt;
&lt;td&gt;571&lt;/td&gt;
&lt;td&gt;96.2294&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;988&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.2 %&lt;/td&gt;
&lt;td&gt;&amp;ldquo;43,451&amp;rdquo;&lt;/td&gt;
&lt;td&gt;LdbcUpdate6AddPost&lt;/td&gt;
&lt;td&gt;&amp;ldquo;1,386&amp;rdquo;&lt;/td&gt;
&lt;td&gt;31.3499&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;&amp;ldquo;2,060&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.01%&lt;/td&gt;
&lt;td&gt;&amp;ldquo;1,848&amp;rdquo;&lt;/td&gt;
&lt;td&gt;LdbcUpdate4AddForum&lt;/td&gt;
&lt;td&gt;103&lt;/td&gt;
&lt;td&gt;17.9417&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;65&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0.00%&lt;/td&gt;
&lt;td&gt;44&lt;/td&gt;
&lt;td&gt;LdbcUpdate1AddPerson&lt;/td&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;22&lt;/td&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;34&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;At this point we have in-depth knowledge of the choke points the
benchmark stresses, and we can give a first assessment of whether the
design meets its objectives for setting an agenda for the coming years
of graph database development.&lt;/p&gt;
&lt;p&gt;The implementation is well optimized in general but still has maybe 30%
room for improvement. We note that this is based on a compressed column
store. One could think that alternative data representations, like
in-memory graphs of structs and pointers between them, are better for
the task. This is not necessarily so; at the least, a compressed column
store is much more space efficient. Space efficiency is the root of cost
efficiency, since as soon as the working set is not in memory, a random
access workload is badly hit.&lt;/p&gt;
&lt;p&gt;The set of choke points (technical challenges) actually revealed by the
benchmark is so far as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Cardinality estimation under heavy data skew —&lt;/em&gt; Many queries take
a tag or a country as a parameter. The cardinalities associated
with tags vary from 29M posts for the most common to 1 for the least
common. Q6 has a common tag (in top few hundred) half the time and a
random, most often very infrequent, one the rest of the time. A
declarative implementation must recognize the cardinality implications
from the literal and plan accordingly. An imperative one would have to
count. Missing this makes Q6 take about 40% of the time instead of 4.1%
when adapting.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Covering indices —&lt;/em&gt; Being able to make multi-column indices that
duplicate some columns from the table often saves an entire table
lookup. For example, an index onpost by author can also contain
the post&amp;rsquo;s creation date.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Multi-hop graph traversal —&lt;/em&gt; Most queries access a two-hop
environment starting at a person. Two queries look for shortest paths of
unbounded length. For the two-hop case, it makes almost no difference
whether this is done as a union or a special graph traversal operator.
For shortest paths, this simply must be built into the engine; doing
this client-side incurs prohibitive overheads. A bidirectional shortest
path operation is a requirement for the benchmark.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Top &lt;em&gt;K&lt;/em&gt; —&lt;/em&gt; Most queries returning posts order results by
descending date. Once there are at least &lt;em&gt;k&lt;/em&gt; results, anything older
than the __k__th can be dropped, adding a dateselection as early as
possible in the query. This interacts with vectored execution, so that
starting with a short vector size more rapidly produces an initial
top &lt;em&gt;k&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Late projection —&lt;/em&gt; Many queries access several columns and touch
millions of rows but only return a few. The columns that are not used in
sorting or selection can be retrieved only for the rows that are
actually returned. This is especially useful with a column store, as
this removes many large columns (e.g., text of a post) from the working
set.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Materialization —&lt;/em&gt; Q14 accesses an expensive-to-compute edge weight,
the number of post-reply pairs between two people. Keeping this
precomputed drops Q14 from the top place. Other materialization would be
possible, for example Q2 (top 20 posts by friends), but since Q2 is just
1% of the load, there is no need. One could of course argue that this
should be 20x more frequent, in which case there could be a point to
this.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Concurrency control —&lt;/em&gt; Read-write contention is rare, as updates are
randomly spread over the database. However, some pages get read very
frequently, e.g., some middle level index pages in the post table.
Keeping a count of reading threads requires a mutex, and there is
significant contention on this. Since the hot set can be one page,
adding more mutexes does not always help. However, hash partitioning the
index into many independent trees (as in the case of a cluster) helps
for this. There is also contention on a mutex for assigning threads to
client requests, as there are large numbers of short operations.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In subsequent posts, we will look at specific queries, what they in fact
do, and what their theoretical performance limits would be. In this way
we will have a precise understanding of which way SNB can steer the
graph DB community.&lt;/p&gt;
&lt;h3 id=&#34;snb-interactive-series&#34;&gt;SNB Interactive Series&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../snb-interactive-part-1-what-is-snb-interactive-really-about&#34;&gt;SNB Interactive, Part 1: What is SNB Interactive Really About?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../snb-interactive-part-2-modeling-choices&#34;&gt;SNB Interactive, Part 2: Modeling Choices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../snb-interactive-part-3-choke-points-and-initial-run-on-virtuoso/&#34;&gt;SNB Interactive, Part 3: Choke Points and Initial Run on Virtuoso&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>SNB and Graphs Related Presentations at GRADES &#39;15</title>
      <link>https://ldbc.github.io/post/snb-and-graphs-related-presentations-at-grades-15/</link>
      <pubDate>Fri, 29 May 2015 00:00:00 +0000</pubDate>
      
      <guid>https://ldbc.github.io/post/snb-and-graphs-related-presentations-at-grades-15/</guid>
      <description>&lt;p&gt;Next 31st of  May the GRADES workshop will take place in Melbourne within the ACM/SIGMOD presentation. GRADES started as an initiative of the Linked Data Benchmark Council in the SIGMOD/PODS 2013 held in New York.&lt;/p&gt;
&lt;p&gt;Among the papers published in this edition we have &amp;ldquo;Graphalytics: A Big Data Benchmark for Graph-Processing Platforms&amp;rdquo;, which presents a new benchmark that uses the Social Network Benchmark data generator of LDBC (that can be found in &lt;a href=&#34;https://github.com/ldbc&#34;&gt;https://github.com/ldbc&lt;/a&gt;) as the base to execute the algorithms used for the benchmark, among which we have BFS, community detection and connected components. We also have &amp;ldquo;Microblogging Queries on Graph Databases: an Introspection&amp;rdquo; which benchmarks two of the most significant Graph Databases in the market, i.e. Neo4j and Sparksee using microblogging queries on top of twitter data. We can finally mention &amp;ldquo;Frappé: Querying the Linux Kernel Dependency Graph&amp;rdquo; which presents a framework for querying and visualising the dependencies of large C/C++ software systems.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://event.cwi.nl/grades2015/program.shtml&#34;&gt;Check the complete agenda.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Meet you in Melbourne!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SNB Interactive Part 2:  Modeling Choices</title>
      <link>https://ldbc.github.io/post/snb-interactive-part-2-modeling-choices/</link>
      <pubDate>Tue, 26 May 2015 00:00:00 +0000</pubDate>
      
      <guid>https://ldbc.github.io/post/snb-interactive-part-2-modeling-choices/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;http://ldbcouncil.org/benchmarks/snb&#34;&gt;​SNB Interactive&lt;/a&gt; is the wild
frontier, with very few rules. This is necessary, among other reasons,
because there is no standard property graph data model, and because the
contestants support a broad mix of programming models, ranging from
in-process APIs to declarative query.&lt;/p&gt;
&lt;p&gt;In the case of &lt;a href=&#34;http://dbpedia.org/resource/Virtuoso_Universal_Server&#34;&gt;Virtuoso&lt;/a&gt;, we have played with &lt;a href=&#34;http://dbpedia.org/resource/SQL&#34;&gt;SQL&lt;/a&gt; and &lt;a href=&#34;http://dbpedia.org/resource/SPARQL&#34;&gt;SPARQL&lt;/a&gt; implementations. For a fixed schema and well known workload, SQL will always win. The reason for this is that this allows to materialize multi-part indices and data orderings that make sense for the application.  In other words, there is transparency into physical design. An RDF application may also have physical design by means ofstructure-aware storage but this is more complex and here we are just concerned with speed and having things work precisely as we intend.&lt;/p&gt;
&lt;h3 id=&#34;schema-design&#34;&gt;Schema Design&lt;/h3&gt;
&lt;p&gt;SNB has a regular schema described by a
&lt;a href=&#34;https://en.wikipedia.org/wiki/Unified_Modeling_Language&#34;&gt;UML&lt;/a&gt; diagram.
This has a number of relationships of which some have attributes.
There are no heterogenous sets, e.g. no need for run-time typed
attributes or graph edges with the same label but heterogeneous end
points. Translation into SQL or RDF is straightforward. Edges with
attributes, e.g. the knows relation between people would end up
represented as a subject with the end points and the date since as
properties. The relational implementation has a two-part primary key and
the date since as a dependent column.  A native property graph database
would use an edge with an extra property for this, as such are typically
supported.&lt;/p&gt;
&lt;p&gt;The only table-level choice has to do with whether &lt;code&gt;posts&lt;/code&gt; and
&lt;code&gt;comments&lt;/code&gt; are kept in the same or different data structures. The Virtuoso
schema has a single table for both, with nullable columns for the
properties that occur only in one. This makes the queries more concise.
There are cases where only non-reply posts of a given author are
accessed. This is supported by having two author foreign key columns
each with its own index. There is a single nullable foreign key from the
reply to the post/comment being replied to.&lt;/p&gt;
&lt;p&gt;The workload has some frequent access paths that need to be supported by
index.  Some queries reward placing extra columns in indices.
For example, a common pattern is accessing the most recent posts of
an author or group of authors.  There, having a composite key
&lt;code&gt;of ps_creatorid&lt;/code&gt;, &lt;code&gt;ps_creationdate&lt;/code&gt;, &lt;code&gt;ps_postid&lt;/code&gt; pays off since the to-k
on &lt;code&gt;creationdate&lt;/code&gt; can be pushed down into the index without needing
a reference to the table.&lt;/p&gt;
&lt;p&gt;The implementation is free to choose data types for
attributes, specifically datetimes. The Virtuoso implementation adopts
the practice of the
&lt;a href=&#34;http://dbpedia.org/resource/DEX_(Graph_database)&#34;&gt;Sparksee&lt;/a&gt; and
&lt;a href=&#34;http://dbpedia.org/resource/Neo4j&#34;&gt;Neo4J&lt;/a&gt; implementations and represents
this is a count of milliseconds since epoch.  This is less confusing,
faster to compare  and more compact than a native datetime datatype that
may or may not have timezones etc. Using a built-in datetime seems to be
nearly always a bad idea. A dimension table or a number for a time
dimension avoids the ambiguities of a calendar or at least makes these
explicit.&lt;/p&gt;
&lt;p&gt;The benchmark allows procedurally maintaining materializations of intermediate results for use by queries as long as these are maintained transaction by transaction.  For example, each person could have the 20 newest posts by immediate contacts precomputed.  This would reduce Q2 &amp;ldquo;top of the wall&amp;rdquo; to a single lookup. This dows not however appear to be worthwhile. The Virtuoso implementation does do one such materialization for Q14: A connection weight is calculated for every pair of persons that know each other. This is related to the count of replies by one or the other to content generated by the other. If there does not exist a single reply in either direction, the weight is taken to be 0. This weight is precomputed after bulk load and subsequently maintained each time a reply is added.  The table for this is the only row-wise structure in the schema and represents a half matrix of connected people, i.e. &lt;code&gt;person1&lt;/code&gt;, &lt;code&gt;person2&lt;/code&gt; -&amp;gt; &lt;code&gt;weight&lt;/code&gt;. &lt;code&gt;Person1&lt;/code&gt; is by convention the one with the smaller &lt;code&gt;p_personid&lt;/code&gt;.  Note that comparing id&amp;rsquo;s in this way is useful but not normally supported by RDF systems. RDF would end up comparing strings of URI&amp;rsquo;s with disastrous performance implications unless an implementation specific trick were used.&lt;/p&gt;
&lt;p&gt;In the next installment we will analyze an actual run.&lt;/p&gt;
&lt;h3 id=&#34;snb-interactive-series&#34;&gt;SNB Interactive Series&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../snb-interactive-part-1-what-is-snb-interactive-really-about&#34;&gt;SNB Interactive, Part 1: What is SNB Interactive Really About?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../snb-interactive-part-2-modeling-choices&#34;&gt;SNB Interactive, Part 2: Modeling Choices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../snb-interactive-part-3-choke-points-and-initial-run-on-virtuoso/&#34;&gt;SNB Interactive, Part 3: Choke Points and Initial Run on Virtuoso&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>LDBC Participates in the 36th Edition of the ACM SIGMOD/PODS Conference</title>
      <link>https://ldbc.github.io/post/ldbc-participates-in-the-36th-edition-of-the-acm-sigmod-pods-conference/</link>
      <pubDate>Mon, 25 May 2015 00:00:00 +0000</pubDate>
      
      <guid>https://ldbc.github.io/post/ldbc-participates-in-the-36th-edition-of-the-acm-sigmod-pods-conference/</guid>
      <description>&lt;p&gt;LDBC is presenting two papers at the next edition of the ACM SIGMOD/PODS
conference held in Melbourne from May 31st to June 4th, 2015. The annual
SCM SIGMOD/PODS conference is a leading international forum for database
researchers, practitioners, developers, and users to explore
cutting-edge ideas and results, and to exchange techniques, tools and
experiences.&lt;/p&gt;
&lt;p&gt;On the industry track, LDBC will be presenting the &lt;em&gt;Social Network
Benchmark Interactive Workload&lt;/em&gt; by Orri Erling (OpenLink Software), Alex
Averbuch (Neo Technology), Josep Larriba-Pey (Sparsity Technologies,
Hassan Chafi (Oracle Labs), Andrey Gubichev (TU Munich), Arnau Prat
(Universitat Politècnica de Catalunya), Minh-Duc Pham (VU University
Amsterdam) and Peter Boncz (CWI).&lt;/p&gt;
&lt;p&gt;You can read more about the &lt;a href=&#34;http://ldbcouncil.org/benchmarks/snb&#34;&gt;Social Network Benchmark here&lt;/a&gt; and collaborate if you&amp;rsquo;re interested!&lt;/p&gt;
&lt;p&gt;The other presentation will be at the GRADES workshop within the SIGMOD
program regarding &lt;em&gt;Graphalytics: A Big Data Benchmark for
Graph-Processing platforms&lt;/em&gt; by Mihai Capotă, Tim Hegeman, Alexandru
Iosup (Delft University of Technology), Arnau Prat
(Universitat Politècnica de Catalunya), Orri Erling (OpenLink Sotware)
and Peter Boncz (CWI). We will provide more information about GRADES and
this specific presentation in a following post as GRADES is part of the
events organized by LDBC.&lt;/p&gt;
&lt;p&gt;Don&amp;rsquo;t forget to check our presentations if you&amp;rsquo;re attending the SIGMOD!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SNB Interactive Part 1: What Is SNB Interactive Really About?</title>
      <link>https://ldbc.github.io/post/snb-interactive-part-1-what-is-snb-interactive-really-about/</link>
      <pubDate>Thu, 14 May 2015 00:00:00 +0000</pubDate>
      
      <guid>https://ldbc.github.io/post/snb-interactive-part-1-what-is-snb-interactive-really-about/</guid>
      <description>&lt;p&gt;This post is the first in a series of blogs analyzing the LDBC Social
Network Benchmark Interactive workload. This is written from the dual
perspective of participating in the benchmark design and of building the
OpenLink Virtuoso implementation of same.&lt;/p&gt;
&lt;p&gt;With two implementations of SNB interactive at four different scales, we
can take a first look at what the benchmark is really about.  The
hallmark of a benchmark implementation is that its
performance characteristics are understood and even if these do not
represent the maximum of the attainable, there are no glaring mistakes
and the implementation represents a reasonable best effort by those who
ought to know, namely the system vendors.&lt;/p&gt;
&lt;p&gt;The essence of a benchmark is a set of trick questions or choke points,
as LDBC calls them.  A number of these were planned from the start.  It
is then the role of experience to tell whether addressing these is
really the key to winning the race.  Unforeseen ones will also surface.&lt;/p&gt;
&lt;p&gt;So far, we see that SNB confronts the implementor with choices in the
following areas:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data model:  Relational, RF, property graph?&lt;/li&gt;
&lt;li&gt;Physical model, e.g. row-wise vs. column wise storage&lt;/li&gt;
&lt;li&gt;Materialized data ordering:  Sorted projections, composite keys,
replicating columns in auxxiliary data structures&lt;/li&gt;
&lt;li&gt;Maintaining precomputed, materialized intermediate results, e.g. use
of materialized views, triggers&lt;/li&gt;
&lt;li&gt;Query optimization:  join order/type, interesting physical data
orderings, late projection, top k, etc.&lt;/li&gt;
&lt;li&gt;Parameters vs. literals:  Sometimes different parameter values  result
in different optimal query plans&lt;/li&gt;
&lt;li&gt;Predictable, uniform latency:  The measurement rules stipulate the the
SUT must not fall behind the simulated workload&lt;/li&gt;
&lt;li&gt;Durability - how to make data durable while maintaining steady
throughput?  Logging vs. checkpointing.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the process of making a benchmark implementation, one
naturally encounters questions about the validity, reasonability and
rationale of the benchmark definition itself.  Additionally, even though
the benchmark might not directly measure certain aspects of a system,
making an implementation will take a system past its usual envelope and
highlight some operational aspects.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Data generation - Generating a mid-size dataset takes time, e.g. 8
hours for 300G.  In a cloud situation, keeping the dataset in S3 or
similar is necessary, re-generating every time is not an option.&lt;/li&gt;
&lt;li&gt;Query mix - Are the relative frequencies of the operations reasonable?
What bias does this introduce?&lt;/li&gt;
&lt;li&gt;Uniformity of parameters:  Due to non-uniform data distributions in
the dataset, there is easily a 100x difference between a &amp;lsquo;fast&amp;rsquo; and
&amp;lsquo;slow&amp;rsquo; case of a single query template.  How long does one need to run
to balance these fluctuations?&lt;/li&gt;
&lt;li&gt;Working set:  Experience shows that there is a large difference
between almost warm and steady state of working set. This can be a
factor of 1.5 in throughput.&lt;/li&gt;
&lt;li&gt;Are the latency constraints reasonable?  In the present case, a
qualifying run must have  under 5% of all query executions starting over
1 second late.  Each execution is scheduled beforehand and done at the
intended time.   If the SUT does not keep up, it will have all available
threads busy and must finish some work before accepting new work, so
some queries will start late.  Is this a good criterion for measuring
consistency of response time?  There are some obvious possibilities of
abuse.&lt;/li&gt;
&lt;li&gt;Is the benchmark easy to implement/run?  Perfection is open-ended and
optimization possibilities infinite, albeit with diminishing returns.
Still, getting startyed should not be too hard.  Since systems will be
highly diverse, testing that these in fact do the same thing is
important.  The SNB validation suite is good for this and given
publicly available reference implementations, the effort of getting
started is not unreasonable.&lt;/li&gt;
&lt;li&gt;Since a Qualifying run must meet latency constraints while going as
fast as possible, setting the performance target involves trial and
error.  Does the tooling make this easy?&lt;/li&gt;
&lt;li&gt;Is the durability rule reasonable?  Right now, one is not required to
do checkpoints but must report the time to roll forward from the last
checkpoint or initial state.  Incenting vendors to build faster recovery
is certainly good, but we are not through with all the implications.&lt;br&gt;
What about redundant clusters?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The following posts will look at the above in light of actual
experience.&lt;/p&gt;
&lt;h3 id=&#34;snb-interactive-series&#34;&gt;SNB Interactive Series&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../snb-interactive-part-1-what-is-snb-interactive-really-about&#34;&gt;SNB Interactive, Part 1: What is SNB Interactive Really About?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../snb-interactive-part-2-modeling-choices&#34;&gt;SNB Interactive, Part 2: Modeling Choices&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../snb-interactive-part-3-choke-points-and-initial-run-on-virtuoso/&#34;&gt;SNB Interactive, Part 3: Choke Points and Initial Run on Virtuoso&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Why Do We Need an LDBC SNB-Specific Workload Driver?</title>
      <link>https://ldbc.github.io/post/why-do-we-need-an-ldbc-snb-specific-workload-driver/</link>
      <pubDate>Tue, 21 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>https://ldbc.github.io/post/why-do-we-need-an-ldbc-snb-specific-workload-driver/</guid>
      <description>&lt;p&gt;TODO fix link below&lt;/p&gt;
&lt;p&gt;In a previous &lt;a href=&#34;http://ldbcouncil.org/blog/tags/driver&#34;&gt;3-part blog series&lt;/a&gt; we touched upon the difficulties of executing the LDBC SNB
Interactive (SNB) workload, while achieving good performance and
scalability. What we didn&amp;rsquo;t discuss is why these difficulties were
unique to SNB, and what aspects of the way we perform workload execution
are scientific contributions - novel solutions to previously unsolved
problems. This post will highlight the differences between SNB and more
traditional database benchmark workloads. Additionally, it will motivate
why we chose to develop a new workload driver as part of this work,
rather than using existing tooling that was developed in other database
benchmarking efforts. To briefly recap, the task of the driver is to run
a transactional database benchmark against large synthetic graph
datasets - &amp;ldquo;graph&amp;rdquo; is the word that best captures the novelty and
difficulty of this work.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Workload Execution - Traditional vs Graph&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Transactional graph workloads differ from traditional relational
workloads in several fundamental ways, one of them being the complex
dependencies that exist between queries of a graph workload.&lt;/p&gt;
&lt;p&gt;To understand what is meant by &amp;ldquo;traditional relational workloads&amp;rdquo;, take
the classical TPC-C benchmark as an example. In TPC-C Remote Terminal
Emulators (emulators) are used to issue update transactions in parallel,
where the transactions issued by these emulators do not depend on one
another. Note, &amp;ldquo;dependency&amp;rdquo; is used here in the context of scheduling,
i.e., one query is dependent on another if it can not start until the
other completes. For example, a New-Order transaction does not depend on
other orders from this or other users. Naturally, the results of
Stock-Level transactions depend on the items that were previously sold,
but in TPC-C it is not an emulator&amp;rsquo;s responsibility to enforce any such
ordering. The scheduling strategy employed by TPC-C is tailored to the
scenario where transactional updates do not depend on one another. In
reality, one would expect to also have scheduling dependencies between
transactions, e.g., checking the status of the order should only be done
after the order is registered in the system.  TPC-C, however, does not
do this and instead only asks for the status of the last order &lt;em&gt;for a
given user&lt;/em&gt;. Furthermore, adding such dependencies to TPC-C would make
scheduling only slightly more elaborate. Indeed, the Load Tester (LT)
would need to make sure a New-Order transaction always precedes the read
requests that check its status, but because users (and their orders) are
partitioned across LTs, and orders belong to a particular user, this
scheduling does not require inter-LT communication.&lt;/p&gt;
&lt;p&gt;A significantly more difficult scheduling problem arises when we
consider the SNB benchmark that models a real-world social network. Its
domain includes users that form a social friendship graph and which
leave posts/comments/likes on each others walls (forums). The update
transactions are generated (exported as a log) by the data generator,
with assigned timestamps, e.g. user 123 added post 456 to forum 789 at
time T. Suppose we partition this workload by user, such that each
driver gets all the updates (friendship requests, posts, comments and
likes on other user&amp;rsquo;s posts etc) initiated by a given user. Now, if the
benchmark is to resemble a real-world social network, the update
operations represent a highly connected (and dependent) network: a user
should not create comments before she joins the network, a friendship
request can not be sent to a non-existent user, a comment can only be
added to a post that already exists, etc. Given a user partitioning
scheme, most such dependencies would cross the boundaries between driver
threads/processes, because the correct execution of update operations
requires that the social network is in a particular state, and that
state depends on the progress of other threads/processes.&lt;/p&gt;
&lt;p&gt;Such scheduling dependencies in the SNB workload essentially replicate
the underlying graph-like shape of its dataset. That is, every time a
user comments on a friend&amp;rsquo;s wall, for example, there is a dependency
between two operations that is captured by an edge of the social
graph. &lt;em&gt;Partitioning the workload among the LTs therefore becomes
equivalent to graph partitioning, a known hard problem.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Because it&amp;rsquo;s a graph&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In short, unlike previous database benchmarking efforts, the SNB
workload has necessitated a redefining of the state-of-the-art in
workload execution. It is no longer sufficient to rely solely on
workload partitioning to safely capture inter-query dependencies in
complex database benchmark workloads. The graph-centric nature of SNB
introduces new challenges, and novel mechanisms had to be developed to
overcome these challenges. To the best of our knowledge, the LDBC SNB
Interactive benchmark is the first benchmark that requires a non-trivial
partitioning of the workload, among the benchmark drivers. In the
context of workload execution, our contribution is therefore the
principled design of a driver that executes dependent update operations
in a performant and scalable way, across parallel/distributed LTs, while
providing repeatable, vendor-independent execution of the benchmark.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Event Driven Post Generation in Datagen</title>
      <link>https://ldbc.github.io/post/event-driven-post-generation-in-datagen/</link>
      <pubDate>Fri, 10 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>https://ldbc.github.io/post/event-driven-post-generation-in-datagen/</guid>
      <description>&lt;p&gt;As discussed in previous posts, one of the features that makes Datagen
more realistic is the fact that the activity volume of the simulated
Persons is not uniform, but forms spikes. In this blog entry I want to
explain more in depth how this is actually implemented inside of the
generator.&lt;/p&gt;
&lt;p&gt;First of all, I start with a few basics of how Datagen works internally.
In Datagen, once the person graph has been created (persons and their
relationships), the activity generation starts. Persons are divided into
blocks of 10k, in the same way they are during friendship edges
generation process. Then, for each person of the block, three types of
forums are created:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The wall of the person&lt;/li&gt;
&lt;li&gt;The albums of the person&lt;/li&gt;
&lt;li&gt;The groups where the person is a moderator&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We will put our attention to group generation, but the same concepts
apply to the other types of forums. Once a group is created, the members
of the group are selected. These are selected from either the friends of
the moderator, or random persons within the same block.&lt;/p&gt;
&lt;p&gt;After assigning the members to the group, the post generation starts. We
have two types of post generators, the uniform post generator and the
event based post generator. Each post generator is responsible of, given
a forum, generate a set of posts for the forum, whose authors are taken
from the set of members of the forum. The uniform post generator
distributes the dates of the generated posts uniformly in the time line
(from the date of the membership until the end of the simulation time).
On the other hand, the event based post generator assigns dates to
posts, based on what we call “flashmob events”.&lt;/p&gt;
&lt;p&gt;Flashmob events are generated at the beginning of the execution. Their
number is predefined by a configuration parameter which is set to 30
events per month of simulation, and the time of the event is distributed
uniformly along all the time line. Also, each event has a volume level
assigned (between 1 and 20) following a power law distribution, which
determines how relevant or important the event is, and a tag
representing the concept or topic of the event. Two different events can
have the same tag. For example, one of the flashmob events created for
SF1 is one related to &amp;ldquo;Enrique Iglesias&amp;rdquo; tag, whose level is 11 and
occurs on 29th of May of 2012 at 09:33:47.&lt;/p&gt;
&lt;p&gt;Once the event based post generation starts for a given group, a subset
of the generated flashmob events is extracted. These events must be
correlated with the tag/topic of the group, and the set of selected
events is restricted by the creation date of the group (in a group one
cannot talk about an event previous to the creation of the group). Given
this subset of events and their volume level, a cumulative probability
distribution (using the events sorted by event date and their level) is
computed, which is later used to determine to which event a given post
is associated. Therefore, those events with a larger lavel will have a
larger probability to receive posts, making their volume larger. Then,
post generation starts, which can be summarized as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Determine the number of posts to generate&lt;/li&gt;
&lt;li&gt;Select a random member of the group that will generate the post&lt;/li&gt;
&lt;li&gt;Determine the event the post will be related to given the
aforementioned cumulative distribution&lt;/li&gt;
&lt;li&gt;Assign the date of the post based on the event date&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In order to assign the date to the post, based on the date of the event
the post is assigned to, we follow the following probability density,
which has been extracted from &lt;a href=&#34;#references&#34;&gt;[1]&lt;/a&gt;. The shape of the probability density
consists of a combination of an exponential function in the 8 hour
interval around the peak, while the volume outside this interval follows
a logarithmic function. The following figure shows the actual shape of
the volume, centered at the date of the event.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;index.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Following the example of &amp;ldquo;Enrique Iglesias&amp;rdquo;, the following figure shows
the activity volume of posts around the event as generated by Datagen.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;index2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this blog entry we have seen how datagen creates event driven user
activity. This allows us to reproduce the heterogenous post creation
density found in a real social network, where post creation is driven by
real world events.&lt;/p&gt;
&lt;h4 id=&#34;references&#34;&gt;References&lt;/h4&gt;
&lt;p&gt;[1] Jure Leskovec, Lars Backstrom, Jon M. Kleinberg: Meme-tracking and
the dynamics of the news cycle. KDD 2009: 497-506&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The LDBC Datagen Community Structure</title>
      <link>https://ldbc.github.io/post/the-ldbc-datagen-community-structure/</link>
      <pubDate>Sun, 15 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>https://ldbc.github.io/post/the-ldbc-datagen-community-structure/</guid>
      <description>&lt;p&gt;This blog entry is about one of the features of DATAGEN that makes it
different from other synthetic graph generators that can be found in the
literature: the community structure of the graph.&lt;/p&gt;
&lt;p&gt;When generating synthetic graphs, one must not only pay attention to
quantitative measures such as the number of nodes and edges, but also to
other more qualitative characteristics such as the degree distribution,
clustering coefficient. Real graphs, and specially social networks, have
typically highly skewed degree distributions with a long tail, a
moderatelly large clustering coefficient and an appreciable community
structure.&lt;/p&gt;
&lt;p&gt;The first two characteristics are deliberately modeled in DATAGEN.
DATAGEN generates persons with a degree distribution that matches that
observed in Facebook, and thanks to the attribute correlated edge
generation process, we obtain graphs with a moderately large clustering
coefficient. But what about the community structure of graphs generated
with DATAGEN? The answer can be found in the paper titled “How
community-like is the structure of synthetically generated graphs”,
which was published in GRADES 2014 &lt;a href=&#34;#references&#34;&gt;[1]&lt;/a&gt;. Here we summarize the paper and
its contributions and findings.&lt;/p&gt;
&lt;p&gt;Existing synthetic graph generators such as Rmat &lt;a href=&#34;#references&#34;&gt;[1]&lt;/a&gt; and Mag &lt;a href=&#34;#references&#34;&gt;[2]&lt;/a&gt;, are
graphs generators designed to produce graphs with long tailed
distributions and large clustering coefficient, but completely ignore
the fact that real graphs are structured into communities. For this
reason, Lancichinetti et al. proposed LFR &lt;a href=&#34;#references&#34;&gt;[3]&lt;/a&gt;, a graph generator that did
not only produced graphs with realistic high level characteristics, but
enforced an appreciable community structure. This generator, has become
the de facto standard for benchmarking community detection algorithms,
as it does not only outputs a graph but also the communities present in
that graph, hence it can be used to test the quality of a community
detection algorithm.&lt;/p&gt;
&lt;p&gt;However, no one studied if the community structure produced by LFR, was
in fact realistic compared to real graphs. Even though the community
structure in in LFR exhibit interesting properties, such as the expected
larger internal density than external, or a longtailed distribution of
community sizes, they lack the noise and inhomogeneities present in a
real graph. And more importantly, how does the community structure of
DATAGEN compares to that exhibited in LFR and reap graphs? Is it more or
less realistic? +
The authors of &lt;a href=&#34;#references&#34;&gt;[1]&lt;/a&gt; set up an experiment where they analized the
characteristics of the communities output by  LFR, and the groups
(groups of people interested in a given topic) output by DATAGEN, and
compared them to a set of real graphs with metadata. These real graphs,
which can be downloaded from the Snap project website, are graphs that
have recently become very popular in the field of community detection,
as they contain ground truth communities extracted from their metadata.
The ground truth graphs used in this experiment are shown in the
following table. For more details about how this ground truth is
generated, please refer to &lt;a href=&#34;#references&#34;&gt;[4]&lt;/a&gt;.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;&lt;em&gt;Nodes&lt;/em&gt;&lt;/th&gt;
&lt;th&gt;&lt;em&gt;Edges&lt;/em&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;Amazon&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;334863&lt;/td&gt;
&lt;td&gt;925872&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;Dblp&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;317080&lt;/td&gt;
&lt;td&gt;1049866&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;Youtube&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;1134890&lt;/td&gt;
&lt;td&gt;2987624&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;em&gt;Livejournal&lt;/em&gt;&lt;/td&gt;
&lt;td&gt;3997962&lt;/td&gt;
&lt;td&gt;34681189&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The authors of &lt;a href=&#34;#references&#34;&gt;[1]&lt;/a&gt; selected  a set of statistical indicators to
characterize the communities:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The clustering coefficient&lt;/li&gt;
&lt;li&gt;The triangle participation ration (TPR), which is the ratio of nodes
that close at least one triangle in the community.&lt;/li&gt;
&lt;li&gt;The bridge ratio, which is the ratio of edges whose removal
disconnects the community.&lt;/li&gt;
&lt;li&gt;The diameter&lt;/li&gt;
&lt;li&gt;The conductance&lt;/li&gt;
&lt;li&gt;The size&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The authors start by analyzing each community of the  ground truth
graphs using the above statistical indicators and ploting the
distributions of each of them. The following are the plots of the
Livejournal graph. We summarize the findings of the authors regarding
real graphs: +
Several indicators (Clustering Coefficient, TPR and Bridge ratio)
exihibit a multimodal distribution, with two peaks aht their extremes.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Many of the communities (44%) have a small clustering coefficient
between 0 and 0.01. Out of them, 56% have just three vertices. On the
other hand, 11% of the communities have a clustering coefficient between
0.99 and 1.0. In between, communities exhibit different values of
clustering coefficients. This trend is also observed for TPR and
Bridgeratio. This suggests that communities cannot be modeled using a
single model.&lt;/li&gt;
&lt;li&gt;84% of the communities have a diameter smaller than five, suggesting
that ground truth communities are small and compact&lt;/li&gt;
&lt;li&gt;Ground truth communities are not very isolated, they have a lot of
connections pointing outside of the community.&lt;/li&gt;
&lt;li&gt;Most of the communities are small (10 or less nodes).&lt;/li&gt;
&lt;li&gt;In general, ground truth communities are, small with a low diameter,
not isolated and with different ranges of internal connectivity.&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;index.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;index2.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Clustering Coefficient&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;TPR&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;index3.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;index4.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Bridge Ratio&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Diameter&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;index5.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;index6.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Conductance&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Size&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The authors performed the same experiment but for DATAGEN and LFR
graphs. They generated a graph of 150k nodes, using their default
parameters. In the case of LFR, they tested five different values of the
mixing factor, which specifies the ratio of edges of the community
pointing outside of the community, They ranged this value from 0 to 0.5.
The following are the distributions for DATAGEN.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;index8.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;index9.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Clustering Coefficient&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;TPR&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;index10.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;index11.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Bridge Ratio&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;TPRDiameter&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;index11.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;index12.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Conductance&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Size&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The main conclusions that can be extracted from DATAGEN can be
summarized asfollows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DATAGEN is able to reproduce the multimodal distribution observed for
clustering coefficient, TPR and bridge ratio.&lt;/li&gt;
&lt;li&gt;The central part of the clustering coefficient is biased towards the
left, in a similar way as observed for the youtube and livejournal
graphs.&lt;/li&gt;
&lt;li&gt;Communities of DATAGEN graphs are not, as in real graphs, isolated,
but in this case their level of isolation if significantly larger.&lt;/li&gt;
&lt;li&gt;The diameter is small like in the real graphs.&lt;/li&gt;
&lt;li&gt;It is significant that communities in DATAGEN graphs are closer to
those observed in Youtube and Livejournal, as these are social networks
like the graphs produced by DATAGEN. We see that DATAGEN is able to
reproduce many of their characteristics.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Finally, the authors repeat the same experiment for LFR graphs. The
following are the plots for the LFR graph with mixing ratio 0.3. From
them, the authors extract the following conclusions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LFR graphs donot show the multimodal distribution observed in real
graphs&lt;/li&gt;
&lt;li&gt;Only the diameter shows a similar shape as in the ground truth.&lt;/li&gt;
&lt;/ul&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;index13.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;index14.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Clustering Coefficient&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;TPR&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;index15.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;index16.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Bridge Ratio&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;TPRDiameter&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;index17.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;index18.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Conductance&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Size&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;To better quanify how similar are the distribuions between the different
graphs,  the authors also show the correlograms for each of the
statisticsl indicators. These correlograms, contain the Spearman&amp;rsquo;s
correlation coefficient between each pair of graphs for a given
statistical indicator. The more blue the color, the better the
correlation is. We see that DATAGEN distributions correlate very well
with those observed in real graphs, specially as we commented above,
with Youtube and Livejournal. On the other hand, LFR only succeds
significantly in the case of the Diameter.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;index19.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;index20.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Clustering Coefficient&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;TPR&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;index21.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;index22.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Bridge Ratio&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;TPRDiameter&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;index23.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;img src=&#34;index24.png&#34; alt=&#34;&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Conductance&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Size&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We see that DATAGEN is able to reproduce a realistics community
structure, compared to existing graph generators. This feature, could be
potentially exploited to define new benchmakrs to measure the quality of
novel community detection algorithms. Stay tuned for future blog posts
about his topic!&lt;/p&gt;
&lt;h4 id=&#34;references&#34;&gt;References&lt;/h4&gt;
&lt;p&gt;[1] Arnau Prat-Pérez,
&lt;a href=&#34;http://dblp.uni-trier.de/pers/hd/d/Dom=iacute=nguez=Sal:David&#34;&gt;David Domínguez-Sal&lt;/a&gt;: How community-like is the structure of synthetically
generated graphs?  &lt;a href=&#34;http://dblp.uni-trier.de/db/conf/sigmod/grades2014.html#PratD14&#34;&gt;GRADES 2014&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2] Deepayan Chakrabarti, Yiping Zhan, and ChristosFaloutsos. R-mat: A
recursive model for graph mining. SIAM 2014&lt;/p&gt;
&lt;p&gt;[3] Myunghwan Kim and Jure Leskovec. Multiplicative attribute graph
model of real-world networks. Internet Mathematics&lt;/p&gt;
&lt;p&gt;[4] Andrea Lancichinetti, Santo Fortunato, and Filippo Radicchi.
Benchmark graphs for testing community detection algorithms. Physical
Review E 2008.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Industry Relevance of the Semantic Publishing Benchmark</title>
      <link>https://ldbc.github.io/post/industry-relevance-of-the-semantic-publishing-benchmark/</link>
      <pubDate>Tue, 03 Mar 2015 00:00:00 +0000</pubDate>
      
      <guid>https://ldbc.github.io/post/industry-relevance-of-the-semantic-publishing-benchmark/</guid>
      <description>&lt;h3 id=&#34;publishing-and-media-businesses-are-going-through-transformation&#34;&gt;Publishing and media businesses are going through transformation&lt;/h3&gt;
&lt;p&gt;I took this picture in June 2010 next to Union Square in San Francisco.
I was smoking and wrestling my jetlag in front of Hilton. In the lobby
inside the SemTech 2010 conference attendants were watching a game from
the FIFA World Cup in South Africa. In the picture, the self-service
newspaper stand is empty, except for one free paper. It was not long
ago, in the year 2000, this stand was full.  Back than the people in the
Bay area were willing to pay for printed newspapers. But this is no
longer true.&lt;/p&gt;
&lt;p&gt;What’s driving this change in publishing and media?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Widespread and instantaneous distribution of information over the
Internet has turned news into somewhat of a &amp;ldquo;commodity&amp;rdquo; and few people
are willing to pay for it&lt;/li&gt;
&lt;li&gt;The wealth of free content on YouTube and similar services spoiled the
comfort of many  mainstream broadcasters;&lt;/li&gt;
&lt;li&gt;Open access publishing has limited academic publishers to sell
journals and books at prices that were considered fair ten years ago.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;Alongside other changes in the industry, publishers figured out that it
is critical to add value through better authoring, promotion,
discoverability, delivery and presentation of precious content.&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;imagine-instant-news-in-context-imagine-personal-channels-imagine--triplestores&#34;&gt;Imagine instant news in context, Imagine personal channels, Imagine &amp;hellip; triplestores&lt;/h3&gt;
&lt;p&gt;While plain news can be created repeatedly, premium content and services
are not as easy to create. Think of an article that not only tells the
new facts, but refers back to previous events and is complemented by an
info-box of relevant facts. It allows one to interpret and comprehend
news more effectively. This is the well-known journalistic aim to put
news in context. It is also well-known that producing such news in &amp;ldquo;near
real time&amp;rdquo; is difficult and expensive using legacy processes and content
management technology.&lt;/p&gt;
&lt;p&gt;Another example would be a news feed that delivers good coverage of
information relevant to a narrow subject – for example a company, a
story line or a region. Judging by the demand for intelligent press
clipping services like
&lt;a href=&#34;http://new.dowjones.com/products/factiva/&#34;&gt;Factiva&lt;/a&gt;, such channels are in
demand but are not straightforward to produce with today’s technology.
Despite the common perception that automated recommendations for related
content and personalized news are technology no-brainers, suggesting
truly relevant content is far from trivial.&lt;/p&gt;
&lt;p&gt;Finally, if we use an example in life sciences, the ability to quickly
find scientific articles discussing asthma and x-rays, while searching
for respiration disorders and radiation, requires a search service that
is not easy to deliver.&lt;/p&gt;
&lt;p&gt;Many publishers have been pressed to advance their business. This, in
turn, had led to quest to innovate. And semantic technology can help
publishers in two fundamental ways:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Generation of rich and &amp;ldquo;meaningful&amp;rdquo; (trying not to use &amp;ldquo;semantic&amp;rdquo; :-)
metadata descriptions;&lt;/li&gt;
&lt;li&gt;Dynamic retrieval of content, based on this rich metadata, enabling
better delivery.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this post I write about &amp;ldquo;semantic annotation&amp;rdquo; and how it enables
application scenarios like BBC’s Dynamic Semantic Publishing (DSP). I
will also present the business case behind DSP. The final part of the
post is about triplestores – semantic graph database engines, used in
DSP. To be more concrete I write about the Semantic Publishing Benchmark
(SPB), which evaluates the performance of triplestores in DSP scenarios.&lt;/p&gt;
&lt;h3 id=&#34;semantic-annotation-produces-rich-metadata-descriptions--the-fuel-for-semantic-publishing&#34;&gt;Semantic Annotation produces Rich Metadata Descriptions – the fuel for semantic publishing&lt;/h3&gt;
&lt;p&gt;The most popular meaning of &amp;ldquo;semantic annotation&amp;rdquo; is the process of
enrichment of text with links to (descriptions of) concepts and entities
mentioned in the text. This usually means tagging either the entire
document or specific parts of it with identifiers of entities. These
identifiers allow one to retrieve descriptions of the entities and
relations to other entities – additional structured information that
fuels better search and presentation.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;02_semantic_repository.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The concept of using
&lt;a href=&#34;http://infosys3.elfak.ni.ac.rs/nastava/attach/SemantickiWebKurs/sdarticle.pdf&#34;&gt;text-mining for automatic semantic annotation&lt;/a&gt; of text with respect to very large
datasets, such as &lt;a href=&#34;http://dbpedia.org/&#34;&gt;DBPedia&lt;/a&gt;, emerged in early 2000.
In practical terms it means using such large datasets as a sort of
gigantic gazetteer (name lookup tool) and the ability to disambiguate.
Figuring out whether &amp;ldquo;Paris&amp;rdquo; in the text refers to the capital of France
or to Paris, Texas, or to Paris Hilton is crucial in such context.
Sometimes this is  massively difficult – try to instruct a computer how
to guess whether &amp;ldquo;Hilton&amp;rdquo; in the second sentence of this post refers to
a hotel from the chain founded by her grandfather or that I had the
chance to meet Paris Hilton in person on the street in San Francisco.&lt;/p&gt;
&lt;p&gt;Today there are plenty of tools (such as the
&lt;a href=&#34;http://www.ontotext.com/semantic-solutions/media-publishing/&#34;&gt;Ontotext Media and Publishing&lt;/a&gt; platform and
&lt;a href=&#34;https://github.com/dbpedia-spotlight/dbpedia-spotlight/wiki&#34;&gt;DBPedia Spotlight&lt;/a&gt;) and services (such as Thomson Reuter’s
&lt;a href=&#34;http://www.opencalais.com/&#34;&gt;OpenCalais&lt;/a&gt; and Ontotext’s
&lt;a href=&#34;http://s4.ontotext.com&#34;&gt;S4&lt;/a&gt;) that offer automatic semantic annotation.
Although text-mining cannot deliver 100% correct annotations, there are
plenty of scenarios, where technology like this would revoluntionize a
business. This is the case with the Dynamic Semantic Publishing scenario
described below.&lt;/p&gt;
&lt;h3 id=&#34;the-bbcs-dynamic-semantic-publishing-dsp&#34;&gt;The BBC’s Dynamic Semantic Publishing (DSP)&lt;/h3&gt;
&lt;p&gt;Dynamic Semantic Publishing is a model for using semantic technology in
media developed by a group led by John O’Donovan and Jem Rayfield at the
BBC. The implementation of DSP behind BBC’s FIFA World Cup 2010 website
was the first high-profile success story for usage of semantic
technology in media. It is also the basis for the SPB benchmark –
sufficient reasons to introduce this use case at length below.&lt;/p&gt;
&lt;p&gt;BBC Future Media &amp;amp; Technology department have transformed the BBC
relational content management model and static publishing framework to a
fully dynamic semantic publishing architecture. With minimal
journalistic management, media assets are being enriched with links to
concepts, semantically described in a triplestore. This novel semantic
approach provides improved navigation, content re-use and re-purposing
through automatic aggregation and rendering of links to relevant
stories. At the end of the day DSP improves the user experience on BBC’s
web site.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&amp;ldquo;A high-performance dynamic semantic publishing framework facilitates the publication of automated metadata-driven web pages that are light-touch, requiring minimal journalistic management, as they automatically aggregate and render links to relevant stories&amp;rdquo;.&lt;/em&gt; &amp;ndash; &lt;a href=&#34;http://www.bbc.co.uk/blogs/bbcinternet/2010/07/bbc_world_cup_2010_dynamic_sem.html&#34;&gt;Jem Rayfield, Senior Technical Architect&lt;/a&gt;, BBC News and Knowledge&lt;/p&gt;
&lt;p&gt;The Dynamic Semantic Publishing (DSP) architecture of the BBC curates
and publishes content (e.g. articles or images) based on embedded Linked
Data identifiers, ontologies and associated inference. It allows for
journalists to determine levels of automation (&amp;ldquo;edited by exception&amp;rdquo;)
and support semantic advertisement placement for audiences outside of
the UK. The following quote explains the workflow when a new article
gets into BBC’s content management system.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&amp;ldquo;In addition to the manual selective tagging process, journalist-authored content is automatically analysed against the World Cup ontology.  A &lt;a href=&#34;http://www.bbc.co.uk/blogs/legacy/bbcinternet/2010/07/bbc_world_cup_2010_dynamic_sem.html#language&#34;&gt;natural language and ontological determiner process&lt;/a&gt; automatically extracts World Cup concepts embedded within a textual representation of a story.  The concepts are moderated and, again, selectively applied before publication. Moderated, automated concept analysis improves the depth, breadth and quality of metadata publishing.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;03_bbc_sport.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Journalist-published metadata is captured and made persistent for querying using the resource description framework (&lt;a href=&#34;http://www.bbc.co.uk/blogs/legacy/bbcinternet/2010/07/bbc_world_cup_2010_dynamic_sem.html#RDF&#34;&gt;&lt;em&gt;RDF&lt;/em&gt;&lt;/a&gt;) metadata representation and triple store technology. &lt;a href=&#34;http://www.bbc.co.uk/blogs/legacy/bbcinternet/2010/07/bbc_world_cup_2010_dynamic_sem.html#BigOWLIM&#34;&gt;A RDF triplestore&lt;/a&gt; and &lt;a href=&#34;http://www.bbc.co.uk/blogs/legacy/bbcinternet/2010/07/bbc_world_cup_2010_dynamic_sem.html#SPARQL&#34;&gt;SPARQL&lt;/a&gt; approach was chosen over and above traditional relational database technologies due to the requirements for interpretation of metadata with respect to an ontological domain model. The high level goal is that the domain ontology allows for intelligent mapping of journalist assets to concepts and queries. The chosen triplestore provides reasoning following the forward-chaining model and thus implied inferred statements are automatically derived from the explicitly applied journalist metadata concepts. For example, if a journalist selects and applies the single concept &amp;ldquo;Frank Lampard&amp;rdquo;, then the framework infers and applies concepts such as &amp;ldquo;England Squad&amp;rdquo;, &amp;ldquo;Group C&amp;rdquo; and &amp;ldquo;FIFA World Cup 2010&amp;rdquo; &amp;hellip;&amp;quot;&lt;/em&gt; &amp;ndash; Jem Rayfield&lt;/p&gt;
&lt;p&gt;One can consider each of the &amp;ldquo;aggregation pages&amp;rdquo; of BBC as a sort of
feed or channel serving content related to a specific topic. If you take
this perspective, with its World Cup 2010 website BBC was able to
provide more than 700 thematic channels.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&amp;ldquo;The World Cup site is a large site with over 700 aggregation pages
(called index pages) designed to lead you on to the thousands of story
pages and content&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;…&lt;/strong&gt;&lt;strong&gt;&lt;em&gt;we are not publishing pages, but publishing content&lt;/em&gt;&lt;/strong&gt; &lt;em&gt;as
assets which are then organized by the metadata dynamically into pages,
but could be re-organized into any format we want much more easily than
we could before.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;04_content_tagging.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;… The index pages are published automatically. This process is what assures us of the highest quality output, but still &lt;strong&gt;save large amounts of time&lt;/strong&gt; in managing the site and &lt;strong&gt;makes it possible for us to efficiently run so many pages&lt;/strong&gt; for the World Cup.&amp;quot;&lt;/em&gt; &amp;ndash; &lt;a href=&#34;http://www.bbc.co.uk/blogs/bbcinternet/2010/07/the_world_cup_and_a_call_to_ac.html&#34;&gt;John O&amp;rsquo;Donovan, Chief Technical Architect, BBC Future Media &amp;amp; Technology&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;To get a real feeling about the load of the triplestore behind BBC&amp;rsquo;s
World Cup web site, here are some statistics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;800+ aggregation pages (Player, Team, Group, etc.), generated through
SPARQL queries;&lt;/li&gt;
&lt;li&gt;Average unique page requests/day: 2 million;&lt;/li&gt;
&lt;li&gt;Average &lt;strong&gt;SPARQL queries/day: 1 million;&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;100s repository updates/inserts per minute&lt;/strong&gt; with OWL 2 RL reasoning;&lt;/li&gt;
&lt;li&gt;Multi data center that is fully resilient, clustered 6 node
triplestore.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;the-semantic-publishing-benchmark&#34;&gt;The Semantic Publishing Benchmark&lt;/h3&gt;
&lt;p&gt;LDBC&amp;rsquo;s &lt;a href=&#34;https://ldbc.github.io/developer/spb&#34;&gt;Semantic Publishing Benchmark&lt;/a&gt; (SPB) measures the performance of an RDF database in a load
typical for metadata-based content publishing, such as the BBC Dynamic
Semantic Publishing scenario. Such load combines tens of updates per
second (e.g. adding metadata about new articles) with even higher
volumes of read requests (SPARQL queries collecting recent content and
data to generate web pages on a specific subject, e.g. Frank Lampard).&lt;/p&gt;
&lt;p&gt;SPB simulates a setup for media that deals with large volumes of
streaming content, e.g. articles, pictures, videos. This content is
being enriched with metadata that describes it through links to
reference knowledge:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Reference knowledge:&lt;/em&gt; taxonomies and databases that include relevant
concepts, entities and factual information (e.g. sport statistics);&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Metadata&lt;/em&gt; for each individual piece of content allows publishers to
efficiently produce live streams of content relevant to specific
subjects.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In this scenario the triplestore holds both reference knowledge and
metadata. The main interactions with the repository are of two types:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Aggregation queries&lt;/em&gt; retrieve content according to various criteria.
There are two sets (mixes) of aggregation queries. The basic one
includes interactive queries that involve retrieval of concrete pieces
of content, as well as aggregation functions, geo-spatial and full-text
search constraints. The analytical query mix includes analytical
queries, faceted search and drill-down queries;&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Updates&lt;/em&gt;, adding new metadata or updating the reference knowledge. It
is important that such updates should immediately impact the results of
the aggregation queries. Imagine a fan checking the page for Frank
Lampard right after he scored a goal – she will be very disappointed to
see out of date statistics there.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;SPB v.1.0 directly reproduces the DSP setup at the BBC. The reference
dataset consists of BBC Ontologies (Core, Sport, News), BBC datasets
(list of F1 teams, MPs, etc.) and an excerpt from
&lt;a href=&#34;http://www.geonames.org/&#34;&gt;Geonames&lt;/a&gt; for the UK. The benchmark is packed
with metadata generator that allows one to set up experiments at
different scales. The metadata generator produces 19 statements per
Creative Work (BBC’s slang for all sorts of media assets). The standard
scale factor is 50 million statements.&lt;/p&gt;
&lt;p&gt;TODO links below&lt;/p&gt;
&lt;p&gt;A more technical introduction to SPB can be found in this
&lt;a href=&#34;http://ldbcouncil.org/blog/getting-started-semantic-publishing-benchmark&#34;&gt;post&lt;/a&gt;.
Results from experiments with SPB on different hardware configurations,
including AWS instances, are available in this
&lt;a href=&#34;http://ldbcouncil.org/blog/sizing-aws-instances-semantic-publishing-benchmark&#34;&gt;post&lt;/a&gt;.
An interesting discovery is that given the current state of the
technology (particularly the GraphDB v.6.1 engine) and  today’s cloud
infrastructure, the load of BBC’s World Cup 2010 website can be handled
at AWS by a cluster that costs only $81/day.&lt;/p&gt;
&lt;p&gt;Despite the fact that SPB v.1.0 follows closely the usage scenario for
triplestores in BBC’s DSP incarnations, it is relevant to a wide range
of media and publishing scenarios, where large volumes of &amp;ldquo;fast flowing&amp;rdquo;
content need to be &amp;ldquo;dispatched&amp;rdquo; to serve various information needs of a
huge number of consumers. The main challenges can be summarized as
follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The Triplestore is used as operational database serving a massive
number of read queries (hundreds of queries per second) in parallel with
tens of update transactions per second. Transactions need to be handled
instantly and in a reliable and consistent manner;&lt;/li&gt;
&lt;li&gt;Reasoning is needed to map content descriptions to queries in a
flexible manner;&lt;/li&gt;
&lt;li&gt;There are specific requirements, such as efficient handling of
full-text search, geo-spatial and temporal constraints.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;spb-v20--steeper-for-the-engines-closer-to-the-publishers&#34;&gt;SPB v.2.0 – steeper for the engines, closer to the publishers&lt;/h3&gt;
&lt;p&gt;We are in the final testing of the new version 2.0 of SPB. The benchmark
has evolved to allow for retrieval of semantically relevant content in a
more advanced manner and at the same time to demonstrate how
triplestores can offer simplified and more efficient querying.&lt;/p&gt;
&lt;p&gt;The major changes in SPB v.2.0 can be summarized as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Much bigger reference dataset: from 170 thousand to 22 million
statements. Now it includes GeoNames data about all of Europe (around 7
million statements) and DBPedia data about companies, people and events
(14 million statements). This way we can simulate media archives
described against datasets with good global coverage for specific types
of objects. Such large reference sets also provide a better testing
ground for experiments with very large content archives – think of 50
million documents (1 billion statements) or more;&lt;/li&gt;
&lt;li&gt;Better interconnected reference data: more than 5 million links
between entities, including 500,000 owl:sameAs links between DBPedia and
Geonames descriptions. The latter evaluates the capabilities of the
engine to deal with data coming from multiple sources, which use
different identifiers for one and the same entity;&lt;/li&gt;
&lt;li&gt;Retrieval of relevant content through links in the reference data,
including inferred ones. To this end it is important than SPB v.2.0
involves much more comprehensive inference, particularly with respect to
transitive closure of parent-company and geographic nesting chains.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>OWL-Empowered SPARQL Query Optimization</title>
      <link>https://ldbc.github.io/post/owl-empowered-sparql-query-optimization/</link>
      <pubDate>Wed, 18 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>https://ldbc.github.io/post/owl-empowered-sparql-query-optimization/</guid>
      <description>&lt;p&gt;The Linked Data paradigm has become the prominent enabler for sharing
huge volumes of data using Semantic Web technologies, and has created
novel challenges for non-relational data management systems, such as RDF
and graph engines. Efficient data access through queries is perhaps the
most important data management task, and is enabled through query
optimization techniques, which amount to the discovery of optimal or
close to optimal execution plans for a given query.&lt;/p&gt;
&lt;p&gt;In this post, we propose a different approach to query optimization,
which is meant to complement (rather than replace) the standard
optimization methodologies for SPARQL queries. Our approach is based on
the use of schema information, encoded using OWL constructs, which often
accompany Linked Data.&lt;/p&gt;
&lt;p&gt;OWL adopts the Open World Assumption and hence OWL axioms are perceived
primarily to infer new knowledge. Nevertheless, ontology designers
consider OWL as an expressive schema language used to express
constraints for validating the datasets, hence following the Closed
World Assumption when interpreting OWL ontologies. Such constraints
include disjointness/equivalence of classes/properties, cardinality
constraints, domain and range restrictions for properties and others.&lt;/p&gt;
&lt;p&gt;This richness of information carried over by OWL axioms can be the basis
for the development of schema-aware techniques that will allow
significant improvements in the performance of existing RDF query
engines when used in tandem with data statistics or even other
heuristics based on patterns found in SPARQL queries. As a simple
example, a cardinality constraint at the schema level can provide a hint
on the proper join ordering, even if data statistics are missing or
incomplete.&lt;/p&gt;
&lt;p&gt;The aim of this post is to show that the richness of information carried
over by OWL axioms under the Close World Assumption can be the basis for
the development of schema-aware optimization techniques that will allow
considerable improvement for query processing. To attain this objective,
we discuss a small set of interesting cases of OWL axioms; a full list
can be found
&lt;a href=&#34;LDBC_D4.4.2_final.pdf&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;schema-based-optimization-techniques&#34;&gt;Schema-Based Optimization Techniques&lt;/h3&gt;
&lt;p&gt;Here we provide some examples of queries, which, when combined with
specific schema constraints expressed in OWL, can help the optimizer in
formulating the (near to) optimal query plans.&lt;/p&gt;
&lt;p&gt;A simple first case is the case of constraint violation. Consider the
query below, which returns all instances of class &lt;code&gt;&amp;lt;A&amp;gt;&lt;/code&gt; which are fillers
of a specific property &lt;code&gt;&amp;lt;P&amp;gt;&lt;/code&gt;. If the underlying schema contains the
information that the range of &lt;code&gt;&amp;lt;P&amp;gt;&lt;/code&gt; is class &lt;code&gt;&amp;lt;B&amp;gt;&lt;/code&gt;, and that class &lt;code&gt;&amp;lt;B&amp;gt;&lt;/code&gt; is
disjoint from class &lt;code&gt;&amp;lt;A&amp;gt;&lt;/code&gt;, then this query should return the empty result,
with no further evaluation (assuming that the constraints associated
with the schema are satisfied by the data). An optimizer that takes into
account schema information should return an empty result in constant
time instead of trying to optimize or evaluate the large star join.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SELECT ?v 
WHERE { ?v rdf : type &amp;lt;A&amp;gt; .   
        ?u &amp;lt;P&amp;gt; ?v . ?u &amp;lt;P&amp;gt; ?v1 .   
        ?u &amp;lt;P1 &amp;gt; ?v2 . ?u &amp;lt;P2 &amp;gt; ?v3 .   
        ?u &amp;lt;P3 &amp;gt; ?v4 . ?u &amp;lt;P4 &amp;gt; ?v5}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Schema-aware optimizers could also prune the search space by eliminating
results that are known a priori not to be in the answer set of a query.
The query above is an extreme such example (where all potential results
are pruned), but other cases are possible, such as the case of the query
below, where all subclasses of class &lt;code&gt;&amp;lt;A1&amp;gt;&lt;/code&gt; can immediately be identified
as not being in the answer set.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SELECT ?c
WHERE { ?x rdf: type ?c . ?x &amp;lt;P&amp;gt; ?y . 
        FILTER NOT EXISTS \{ ?x rdf: type &amp;lt;A1 &amp;gt; }}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Another category of schema-empowered optimizations has to do with
improved selectivity estimation. In this respect, knowledge about the
cardinality (minimum cardinality, maximum cardinality, exact
cardinality, functionality) of a property can be exploited to formulate
better query plans, even if data statistics are incomplete, missing or
erroneous.&lt;/p&gt;
&lt;p&gt;Similarly, taking into account class hierarchies, or the definition of
classes/properties via set theoretic constructs (union, intersection) at
the schema level, can provide valuable information on the selectivity of
certain triple patterns, thus facilitating the process of query
optimization. Similar effects can be achieved using information about
properties (functionality, transitivity, symmetry etc).&lt;/p&gt;
&lt;p&gt;As an example of these patterns, consider the query below, where class
&lt;code&gt;&amp;lt;C&amp;gt;&lt;/code&gt; is defined as the intersection of classes &lt;code&gt;&amp;lt;C1&amp;gt;&lt;/code&gt;,&lt;code&gt; &amp;lt;C2&amp;gt;&lt;/code&gt;. Thus, the
triple pattern &lt;code&gt;(?x rdf:type &amp;lt;C&amp;gt;)&lt;/code&gt; is more selective than &lt;code&gt;(?y rdf:type &amp;lt;C1&amp;gt;)&lt;/code&gt; and &lt;code&gt;(?z rdf:type &amp;lt;C2&amp;gt;)&lt;/code&gt; and this should be immediately recognizable
by the optimizer, without having to resort to cost estimations. This
example shows also how unnecessary triple patterns can be pruned from a
query to reduce the number of necessary joins. Figure 1 illustrates the
query plan obtained when the OWL intersectionOf construct is used.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SELECT ?x 
WHERE { ?x rdf: type &amp;lt;C&amp;gt; . ?x &amp;lt;P1 &amp;gt; ?y . 
        ?y rdf : type &amp;lt;C1 &amp;gt; . ?y &amp;lt;P2 &amp;gt; ?z . ?z rdf : type &amp;lt;C2 &amp;gt; }
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;owl_constraints.png&#34; alt=&#34;image&#34;&gt;&lt;/p&gt;
&lt;p&gt;Schema information can also be used by the query optimizer to rewrite
SPARQL queries to equivalent ones that are found in a form for which
already known optimization techniques are easily applicable. For
example, the query below could easily be transformed into a classical
star-join query if we know (from the schema) that property &lt;code&gt;P4&lt;/code&gt; is a
symmetric property.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SELECT ?y ?y1 ?y2 ?y3 
WHERE { ?x &amp;lt;P1 &amp;gt; ?y . ?x &amp;lt;P2 &amp;gt; ?y1 . 
        ?x &amp;lt;P3 &amp;gt; ?y2 . ?y3 &amp;lt;P4 &amp;gt; ?x }
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;In this post we argued that OWL-empowered optimization techniques can be
beneficial for SPARQL query optimization when used in tandem with
standard heuristics based on statistics. We provided some examples which
showed the power of such optimizations in various cases, namely:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cases where the search space can be pruned due to the schema and the
associated constraints; an extreme special sub-case is the
identification of queries that violate schema constraints and thus
produce no results.&lt;/li&gt;
&lt;li&gt;Cases where the schema can help in the estimation of triple pattern
selectivity, even if statistics are incomplete or missing.&lt;/li&gt;
&lt;li&gt;Cases where the schema can identify redundant triple patterns that do
not affect the result and can be safely eliminated from the query.&lt;/li&gt;
&lt;li&gt;Cases where the schema can be used for rewriting a query in an
equivalent form that would facilitate optimization using well-known
optimization techniques.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This list is by no means complete, as further cases can be identified by
optimizers. Our aim in this post was not to provide a complete listing,
but to demonstrate the potential of the idea in various directions.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Third TUC Meeting</title>
      <link>https://ldbc.github.io/event/third-tuc-meeting/</link>
      <pubDate>Tue, 19 Nov 2013 08:00:00 +0000</pubDate>
      
      <guid>https://ldbc.github.io/event/third-tuc-meeting/</guid>
      <description>&lt;p&gt;The LDBC consortium is pleased to announce the third Technical User Community (TUC) meeting!&lt;/p&gt;
&lt;p&gt;This will be a one day event in London on the &lt;strong&gt;19 November
2013&lt;/strong&gt; running in collaboration with the
&lt;a href=&#34;http://www.graphconnect.com/london/&#34;&gt;GraphConnect&lt;/a&gt;
event (18/19 November). Registered TUC
participants that would like a free pass to all of GraphConnect should
register for GraphConnect using this following coupon
code: &lt;strong&gt;LDBCTUC&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The TUC event will include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Introduction to the objectives and progress of the LDBC project&lt;/li&gt;
&lt;li&gt;Description of the progress of the benchmarks being evolved through
Task Forces&lt;/li&gt;
&lt;li&gt;Users explaining their use-cases and describing the limitations they
have found in current technology&lt;/li&gt;
&lt;li&gt;Industry discussions on the contents of the benchmarks&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We will also be launching the LDBC non-profit organization, so anyone
outside the EU project will be able to
join as a member.&lt;/p&gt;
&lt;p&gt;We will kick off  new benchmark development task forces in the coming
year, and talks at this coming TUC will play an important role in
deciding the use case scenarios that will drive those
benchmarks.&lt;/p&gt;
&lt;p&gt;All users of RDF and graph databases are welcome to attend. If you are
interested, please contact: ldbc AT ac DOT upc DOT edu&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#agenda&#34;&gt;Agenda&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#logistics&#34;&gt;Logistics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#ldbc/tucbackground&#34;&gt;LDBC/TUC Background&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#socialnetworkbenchmark&#34;&gt;Social Network Benchmark&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#semanticpublishingbenchmark&#34;&gt;Semantic Publishing Benchmark &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;agenda&#34;&gt;Agenda&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;November 19th - Public TUC Meeting&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;8:00
Breakfast and registration will open
for Graph Connect/TUC at 8:00 am (Dexter
House)&lt;/p&gt;
&lt;p&gt;short LDBC presentation (Peter Boncz) during GraphConnect keynote by
Emil Eifrem (09:00-09:30 Dexter House)&lt;/p&gt;
&lt;p&gt;NOTE: the TUC meeting is at the Tower Hotel, nearby Dexter
House.&lt;/p&gt;
&lt;p&gt;10:00 TUC Meeting Opening (Peter
Boncz)&lt;/p&gt;
&lt;p&gt;10:10
TUC
Presentations (RDF
Application Descriptions)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Johan Hjerling (BBC): &lt;em&gt;&lt;strong&gt;&lt;a href=&#34;attachments/4325436/5275669.pdf&#34;&gt;BBC Linked Data and the Semantic
Publishing Benchmark&lt;/a&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Andreas Both (Unister): &lt;em&gt;&lt;strong&gt;&lt;a href=&#34;attachments/4325436/5505027.pdf&#34;&gt;Ontology-driven applications in an
e-commerce context&lt;/a&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Nuno Carvalho (Fujitsu Laboratories Europe): &lt;a href=&#34;attachments/4325436/5275666.pdf&#34;&gt;&lt;em&gt;&lt;strong&gt;Fujitsu RDF use
cases and benchmarking
requirements&lt;/strong&gt;&lt;/em&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Robina Clayphan (Europeana): &lt;em&gt;&lt;strong&gt;&lt;a href=&#34;attachments/4325436/4816977.ppt&#34;&gt;Europeana and Open
Data&lt;/a&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;11:30 Semantic Publishing Benchmark (SPB)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Venelin Kotsev (Ontotext - LDBC): &lt;em&gt;&lt;strong&gt;&lt;a href=&#34;attachments/4325436/4816974.pdf&#34;&gt;Semantic Publishing Benchmark
Task Force Update&lt;/a&gt;&lt;/strong&gt;&lt;/em&gt; and
&lt;em&gt;&lt;strong&gt;&lt;a href=&#34;attachments/4325436/4816974.pdf&#34;&gt;Report&lt;/a&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;12:00-13:00 Lunch at the Graph Connect venue&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Talks During Lunch:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;[Pedro Furtado, Jorge Bernardino (Univ. Coimbra): &lt;strong&gt;&lt;a href=&#34;attachments/4325436/5275671.pdf&#34;&gt;KEYSTONE Cost
Action&lt;/a&gt;&lt;/strong&gt;]&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;13:00 TUC Presentations (Graph
Application Descriptions)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Minqi Zhou / Weining Qian (East China Normal
University): &lt;em&gt;&lt;strong&gt;&lt;a href=&#34;attachments/4325436/5275670.pdf&#34;&gt;Elastic and realistic social media data
generation&lt;/a&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Andrew Sherlock (Shapespace): &lt;em&gt;&lt;strong&gt;Shapespace Use Case&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Sebastian Verheughe (Telenor): &lt;em&gt;&lt;strong&gt;&lt;a href=&#34;attachments/4325436/5275667.pdf&#34;&gt;Real-time Resource
Authorization&lt;/a&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;14:00 Social Network Benchmark (SNB)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Norbert Martinez (UPC - LDBC): &lt;em&gt;&lt;strong&gt;&lt;a href=&#34;attachments/4325436/5505025.pdf&#34;&gt;Social Network Benchmark Task
Force Update&lt;/a&gt;&lt;/strong&gt;&lt;/em&gt;
and &lt;a href=&#34;attachments/4325436/4816975.pdf&#34;&gt;Report&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;14:30 Break&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;14:45 TUC Presentations (Graph Analytics)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Keith Houck (IBM): &lt;em&gt;&lt;strong&gt;Benchmarking experiences with [System G Native
Store (tentative title)]&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Abraham Bernstein (University of Zurich): &lt;em&gt;&lt;strong&gt;Streams and Advanced
Processing: Benchmarking RDF querying beyond the Standard SPARQL
Triple Store&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Luis Ceze (University of Washington): &lt;em&gt;&lt;strong&gt;Grappa and GraphBench
Status Update&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;em&gt;15:45 Break&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;16:00 TUC Presentations* (Possible Future RDF Benchmarking Topics)*&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Christian-Emil Ore (Unit for Digital Documentation, University of
Oslo, Norway): &lt;em&gt;&lt;strong&gt;CIDOC-CRM&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;[Atanas
Kiryakov (Ontotext): &lt;em&gt;&lt;strong&gt;&lt;a href=&#34;attachments/4325436/5275672.pdf&#34;&gt;Large-scale
Reasoning with a Complex Cultural Heritage Ontology (CIDOC
CRM)&lt;/a&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Kostis Kyzirakos (National and Kapodistrian University of Athens /
CWI): &lt;em&gt;&lt;strong&gt;&lt;a href=&#34;attachments/4325436/5275668.pdf&#34;&gt;Geographica: A Benchmark for Geospatial RDF
Stores&lt;/a&gt;&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Xavier Lopez (Oracle): &lt;em&gt;&lt;strong&gt;W3C Property Graph progress&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;li&gt;Thomas Scharrenbach (University
Zurich) &lt;em&gt;&lt;strong&gt;PCKS:  Benchmarking Semantic Flow Processing Systems&lt;/strong&gt;&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;17:20 Meeting Conclusion (Josep Larriba Pey)&lt;/p&gt;
&lt;p&gt;17:30 End of TUC meeting&lt;/p&gt;
&lt;p&gt;19:00 Social dinner&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;November 20th - Internal LDBC Meeting&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;10:00 Start&lt;/p&gt;
&lt;p&gt;12:30 &lt;em&gt;End of meeting&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;coffee and lunch provided&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;logistics&#34;&gt;Logistics&lt;/h3&gt;
&lt;p&gt;Date&lt;/p&gt;
&lt;p&gt;19th November 2013&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Location&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The TUC meeting will be held in &lt;strong&gt;The Tower&lt;/strong&gt; hotel (&lt;a href=&#34;http://goo.gl/qZt8Fz&#34;&gt;Google Maps
link&lt;/a&gt;) approximately 4 minutes
walk from
the &lt;a href=&#34;http://www.graphconnect.com/london/&#34;&gt;GraphConnect&lt;/a&gt; conference
in London.&lt;/p&gt;
&lt;p&gt;Getting there&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;From City Airport is the easiest: short ride on the DLR to Tower
Gateway. Easy.&lt;/li&gt;
&lt;li&gt;From London Heathrow: first need to take the Heathrow Express to
Paddington. Then take the Circle line to Tower Hill. &lt;a href=&#34;attachments/4325436/4554995.pdf&#34;&gt;See
attached&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Accomodation&lt;/p&gt;
&lt;p&gt;Tower Hill is
nice &lt;a href=&#34;http://www.booking.com/hotel/gb/sleep-inn-city-of-london.en-us.html&#34;&gt;http://www.booking.com/hotel/gb/sleep-inn-city-of-london.en-us.html&lt;/a&gt; - book
early to get a good rate&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Social Dinner&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The social dinner will take place at 7 pm on Nov 19.** TODO more
details**&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Travel costs&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;[There is some small budget available that can be used to assist some
attendees that are otherwise unable to fund their trip. Please contact
us using the following email address if you would like more
information: ldbcgrants AT ac DOT upc DOT
edu]&lt;/p&gt;
&lt;h3 id=&#34;ldbctuc-background&#34;&gt;LDBC/TUC Background&lt;/h3&gt;
&lt;p&gt;Looking back, we have been working on two benchmarks for the past year:
a Social Network Benchmark (SNB) and a Semantic Publishing Benchmark
(SPB). While below we provide a short summary, all the details of the
work on these benchmark development efforts can be found in the first
yearly progress
reports:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;attachments/4325436/4816974.pdf&#34;&gt;LDBC_SNB_Report_Nov2013.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;attachments/4325436/4816974.pdf&#34;&gt;LDBC_SPB_Report_Nov2013.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A summary of these efforts can be read below or, for a more detailed
account, please refer to: &lt;a href=&#34;attachments/4325436/4554967.pdf&#34;&gt;The Linked Data Benchmark Council: a Graph
and RDF industry benchmarking effort&lt;/a&gt;&lt;br&gt;
Annual reports about the progress, results, and future work of these two
efforts will soon be available for download here, and will be discussed
in depth at the
TUC.&lt;/p&gt;
&lt;h4 id=&#34;social-network-benchmark&#34;&gt;Social Network Benchmark&lt;/h4&gt;
&lt;p&gt;The Social Network Benchmark (SNB) is designed for evaluating a broad
range of technologies for tackling graph data management
workloads. The systems
targeted are quite broad: from graph, RDF, and relational database
systems to Pregel-like graph compute
frameworks. The social
network scenario was chosen with the following goals in mind:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;it should be understandable, and the relevance of managing such
data should be
understandable&lt;/li&gt;
&lt;li&gt;it should cover the complete range of interesting challenges,
according to the benchmark
scope&lt;/li&gt;
&lt;li&gt;the queries should be realistic, i.e., similar data and workloads
are encountered in
practice&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;SNB includes a data generator for creation of synthetic social network
data with the following characteristics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;data schema is representative of real social networks&lt;/li&gt;
&lt;li&gt;data generated includes properties occurring in real data, e.g.
irregular structure, structure/value correlations, power-law
distributions&lt;/li&gt;
&lt;li&gt;the software generator is easy-to-use, configurable and
scalable&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;SNB is intended to cover a broad range of aspects of social network data
management, and therefore includes three distinct workloads:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Interactive&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Tests system throughput with relatively simple queries and
concurrent updates, it is designed to test ACID features and
scalability in an online operational
setting.&lt;/li&gt;
&lt;li&gt;The targeted systems are expected to be those that offer
transactional
functionality.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Business Intelligence&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Consists of complex structured queries for analyzing online
behavior of users for marketing purposes, it is designed to
stress query execution and
optimization.&lt;/li&gt;
&lt;li&gt;The targeted systems are expected to be those that offer an
abstract query
language.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Graph Analytics&lt;/strong&gt;
&lt;ul&gt;
&lt;li&gt;Tests the functionality and scalability of systems for graph
analytics, which typically cannot be expressed in a query
language.&lt;/li&gt;
&lt;li&gt;Analytics is performed on most/all of the data in the graph as
a single operation and produces large intermediate results, and
it is not not expected to be transactional or need
isolation.&lt;/li&gt;
&lt;li&gt;The targeted systems are graph compute frameworks though
database systems may compete, for example by using iterative
implementations that repeatedly execute queries and keep
intermediate results in temporary data
structures.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;semantic-publishing-benchmark&#34;&gt;Semantic Publishing Benchmark&lt;/h4&gt;
&lt;p&gt;The Semantic Publishing Benchmark (SPB) simulates the management and
consumption of RDF metadata that describes media assets, or creative
works.&lt;/p&gt;
&lt;p&gt;The scenario is a media organization that maintains RDF descriptions of
its catalogue of creative works &amp;ndash; input was provided by actual media
organizations which make heavy use of RDF, including the BBC. The
benchmark is designed to reflect a scenario where a large number of
aggregation agents provide the heavy query workload, while at the same
time a steady stream of creative work description management operations
are in progress. This benchmark only targets RDF databases, which
support at least basic forms of semantic inference. A tagging ontology
is used to connect individual creative work descriptions to instances
from reference datasets,
e.g. sports, geographical, or political information.  The data used will
fall under the following categories: reference data, which is a
combination of several Linked Open Data datasets, e.g. GeoNames and
DBpedia; domain ontologies, that are specialist ontologies used to
describe certain areas of expertise of the publishing,  e.g., sport and
education; publication asset ontologies, that describe the structure and
form of the assets that are published, e.g., news stories, photos,
video, audio, etc.; and tagging ontologies and the metadata, that links
assets with reference/domain ontologies.&lt;/p&gt;
&lt;p&gt;The data generator is initialized by using several ontologies and
datasets. The instance data collected from these datasets are then used
at several points during the execution of the benchmark. Data generation
is performed by generating SPARQL fragments for create operations on
creative works and executing them against the RDF database system.&lt;/p&gt;
&lt;p&gt;Two separate workloads are modeled in SPB:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Editorial&lt;/strong&gt;
Simulates creating, updating and deleting creative work metadata
descriptions.
Media companies
use both manual and semi-automated processes for efficiently and
correctly managing asset descriptions, as well as annotating
them with relevant
instances from reference
ontologies.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Aggregation&lt;/strong&gt;
Simulates the
dynamic aggregation of content for consumption by the distribution
pipelines (e.g. a web-site). The publishing
activity is described as &amp;ldquo;dynamic&amp;rdquo;, because the content is not
manually selected and arranged on, say, a web page. Instead, templates
for pages are defined and the content is selected when a consumer
accesses the page.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;attachments&#34;&gt;Attachments:&lt;/h3&gt;
&lt;p&gt;&lt;img src=&#34;images/icons/bullet_blue.gif&#34; alt=&#34;&#34;&gt;
&lt;a href=&#34;attachments/4325436/4554967.pdf&#34;&gt;article.pdf&lt;/a&gt; (application/pdf)&lt;br&gt;
&lt;img src=&#34;images/icons/bullet_blue.gif&#34; alt=&#34;&#34;&gt;
&lt;a href=&#34;attachments/4325436/4554995.pdf&#34;&gt;ldbc_tuc_london.pdf&lt;/a&gt;
(application/pdf)&lt;br&gt;
&lt;img src=&#34;images/icons/bullet_blue.gif&#34; alt=&#34;&#34;&gt;
&lt;a href=&#34;attachments/4325436/4816976.pdf&#34;&gt;LDBC_SPB_Report_Nov2013.pdf&lt;/a&gt;
(application/pdf)&lt;br&gt;
&lt;img src=&#34;images/icons/bullet_blue.gif&#34; alt=&#34;&#34;&gt;
&lt;a href=&#34;attachments/4325436/4816975.pdf&#34;&gt;LDBC_SNB_Report_Nov2013.pdf&lt;/a&gt;
(application/pdf)&lt;br&gt;
&lt;img src=&#34;images/icons/bullet_blue.gif&#34; alt=&#34;&#34;&gt;
&lt;a href=&#34;attachments/4325436/4816974.pdf&#34;&gt;LDBC_SPB_Report_Nov2013.pdf&lt;/a&gt;
(application/pdf)&lt;br&gt;
&lt;img src=&#34;images/icons/bullet_blue.gif&#34; alt=&#34;&#34;&gt;
&lt;a href=&#34;attachments/4325436/4816977.ppt&#34;&gt;ldbc_tuc_19-11-2013_Europeana-Robina_Clayphan.ppt&lt;/a&gt;
(application/vnd.ms-powerpoint)&lt;br&gt;
&lt;img src=&#34;images/icons/bullet_blue.gif&#34; alt=&#34;&#34;&gt;
&lt;a href=&#34;attachments/4325436/5275666.pdf&#34;&gt;LDBC-TUC-Fujitsu-Final.pdf&lt;/a&gt;
(application/pdf)&lt;br&gt;
&lt;img src=&#34;images/icons/bullet_blue.gif&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;attachments/4325436/5275667.pdf&#34;&gt;LDBC London 19
Nov 2013 - Telenor Resource
Authorization.pdf&lt;/a&gt; (application/pdf)&lt;br&gt;
&lt;img src=&#34;images/icons/bullet_blue.gif&#34; alt=&#34;&#34;&gt;
&lt;a href=&#34;attachments/4325436/5275668.pdf&#34;&gt;Kyzirakos-Geographica.pdf&lt;/a&gt;
(application/pdf)&lt;br&gt;
&lt;img src=&#34;images/icons/bullet_blue.gif&#34; alt=&#34;&#34;&gt;
&lt;a href=&#34;attachments/4325436/5275669.pdf&#34;&gt;BBC-JohanHjerling-LDBC-TUC-3.pdf&lt;/a&gt;
(application/pdf)&lt;br&gt;
&lt;img src=&#34;images/icons/bullet_blue.gif&#34; alt=&#34;&#34;&gt;
&lt;a href=&#34;attachments/4325436/5275670.pdf&#34;&gt;LDBC_TUC_wnqian.pdf&lt;/a&gt;
(application/pdf)&lt;br&gt;
&lt;img src=&#34;images/icons/bullet_blue.gif&#34; alt=&#34;&#34;&gt;
&lt;a href=&#34;attachments/4325436/5275671.pdf&#34;&gt;LondonLDBCKEYSTONE.pdf&lt;/a&gt;
(application/pdf)&lt;br&gt;
&lt;img src=&#34;images/icons/bullet_blue.gif&#34; alt=&#34;&#34;&gt;
&lt;a href=&#34;attachments/4325436/5275672.pdf&#34;&gt;LDBC-CRM-reasoning-Alexiev2013-AK.pdf&lt;/a&gt;
(application/pdf)&lt;br&gt;
&lt;img src=&#34;images/icons/bullet_blue.gif&#34; alt=&#34;&#34;&gt;
&lt;a href=&#34;attachments/4325436/5505025.pdf&#34;&gt;LDBC_TUC3_SNB.pdf&lt;/a&gt;
(application/download)&lt;br&gt;
&lt;img src=&#34;images/icons/bullet_blue.gif&#34; alt=&#34;&#34;&gt; &lt;a href=&#34;attachments/4325436/5505026.pdf&#34;&gt;LDBC_Status of
the Semantic Publishing Benchmark.pdf&lt;/a&gt;
(application/download)&lt;br&gt;
&lt;img src=&#34;images/icons/bullet_blue.gif&#34; alt=&#34;&#34;&gt;
&lt;a href=&#34;attachments/4325436/5505027.pdf&#34;&gt;andreas-both__linked-data-benchmark-concil__3rd-technical-user-meeting_2013.pdf&lt;/a&gt;
(application/download)\&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Second TUC Meeting</title>
      <link>https://ldbc.github.io/event/second-tuc-meeting/</link>
      <pubDate>Mon, 22 Apr 2013 10:00:00 +0000</pubDate>
      
      <guid>https://ldbc.github.io/event/second-tuc-meeting/</guid>
      <description>&lt;p&gt;The LDBC consortium are pleased to announce the second Technical User
Community (TUC) meeting.&lt;/p&gt;
&lt;p&gt;This will be a two day event in Munich on the &lt;strong&gt;22/23rd April 2013&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The event will include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Introduction to the objectives and progress of the LDBC project.&lt;/li&gt;
&lt;li&gt;Description of the progress of the benchmarks being evolved through
Task Forces.&lt;/li&gt;
&lt;li&gt;Users explaining their use-cases and describing the limitations they
have found in current technology.&lt;/li&gt;
&lt;li&gt;Industry discussions on the contents of the benchmarks.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;All users of RDF and graph databases are welcome to attend. If you are
interested, please contact: ldbc AT ac DOT upc DOT edu&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;News:&lt;/strong&gt; due to the airline strikes at Lufthansa and El Al, Herman
Ravkin (BIG project) and Jesús Lanchas (ACCESO) and Thomas Scharrenbach
(University of Zurich) cannot come, and Norbert Martinez will arrive
only tomorrow. The schedule has been adapted, and may change again, keep
an eye on it.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#agenda&#34;&gt;Agenda&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#slides&#34;&gt;Slides&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#logistics&#34;&gt;Logistics&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#date&#34;&gt;Date&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#location&#34;&gt;Location&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#venue&#34;&gt;Venue&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Getting to the TUM Campus from the Munich city center:
Subway
(U-Bahn)&lt;/li&gt;
&lt;li&gt;Getting to the TUM Campus from the Munich
Airport&lt;/li&gt;
&lt;li&gt;Getting to the TUM Campus from Garching:
U-Bahn&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gettingthere&#34;&gt;Getting there&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#accomodation&#34;&gt;Accomodation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#socialDinner&#34;&gt;Social Dinner&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#travelcosts&#34;&gt;Travel costs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;agenda&#34;&gt;Agenda&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;April 22nd&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;10:00 &lt;em&gt;Registration.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;10:30 Josep Lluis Larriba Pey (UPC) - &lt;em&gt;Welcome and Introduction.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;10:30 Peter Boncz (VUA): &lt;a href=&#34;attachments/2523698/2687373.pptx&#34;&gt;LDBC: goals and
status&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;       *Social Network Use Cases (with discussion moderated by Josep
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lluis Larriba Pey)*&lt;/p&gt;
&lt;p&gt;11:00 Josep Lluis Larriba Pey (UPC):  &lt;a href=&#34;attachments/2523698/2687372.pdf&#34;&gt;Social Network Benchmark Task
Force&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;11:30 Gustavo
González (Mediapro): &lt;a href=&#34;attachments/2523698/2687367.pdf&#34;&gt;Graph-based User Modeling through Real-time Social
Streams&lt;/a&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;p&gt;12:00 Klaus Großmann (Dshini): &lt;a href=&#34;attachments/2523698/2687365.pdf&#34;&gt;Neo4j at
Dshini&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;12:30 Lunch&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;     *Semantic Publishing Use Cases (with discussion moderated by
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Barry Bishop)*&lt;/p&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;13:30 Barry Bishop
(Ontotext): &lt;a href=&#34;attachments/2523698/2687366.pptx&#34;&gt;Semantic Publishing Benchmark Task
Force&lt;/a&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;14:00 Dave Rogers
(BBC): &lt;a href=&#34;attachments/2523698/2687364.pptx&#34;&gt;Linked Data Platform at the
BBC&lt;/a&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;p&gt;14:30 Edward Thomas (Wolters Kluwer): &lt;a href=&#34;attachments/2523698/2687374.pdf&#34;&gt;Semantic Publishing at Wolters
Kluwer&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;15:00 coffee
break&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Projects Related to LDBC&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;15:30 Fabian Suchanek (MPI): &amp;ldquo;YAGO: A large knowledge base from
Wikipedia and WordNet&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;16:00 Antonis
Loziou (VUA): &lt;a href=&#34;attachments/2523698/2687375.pptx&#34;&gt;The OpenPHACTS approach to data
integration&lt;/a&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;16:30 Mirko Kämpf
(Brox): &amp;ldquo;GeoKnow - Spatial Data Web project and Supply Chain Use
Case&amp;rdquo;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;17:00 &lt;!-- raw HTML omitted --&gt;&lt;em&gt;End
of first day&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;19:00 Social dinner&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;April 23rd&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;      *Industry &amp;amp; Hardware Aspects*
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;10:00 Xavier Lopez (Oracle): &lt;a href=&#34;attachments/2523698/2687384.pdf&#34;&gt;Graph Database Performance an Oracle
Perspective.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;10:30 Pedro Trancoso (University of Cyprus): &amp;ldquo;Benchmarking and computer
architecture: the research side&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;11:00 &lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;coffee break&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;11:30 Peter Boncz (VUA) moderates: &amp;ldquo;&lt;a href=&#34;TUC-Meeting-Social-Network-Task-Force_2524095.html&#34;&gt;next steps in the Social Networking
Task Force&lt;/a&gt;&amp;rdquo;&lt;/p&gt;
&lt;p&gt;12:00 Barry Bishop (Ontotext) moderates: &amp;ldquo;next steps in the Semantic
Publishing Task Force&amp;rdquo;&lt;/p&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;12:30 &lt;em&gt;End of
meeting&lt;/em&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;h3 id=&#34;slides&#34;&gt;Slides&lt;/h3&gt;
&lt;p&gt;The slides of the presentations during the meeting will we posted here.&lt;/p&gt;
&lt;h3 id=&#34;logistics&#34;&gt;Logistics&lt;/h3&gt;
&lt;h4 id=&#34;date&#34;&gt;Date&lt;/h4&gt;
&lt;p&gt;22nd and 23th April 2013&lt;/p&gt;
&lt;h4 id=&#34;location&#34;&gt;Location&lt;/h4&gt;
&lt;p&gt;The TUC meeting will be held at LE009 room at LRZ
(Leibniz-Rechenzentrum) located inside the TU Munich campus in Garching,
Germany. The address is:&lt;/p&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;LRZ (Leibniz-Rechenzentrum)&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;Boltzmannstraße 1&lt;!-- raw HTML omitted --&gt;&lt;br&gt;
&lt;!-- raw HTML omitted --&gt;85748 Garching&lt;!-- raw HTML omitted --&gt;,  Germany&lt;/p&gt;
&lt;h4 id=&#34;venue&#34;&gt;Venue&lt;/h4&gt;
&lt;p&gt;To reach the campus, there are several options, including Taxi and
Subway
&lt;a href=&#34;http://www.in.tum.de/fileadmin/user_upload/Sonstiges/anfahrt_garching.pdf&#34;&gt;Ubahn&lt;/a&gt;&lt;/p&gt;
&lt;h5 id=&#34;getting-to-the-tum-campus-from-the-munich-city-center-subway-u-bahn&#34;&gt;Getting to the TUM Campus from the Munich city center: Subway (U-Bahn)&lt;/h5&gt;
&lt;p&gt;Take the U-bahn line U6 in the direction of Garching-Forschungszentrum,
exit at the end station. Take the south exit to MI-Building and LRZ on
the Garching Campus. The time of the journey from the city center is
approx.25-30 minutes. In order to get here from the City Center, you
need the Munich XXL ticket that costs around 7.50 euros and covers all
types of transportation for one day. The ticket has to be validated
before ride.&lt;/p&gt;
&lt;h5 id=&#34;getting-to-the-tum-campus-from-the-munich-airport&#34;&gt;Getting to the TUM Campus from the Munich Airport&lt;/h5&gt;
&lt;ol&gt;
&lt;li&gt;(except weekends) S-Bahn S8 line in the direction of (Hauptbahnhof)
Munich Central Station until the third stop, Ismaning (approx. 13
minutes). From here Bus Nr. 230 until stop MI-Building on the
Garching Campus. Alternatively: S1 line until Neufahrn, then with
the Bus 690, which stops at Boltzmannstraße.&lt;/li&gt;
&lt;li&gt;S-Bahn lines S8 or S1 towards City Center until Marienplatz stop.
Then change to U-bahn U6 line towards Garching-Forschungszentrum,
exit at the last station. Take the south exit to MI-Building and
LRZ.&lt;/li&gt;
&lt;li&gt;Taxi: fare is ca,30-40 euros.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For cases 1 and 2, before the trip get the One-day Munich Airport ticket
and validate it. It will cover all public transportation for that day.&lt;/p&gt;
&lt;h5 id=&#34;getting-to-the-tum-campus-from-garching-u-bahn&#34;&gt;Getting to the TUM Campus from Garching: U-Bahn&lt;/h5&gt;
&lt;p&gt;The city of Garching is located on the U6 line, one stop before the
Garching-Forschungszentrum**.** In order to get from Garching to
Garching-Forschungszentrum with the U-bahn, a special one-way ticket
called Kurzstrecke (1.30 euros) can be purchased.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Finding LRZ@TUM&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;http://www.openstreetmap.org/?mlat=48.2615702464&amp;amp;mlon=11.6686558264&amp;amp;zoom=32&#34;&gt;OpenStreetMap link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://maps.google.com/maps?q=48.2615702464,11.6686558264&amp;amp;spn=0.005,0.005&amp;amp;t=k&#34;&gt;GoogleMaps link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;attachments/2523698/2687268.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;attachments/2523698/2687269.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;getting-there&#34;&gt;Getting there&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Flying: Munich&lt;/strong&gt; airport is located 28.5 km northeast of Munich. There
are two ways to get from the airport to the city center: suburban train
(S-bahn) and Taxi.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;S-Bahn:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;S-bahn lines S1 and S8 will get you from the Munich airport to the city
center, stopping at both Munich Central Station (Hauptbahnhof) and
Marienplatz. One-day Airport-City ticket costs 11.20 euros and is
valid for the entire Munich area public transportation during the day of
purchase (the tickets needs to be validated before the journey). S-bahn
leaves every 5-20 minutes and reaches the city center in approx.40
minutes&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Taxi:&lt;/strong&gt; taxi from the airport to the city center costs approximately
50 euros&lt;/p&gt;
&lt;h4 id=&#34;accomodation&#34;&gt;Accomodation&lt;/h4&gt;
&lt;p&gt;The following hotels are recommended. The first two are located near the
TUM campus in
&lt;a href=&#34;https://maps.google.com/maps/ms?msid=215046094125658833100.0004d59d7a9baeb5c11f1&amp;amp;msa=0&amp;amp;ll=48.240795,11.660957&amp;amp;spn=0.115127,0.253029&#34;&gt;this google map&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Hotel Koenig Ludwig II&lt;br&gt;
Bürgerplatz 3,&lt;br&gt;
Garching, Germany&lt;/p&gt;
&lt;p&gt;Tel: +49 89 320 50 46&lt;br&gt;
&lt;!-- raw HTML omitted --&gt;&lt;a href=&#34;http://www.hkl.de/home-en/&#34;&gt;http://www.hkl.de/home-en/&lt;/a&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Hotel Am Park&lt;br&gt;
Bürgermeister-Amon-Straße 2,&lt;br&gt;
Garching, Germany&lt;/p&gt;
&lt;p&gt;Tel: +49 89 320 40 84&lt;br&gt;
&lt;!-- raw HTML omitted --&gt;&lt;a href=&#34;http://hotel-am-park.com/&#34;&gt;http://hotel-am-park.com/&lt;/a&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Hotel-Pension Carolin&lt;br&gt;
Kaulbachstraße 42&lt;br&gt;
Munich, Germany&lt;/p&gt;
&lt;p&gt;Tel: +49 89 34 57 57&lt;br&gt;
&lt;!-- raw HTML omitted --&gt;&lt;a href=&#34;http://www.pension-carolin.com/&#34;&gt;http://www.pension-carolin.com/&lt;/a&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Cosmopolitan Hotel&lt;br&gt;
Hohenzollernstrasse 5&lt;br&gt;
Munich, Germany&lt;/p&gt;
&lt;p&gt;Tel: +49 89 38 38 10&lt;br&gt;
&lt;!-- raw HTML omitted --&gt;&lt;a href=&#34;http://www.cosmopolitanhotel.de/en/&#34;&gt;http://www.cosmopolitanhotel.de/en/&lt;/a&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Hotel Pullmann&lt;br&gt;
Theodor-Dombart-Straße 4&lt;br&gt;
Munich, Germany&lt;/p&gt;
&lt;p&gt;Tel: +49 89 360 99 0&lt;br&gt;
&lt;!-- raw HTML omitted --&gt;&lt;a href=&#34;http://www.pullman-hotel-munich.com/default-en.html&#34;&gt;http://www.pullman-hotel-munich.com/default-en.html&lt;/a&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;h4 id=&#34;social-dinner&#34;&gt;&lt;strong&gt;Social Dinner&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;The social dinner will take place at 7 pm on April 22 in Hofbräuhaus
(second floor)&lt;/p&gt;
&lt;p&gt;Address: Hofbräuhaus, Platzl 9, Munich&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://maps.google.com/maps?f=q&amp;amp;source=s_q&amp;amp;hl=en&amp;amp;geocode=&amp;amp;q=Hofbr%C3%A4uhaus,+Platzl+9,+M%C3%BCnchen,+Germany&amp;amp;aq=0&amp;amp;oq=hof&amp;amp;sll=37.0625,-95.677068&amp;amp;sspn=53.741627,105.46875&amp;amp;vpsrc=6&amp;amp;ie=UTF8&amp;amp;hq=Hofbr%C3%A4uhaus,+Platzl+9,&amp;amp;hnear=%D0%9C%D1%8E%D0%BD%D1%85%D0%B5%D0%BD,+%D0%92%D0%B5%D1%80%D1%85%D0%BD%D1%8F%D1%8F+%D0%91%D0%B0%D0%B2%D0%B0%D1%80%D0%B8%D1%8F,+%D0%91%D0%B0%D0%B2%D0%B0%D1%80%D0%B8%D1%8F,+%D0%93%D0%B5%D1%80%D0%BC%D0%B0%D0%BD%D0%B8%D1%8F&amp;amp;ll=48.137697,11.578436&amp;amp;spn=0.002796,0.006437&amp;amp;t=m&amp;amp;z=18&amp;amp;iwloc=A&amp;amp;cid=12232182229576260143&#34;&gt;GoogleMaps link&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;travel-costs&#34;&gt;Travel costs&lt;/h4&gt;
&lt;p&gt;There is some small budget available that can be used to assist some
attendees that are otherwise unable to fund their trip. Please contact
us using the following email address if you would like more
information: ldbcgrants AT ac DOT upc DOT edu&lt;/p&gt;
&lt;h3 id=&#34;attachments&#34;&gt;Attachments:&lt;/h3&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;
&lt;a href=&#34;attachments/2523698/2687267.png&#34;&gt;e19940cef9.png&lt;/a&gt; (image/png)&lt;br&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;a href=&#34;attachments/2523698/2687268.gif&#34;&gt;getBuildingMap.gif&lt;/a&gt; (image/gif)&lt;br&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;a href=&#34;attachments/2523698/2687269.gif&#34;&gt;mapbigger.gif&lt;/a&gt; (image/gif)&lt;br&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;a href=&#34;attachments/2523698/2687270.png&#34;&gt;5d84f31b1f.png&lt;/a&gt; (image/png)&lt;br&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;a href=&#34;attachments/2523698/2687364.pptx&#34;&gt;BBC_LDBC_presentation_Dave_Rogers.pptx&lt;/a&gt;
(application/vnd.openxmlformats-officedocument.presentationml.presentation)&lt;br&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;a href=&#34;attachments/2523698/2687365.pdf&#34;&gt;Dshini_LDBC_Klaus_Grossman.pdf&lt;/a&gt;
(application/pdf)&lt;br&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;a href=&#34;attachments/2523698/2687366.pptx&#34;&gt;Ontotext_LDBC_Barry_Bishop_Publishing.pptx&lt;/a&gt;
(application/vnd.openxmlformats-officedocument.presentationml.presentation)&lt;br&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;a href=&#34;attachments/2523698/2687367.pdf&#34;&gt;MediaPro_LDBC_Gustavo_Gonzalez-Sanchez.pdf&lt;/a&gt;
(application/pdf)&lt;br&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;a href=&#34;attachments/2523698/2687372.pdf&#34;&gt;UPC_LDBC_Norbert_Martinez.pdf&lt;/a&gt;
(application/pdf)&lt;br&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;a href=&#34;attachments/2523698/2687373.pptx&#34;&gt;CWI_LDBC_Peter_Boncz.pptx&lt;/a&gt;
(application/vnd.openxmlformats-officedocument.presentationml.presentation)&lt;br&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;a href=&#34;attachments/2523698/2687374.pdf&#34;&gt;Wolters_Kluwer_LDBC_Edward_Thomas.pdf&lt;/a&gt;
(application/pdf)&lt;br&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;a href=&#34;attachments/2523698/2687375.pptx&#34;&gt;OpenPhacts_LDBC_Antonis_Loizou.pptx&lt;/a&gt;
(application/vnd.openxmlformats-officedocument.presentationml.presentation)&lt;br&gt;
&lt;!-- raw HTML omitted --&gt; &lt;a href=&#34;attachments/2523698/2687384.pdf&#34;&gt;Graph
Database Performance an Oracle
Perspective.pdf&lt;/a&gt; (application/pdf)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>First TUC Meeting</title>
      <link>https://ldbc.github.io/event/first-tuc-meeting/</link>
      <pubDate>Mon, 19 Nov 2012 09:00:00 +0100</pubDate>
      
      <guid>https://ldbc.github.io/event/first-tuc-meeting/</guid>
      <description>&lt;p&gt;The LDBC consortium are pleased to announce the first Technical User
Community (TUC) meeting. This will be a two day event in Barcelona on
the &lt;strong&gt;19/20th November 2012&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;So far more than six commercial consumers of graph/RDF database
technology have expressed an interest in attending the event and more
are welcome. The proposed format of the event wil include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Introduction by the coordinator and technical director explaining
the objectives of the LDBC project&lt;/li&gt;
&lt;li&gt;Invitation to users to explain their use-cases and describe the
limitations they have found in current technology&lt;/li&gt;
&lt;li&gt;Brain-storming session for identifying trends and mapping out
strategies to tackle existing choke-points&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The exact agenda will be published here as things get finalised before
the event.&lt;/p&gt;
&lt;p&gt;All users of RDF and graph databases are welcome to attend. If you are
interested, please contact: ldbc AT ac DOT upc DOT edu&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#agenda&#34;&gt;Agenda&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#slides&#34;&gt;Slides&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#logistics&#34;&gt;Logistics&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#date&#34;&gt;Date&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#location&#34;&gt;Location&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#venue&#34;&gt;Venue&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#gettingthere&#34;&gt;Getting there&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#accomodation&#34;&gt;Accomodation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#travelcosts&#34;&gt;Travel costs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;agenda&#34;&gt;Agenda&lt;/h3&gt;
&lt;p&gt;We will start at 9:00 on Monday for a full day, followed by a half a day
on Tuesday to allow attendees to travel home on the evening of the 20th.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Day 1&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;09:00 Welcome (Location: Aula Master)&lt;br&gt;
09:30 Project overview (Enphasis on task forces?) + Questionnaire
results?&lt;br&gt;
10:30 Coffee break&lt;br&gt;
11:00 User talks (To gather information for use cases?)&lt;/p&gt;
&lt;p&gt;13:00 Lunch&lt;/p&gt;
&lt;p&gt;14:00 User talks (cont.)&lt;br&gt;
15:00 Use case discussions (based on questionnaire results + consortium
proposal + user talks).&lt;br&gt;
16:00 Task force proposals (consortium)&lt;br&gt;
17:00 Finish first day&lt;/p&gt;
&lt;p&gt;20:00 Social dinner&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Day 2&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;10:00 Task force discussion (consortium + TUC)&lt;br&gt;
11:00 Coffe break&lt;br&gt;
11:30 Task force discussion (consortium + TUC)&lt;br&gt;
12:30 Summaries (Task forces, use cases, &amp;hellip;) and actions&lt;/p&gt;
&lt;p&gt;13:00 Lunch and farewell&lt;/p&gt;
&lt;p&gt;15:00 LDBC Internal meeting&lt;/p&gt;
&lt;h3 id=&#34;slides&#34;&gt;Slides&lt;/h3&gt;
&lt;p&gt;Opening session:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;attachments/1671180/2686995.pptx&#34;&gt;CWI - Peter Boncz&lt;/a&gt; - Objectives&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;attachments/1671180/2687001.pdf&#34;&gt;UPC - Larri&lt;/a&gt; - Questionnaire&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;User stories:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;attachments/1671180/2686998.pdf&#34;&gt;BBC - Jem Rayfield&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;CA Technologies - Victor Muntés&lt;/li&gt;
&lt;li&gt;Connected Discovery (Open Phacts) - Bryn Williams-Jones&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;attachments/1671180/2687003.pptx&#34;&gt;Elsevier - Alan Yagoda&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;attachments/1671180/2687000.pptx&#34;&gt;&lt;!-- raw HTML omitted --&gt;ERA7 Bioinformatics&lt;!-- raw HTML omitted --&gt; - Eduardo
Pareja&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;a href=&#34;attachments/1671180/2687005.pptx&#34;&gt;Press
Association - Jarred
McGinnis&lt;/a&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;attachments/1671180/2687004.pptx&#34;&gt;&lt;!-- raw HTML omitted --&gt;RJLee - David
Neuer&lt;!-- raw HTML omitted --&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;attachments/1671180/2686994.pdf&#34;&gt;Yale - Lec Maj&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Benchmark proposals:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;attachments/1671180/2686991.pdf&#34;&gt;&lt;!-- raw HTML omitted --&gt;Publishing benchmark proposal -
&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;Ontotext - &lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;Barry
Bishop&lt;!-- raw HTML omitted --&gt;&lt;/a&gt;&lt;!-- raw HTML omitted --&gt; &lt;!-- raw HTML omitted --&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;attachments/1671180/2687002.pdf&#34;&gt;Social Network Benchmark Proposal - UPC -
Larri&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;logistics&#34;&gt;Logistics&lt;/h4&gt;
&lt;h5 id=&#34;date&#34;&gt;Date&lt;/h5&gt;
&lt;p&gt;19th and 20th November 2012&lt;/p&gt;
&lt;h5 id=&#34;location&#34;&gt;Location&lt;/h5&gt;
&lt;p&gt;The TUC meeting will be held at “Aula Master” at A3 building located
inside the “Campus Nord de la UPC” in Barcelona. The address is:&lt;/p&gt;
&lt;p&gt;Aula Master&lt;br&gt;
Edifici A3, Campus Nord UPC&lt;br&gt;
C. Jordi Girona, 1-3&lt;br&gt;
08034 Barcelona, Spain&lt;/p&gt;
&lt;h4 id=&#34;venue&#34;&gt;Venue&lt;/h4&gt;
&lt;p&gt;To reach the campus, there are several options, including Taxi, &lt;a href=&#34;http://www.tmb.cat/ca/c/document_library/get_file?uuid=c8996f6c-8ad5-4d21-b59b-faf9fceebd80&amp;amp;groupId=10168&#34;&gt;Metro&lt;/a&gt; and
&lt;a href=&#34;http://www.tmb.cat/ca/c/document_library/get_file?uuid=5e6af5e2-7677-4ce8-85bb-8e63f2b086f1&amp;amp;groupId=10168&#34;&gt;Bus&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;attachments/1671180/1933315.jpg&#34; alt=&#34;&#34;&gt;&lt;br&gt;
&lt;strong&gt;Finding UPC&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;attachments/1671180/1933318.jpg&#34; alt=&#34;&#34;&gt;&lt;br&gt;
&lt;strong&gt;Finding the meeting room&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&#34;getting-there&#34;&gt;Getting there&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Flying:&lt;/strong&gt; Barcelona airport is situated 12 km from the city. There are
several ways of getting from the airport to the centre of Barcelona, the
cheapest of which is to take the train located outside just a few
minutes walking distance past the parking lots at terminal 2 (there is a
free bus between terminal 1 and terminal 2, see this
&lt;a href=&#34;http://goo.gl/maps/iJqlj&#34;&gt;map of the airport&lt;/a&gt;.
It is possible to buy 10 packs of train tickets which makes it cheaper.
Taking the bus to the centre of town is more convenient as they leave
directly from terminal 1 and 2, however it is more expensive than the
train.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Rail:&lt;/strong&gt; The Renfe commuter train leaves the airport every 30 minutes
from 6.13 a.m. to 11.40 p.m. Tickets cost around 3€ and the journey to
the centre of Barcelona (Sants or Plaça Catalunya stations) takes 20
minutes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bus:&lt;/strong&gt; The Aerobus leaves the airport every 12 minutes, from 6.00 a.m.
to 24.00, Monday to Friday, and from 6.30 a.m. to 24.00 on Saturdays,
Sundays and public holidays. Tickets cost 6€ and the journey ends in
Plaça Catalunya in the centre of Barcelona.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Taxi:&lt;/strong&gt; From the airport, you can take one of Barcelona&amp;rsquo;s typical
black and yellow taxis. Taxis may not take more than four passengers.
Unoccupied taxis display a green light and have a clearly visible sign
showing LIBRE or LLIURE. The trip to Sants train station costs
approximately €16 and trips to other destinations in the city cost
approximately €18.&lt;/p&gt;
&lt;p&gt;**Train and bus: **Barcelona has two international train stations: Sants
and França. Bus companies have different points of arrival in different
parts of the city. You can find detailed information in the following
link: &lt;a href=&#34;http://www.barcelona-airport.com/eng/transport_eng.htm&#34;&gt;http://www.barcelona-airport.com/eng/transport_eng.htm&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;attachments/1671180/1933316.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The locations of the airport and the city centre&lt;/strong&gt;&lt;/p&gt;
&lt;h4 id=&#34;accomodation&#34;&gt;Accomodation&lt;/h4&gt;
&lt;p&gt;The following hotels are recommended. The two first are located near the
UPC campus and they take 10-15 min by foot to reach the TUC meeting
location. The two last are located at the city center. They require
about 30 min (taking metro L3 at plaça Catalunya) to reach the TUC
meeting location. You can see the hotel locations
in &lt;a href=&#34;https://maps.google.es/maps/ms?msid=205398946114145535508.0004cbc8999989b0bcd43&amp;amp;msa=0&amp;amp;ll=41.390461,2.135639&amp;amp;spn=0.039923,0.077162&#34;&gt;this google map&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;&lt;!-- raw HTML omitted --&gt;&lt;/p&gt;
&lt;p&gt;Hotel Husa Pedralbes&lt;br&gt;
Fontcoberta, 4&lt;br&gt;
8034 Barcelona&lt;br&gt;
932 037 112&lt;br&gt;
&lt;a href=&#34;http://www.hotelhusapedralbes.com/&#34;&gt;http://www.hotelhusapedralbes.com/&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Hotel Bonanova Park&lt;br&gt;
Capita Arenas, 51&lt;br&gt;
08034 Barcelona&lt;br&gt;
932 04 09 00&lt;br&gt;
&lt;a href=&#34;http://www.hotelbonanovapark.com/&#34;&gt;http://www.hotelbonanovapark.com/&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Hotel Inglaterra Barcelona&lt;br&gt;
Carrer de Pelai, 14&lt;br&gt;
08001 Barcelona&lt;br&gt;
935 05 11 00&lt;br&gt;
&lt;a href=&#34;http://www.hotel-inglaterra.com/&#34;&gt;http://www.hotel-inglaterra.com/&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Hotel Jazz&lt;br&gt;
Carrer de Pelai, 3&lt;br&gt;
08001 Barcelona&lt;br&gt;
935 52 96 96&lt;br&gt;
&lt;a href=&#34;http://www.hoteljazz.com/&#34;&gt;http://www.hoteljazz.com/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In addition to the hotels above, there is the possibility to stay at the &amp;ldquo;Torre Girona&amp;rdquo; residence. It is the closest and cheapest option available. You can find &lt;a href=&#34;http://www.resa.es/eng/Residences/Torre-Girona/daily-accommodation/(reservas)/Diario/(sec)/full&#34;&gt;detailed information here&lt;/a&gt;. Basically,
it costs 53 euros for a single room and 60 to 88 euros for a double room
depending if it is occupied by one or two people. Currently, there are
20 individual and 20 double rooms free for these days. If anyone is
interested in this option, you should send an email to &lt;a href=&#34;mailto:torregirona@resa.es&#34;&gt;torregirona@resa.es&lt;/a&gt; asking
for a reservation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Torre Girona Residence Hall&lt;/strong&gt;&lt;br&gt;
Passeig dels Tillers, 19&lt;br&gt;
08034 Barcelona&lt;br&gt;
Telephone: 0034 93 390 43 00&lt;br&gt;
Fax: 0034 93 205 69 10&lt;br&gt;
E-mail: &lt;a href=&#34;mailto:torregirona@resa.es&#34;&gt;torregirona@resa.es&lt;/a&gt;&lt;/p&gt;
&lt;h4 id=&#34;travel-costs&#34;&gt;Travel costs&lt;/h4&gt;
&lt;p&gt;There is some small budget available that can be used to assist some
attendees that are otherwise unable to fund their trip. Please contact
us using the following email address if you would like more
information: ldbcgrants AT ac DOT upc DOT edu&lt;/p&gt;
&lt;h3 id=&#34;attachments&#34;&gt;Attachments:&lt;/h3&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;
&lt;a href=&#34;attachments/1671180/1933315.jpg&#34;&gt;upc_map.jpg&lt;/a&gt; (image/jpeg)&lt;br&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;a href=&#34;attachments/1671180/1933316.jpg&#34;&gt;barcelona_map.jpg&lt;/a&gt; (image/jpeg)&lt;br&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;a href=&#34;attachments/1671180/1933317.jpg&#34;&gt;bus_map.jpg&lt;/a&gt; (image/jpeg)&lt;br&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;a href=&#34;attachments/1671180/1933318.jpg&#34;&gt;meeting_room_map.jpg&lt;/a&gt; (image/jpeg)&lt;br&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;a href=&#34;attachments/1671180/2686991.pdf&#34;&gt;ldbc_tuc_19-11-2012_Ontotext-Barry_Bishop.pdf&lt;/a&gt;
(application/force-download)&lt;br&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;a href=&#34;attachments/1671180/2686994.pdf&#34;&gt;ldbc_tuc_19-11-2012_Yale-Lec_Maj.pdf&lt;/a&gt;
(application/force-download)&lt;br&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;a href=&#34;attachments/1671180/2686995.pptx&#34;&gt;ldbc_tuc_19-11-2012_CWI-Peter_Boncz.pptx&lt;/a&gt;
(application/vnd.openxmlformats-officedocument.presentationml.presentation)&lt;br&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;a href=&#34;attachments/1671180/2686998.pdf&#34;&gt;ldbc_tuc_19-11-2012_BBC-Jem_Rayfield.pdf&lt;/a&gt;
(application/force-download)&lt;br&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;a href=&#34;attachments/1671180/2687000.pptx&#34;&gt;ERA7_BIOINFORMATICS_Bio4j_LDBC_TUC_meeting_Nov_2012.pptx&lt;/a&gt;
(application/vnd.openxmlformats-officedocument.presentationml.presentation)&lt;br&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;a href=&#34;attachments/1671180/2687001.pdf&#34;&gt;ldbc_tuc_19-11-2012_Questionnaire.pdf&lt;/a&gt;
(application/pdf)&lt;br&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;a href=&#34;attachments/1671180/2687002.pdf&#34;&gt;ldbc_tuc_19-11-2012_social_network_taskforce_proposal.pdf&lt;/a&gt;
(application/pdf)&lt;br&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;a href=&#34;attachments/1671180/2687003.pptx&#34;&gt;ldbc_tuc_19-11-2012_Elsevier-Alan_Yagoda.pptx&lt;/a&gt;
(application/vnd.openxmlformats-officedocument.presentationml.presentation)&lt;br&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;a href=&#34;attachments/1671180/2687004.pptx&#34;&gt;ldbc_tuc_19-11-2012_RJLee-David_Neuer.pptx&lt;/a&gt;
(application/vnd.openxmlformats-officedocument.presentationml.presentation)&lt;br&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;a href=&#34;attachments/1671180/2687005.pptx&#34;&gt;ldbc_tuc_19-11-2012_Press_Association-Jarred_McGinnis.pptx&lt;/a&gt;
(application/vnd.openxmlformats-officedocument.presentationml.presentation)&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>